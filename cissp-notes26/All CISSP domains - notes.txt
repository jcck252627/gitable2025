
‚Ä¢ Domain 1: Security and Risk Management
‚Ä¢ Domain 2: Asset Security
‚Ä¢ Domain 3: Security Architecture and Engineering
‚Ä¢ Domain 4: Communication and Network Security
‚Ä¢ Domain 5: Identity and Access Management (IAM)
‚Ä¢ Domain 6: Security Assessment and Testing
‚Ä¢ Domain 7: Security Operations
‚Ä¢ Domain 8: Software Development Security
https://drive.google.com/file/d/1y8LiZUfuVykYvsjY3GqNESLdyCEK4icf/view?usp=sharing
https://drive.google.com/file/d/1y8LiZUfuVykYvsjY3GqNESLdyCEK4icf/view?usp=drive_link

CISSP EASY/MID questions #3 - ALL CISSP domains 250 Q - 2025
CISSP EASY/MID questions #4 - ALL CISSP domains 250 Q - 2025 ******
CISSP EASY/MID questions #5 - ALL CISSP domains 250 Q - 2025

CISSP Easy/Mid practice questions: Domain 3 & 4 - 2025 Exam
CISSP Easy/Mid practice questions: Domain 1 & 2 - 2025 Exam
CISSP Easy/Mid practice questions: Domain 5 & 6 - 2025 Exam
CISSP Easy/Mid practice questions: Domain 7 & 8 - 2025 Exam

HARD CISSP practice questions #1: All CISSP domains - 125Q
HARD CISSP practice questions #5: All CISSP domains - 125Q

COMPLEX CISSP practice questions #4: All CISSP domains 125Q
COMPLEX CISSP practice questions #3: All CISSP domains 125Q

CISSP: How to study (plans, tips, materials, approach) 2025

Latest CISSP Practice Tests 700 In-Depth Q/As & Explanations

Framework and Compliance lists:

 COBIT 2019: 
 COBIT, which stands for Control Objectives for Information and Related Technologies, is primarily a framework focused on governance and management of enterprise IT. Although COBIT can help organizations align IT with business strategies and manage IT-related risks, 
 COBIT (Control Objectives for Information and Related Technologies)
 
Business-focused IT governance framework.
Aligns IT goals with business objectives and risk management.
  Electronic Communications Privacy Act (ECPA)
 SOX

HIPAA
**HIPAA = Health Insurance Portability and Accountability Act (1996)**
**HIPAA establishes a framework for protecting health information, and its enforcement is carried out by the Office for Civil Rights (OCR) and the Health and Human Services (HHS). OCR is responsible for enforcing the HIPAA Privacy and Security Rules, conducting compliance reviews, and investigating complaints. HHS, on the other hand, is involved in developing and updating HIPAA regulations and ensuring equal access to health and human services. 

**From a CISSP perspective, HIPAA is: ‚úî A U.S. federal law protect¬≠ing PHI, Protected Health Information (PHI) includes:
	-Medical records
	-Billing data
	-Diagnoses
	-Lab results
	-Any data that can identify a patient
**HIPAA aligns to the ‚ÄúC‚Äù in GRC ‚Äî Compliance.

A law with three major rules the exam focuses on:
**Privacy Rule** ‚Äì defines when PHI can/cannot be used or disclosed
**Security Rule** ‚Äì protects electronic PHI (ePHI) via administrative, physical, and technical controls
**Breach Notification Rule** ‚Äì requires timely disclosure to affected individuals + HHS (US Department of Health and Human Services)
**Covered entities:** (hospitals, insurers, clinics, providers, plans, clearinghouses, doctors, pharmacies)
**Business associates:** (cloud vendors, billing companies, MSPs, labs) - any person/org/services that works with entities that handles PII/PHI

‚úîPrivacy Rule...
		- Governs: Use & disclosure of PHI, Minimum necessary requirement, Patient rights (access, correction, restrictions)
		**CISSP mindset: Privacy Rule = policy & governance.

‚úî Security Rule 
		**Requires administrative, physical, and technical safeguards for ePHI:
		**Administrative Safeguards
		-	Risk assessments
		-	Workforce training
		-	Policies/program oversight
		**Physical Safeguards
			Facility access
			Device/media controls
		**Technical Safeguards
		-	Access control
		-	Audit logs
		-	Encryption (addressable, not mandatory)
		-	Integrity controls
		**CISSP mindset: Security Rule = technical + operational security controls.**	
‚úî Breach Notification Rule ...If PHI is breached:
		-	Notify affected individuals without unreasonable delay
		-	Notify HHS
		-	Notify media if > 500 individuals affected
		**	CISSP mindset: Breach = immediate reporting + incident response. CISSP will NEVER choose ‚Äúwait for investigation to complete before notifying.‚Äù
How CISSP Wants You to Think About HIPAA
Use these principles:
‚úî Compliance requirements override convenience
‚úî Patient rights and privacy always come first
‚úî Administrative controls are equally important
‚úî Risk assessment is the foundation of HIPAA Security Rule
‚úî Breach notification must be prompt
‚úî PHI = highly regulated, no exceptions
 
 PCI DSS
 
 COBIT -	 COBIT (Control Objectives for Information and Related Technology) is a framework developed by ISACA for IT management and governance. It's a toolset that allows managers to bridge the gap between control requirements, technical issues, and business risks. It's the correct answer in this case because one of the objectives of COBIT is to help businesses create optimal value from IT by maintaining a balance between realizing benefits and optimizing risk levels. Therefore, it can be used for self-directed risk management. 

 CSA START
 FedRAMP (Federal Risk and Authorization Management Program)  .. 3PAO
 FISMA
 GDRP
 ITIL
 
COSO Internal Control Framework
Focuses on enterprise internal controls and risk management.
Often referenced for financial systems and compliance.


SOC 1 / SOC 2 (AICPA)/SOC3 / each with type 1/2
**SOC = System and Organization Controls.. They are audits performed under AICPA SSAE 18 standards.
** AICPA = American Institute of Certified Public Accountants (AICPA) Auditing Standards Board.
**SOC reports are third‚Äëparty independent audits
**They support **vendor due diligence, supplier risk, and compliance assurance**
**SOC is NOT security certification (they are attestation, not certification)
**SOC reports are a GRC artifact, not a technical control
**Type 1 - A point in time report ... completedthe first time to get SOC certification ... ‚ÄúAre the controls designed properly now?‚Äù
**Type 2 - conducted annually , show how controls operated over a period of time typically 6-12 months ... ‚ÄúDo the controls actually work during this period?‚Äù ... **CISSP perferred type 2 for RMF** 

‚û°	SOC 1 = FINANCIAL
	Sections: 1. Auditor‚Äôs Report / 2. Management Assertion / 3. System Description/ 4. Control Objectives (financial oriented)/ 5. Controls and Tests / 6. Results / Exceptions
	CISSP GRC Tie‚ÄëIns:  Vendor financial risk, Third‚Äëparty assurance, Governance requirements for financial systems, SOX compliance evidence

‚û°‚û°SOC 2 = SECURITY .............**MOST important for CISSP**
Purpose
Evaluate controls relevant to the Trust Services Criteria (TSC):

Security (mandatory)
Availability
Processing Integrity
Confidentiality
Privacy

SOC 2 Sections
Auditor‚Äôs Opinion
Management Assertion
System Description
Trust Services Criteria
Controls & Test Procedures
Results / Exceptions

CISSP GRC Tie‚ÄëIns
Cloud vendor risk assessments
Privacy compliance audits
Security governance assurance
Contractual due diligence
Supply chain risk management


‚û° SOC 3 = PUBLIC‚ÄëFACING SECURITY SUMMARY
Purpose
A public, high-level version of SOC 2 with no confidential details.
Used For
Marketing
Publishing on website
Easy proof of security posture
CISSP GRC Tie‚ÄëIn:  Used for governance transparency and stakeholder assurance, but not acceptable for vendor risk assessments.

SOC 1 is Financial reporting controls (ICFR) for Auditors, CFO, regulators
SOC 2 focuses on security, availability, confidentiality, integrity, privacy. need registered CPA firms/CPAs to conduct audits and issue reports for **security and customer teams** ....
SOC 3 for Public summary of SOC 2 for giving to Public, marketing, customers

 SOC 1 vs SOC 2
Your organization is evaluating a payroll vendor that processes financial transactions. Which report provides the MOST relevant assurance?
‚úî Correct Answer: SOC 1 Type II
‚û° CISSP Logic:
Financial reporting = SOC 1.
Assurance = Type II.
 
 CISSP Exam Traps to Avoid
‚ùå Trap 1: ‚ÄúSOC 1 is about security‚Äù
No ‚Äî SOC 1 = financial.
‚ùå Trap 2: ‚ÄúSOC 3 is enough for due diligence‚Äù
Never. It is not acceptable for risk assessments.
‚ùå Trap 3: ‚ÄúType I is as strong as Type II‚Äù
Type II is always stronger.
‚ùå Trap 4: ‚ÄúSOC reports certify security‚Äù
They do not certify. They attest.
‚ùå Trap 5: ‚ÄúSOC 2 always includes all TSC‚Äù
Only Security is mandatory.
 
 
 : The Common Criteria for Information Technology Security Evaluation (Common Criteria or CC) is an international standard (ISO/IEC 15408) for computer security certification
 ISO / IEC (International Standards)
 SO/IEC 27001 is an information security standard that is part of the ISO/IEC 27000 family of standards. It's used for the establishment, implementation, maintenance, and continual improvement of an information security management system (ISMS). This framework is focused more on managing information security
 ISO 27001
 ISO 27002: 
 ISO 27002 is a well-established international standard that provides best practice recommendations on information security controls, making it a suitable reference for listing security controls in application development.
ISO/IEC 27001
Specifies requirements for an Information Security Management System (ISMS).
Focuses on governance, policy, risk management, and continual improvement.
ISO/IEC 27002 
Provides best-practice security controls to support ISO 27001.
Answers ‚ÄúHow do we implement security controls?‚Äù
ISO/IEC 27005
Risk management framework aligned with ISO 27001.
Focuses on identifying, analyzing, and treating information security risks.
 Federal Information Processing Standards (FIPS) are publicly announced standards developed by the United States Federal government for use in computer systems by all non-military government agencies and by government contractors. FIPS standards specify certain requirements for IT products in terms of their security functionalities and capabilities. FIPS 140-2, for instance, is a widely recognized standard for cryptographic modules

 when evaluating a security product according to ISO/IEC 15408-1
primary evaluation criterion, as it assesses the security features provided by the product. Functionality refers to the specific set of security functions that the product is designed to perform. These functions are evaluated against a set of specified security objectives to ensure that the product adequately addresses the identified threats and risks. 
 ISO 15489-1 emphasizes the importance of ensuring the confidentiality of records by implementing appropriate access controls.

SABSA is more geared towards designing and implementing security architecture based on business requirements, and while risk management is a part of this


 Common Weakness Enumeration (CWE).

Brute force.
Smurf attacks.
Reply attack vs MIM attack
 preponderance of the evidence
Kerberos 
 The user sends a request for a service ticket to the authentication server: In the Kerberos authentication process, the first step involves the user (or client) sending an authentication request to the Kerberos authentication server. This is to obtain a Ticket Granting Ticket (TGT), which is then used in subsequent steps to access services.

(APT) group with a known modus operandi.
 An access control list (ACL)  vs RBAC
 
 syntactic validation
  Regular expressions, also known as regex, are a fundamental tool in computer science and programming for pattern matching within text. They're primarily used to detect specific sequences of characters and are very useful in tasks related to parsing and validating the syntax of text. Syntactic validation involves verifying that the structure of data matches a specific pattern or sequence, and regular expressions are primarily used for this purpose. For example, a regular expression can be used to check if a user's input matches the format of an email address, a phone number, a date, and so forth. Regular expressions are the primary method of syntactic validation because they allow for precise and flexible definition of what constitutes valid syntax. 

domain 3
Cipher Block Chaining (CBC)
Cipher Feedback (CFB)
Counter Mode (CTR)
Output Feedback (OFB)

Correct Answer: B) Establishing a dedicated patient notification protocol

Explanation:

Establishing a dedicated patient notification protocol should be prioritized to improve the effectiveness of the incident response in a healthcare organization. In the event of a data breach involving patient information, healthcare providers are often legally required to notify affected individuals within a specific timeframe. A dedicated notification protocol ensures that the organization can quickly and effectively communicate with patients about the breach, what information was involved, and what steps are being taken to address the incident. This not only helps in maintaining transparency and trust with patients but also ensures compliance with health information privacy regulations, such as HIPAA in the United States.

Incorrect Answer Explanations:

A) Accelerating the escalation process to senior management is important for decision-making and resource allocation, but it does not directly address the need to communicate with affected individuals.

C) Integrating a forensic analysis phase is crucial for understanding the breach and preventing future incidents, but it is secondary to the immediate requirement of patient notification.

D) Creating a specialized response team for healthcare data incidents can improve response capabilities, but without a protocol for patient notification, the organization may still fail to meet regulatory obligations.

domain 4

to learn from chatgtp about different type of firewall and which situation are best for which type in cissp context ......

The primary purpose of a DMZ, or Demilitarized Zone, is to provide an additional layer of security by separating the internal network from the internet. This is achieved by creating a subnetwork where servers that need to be accessed from the internet (like web servers or email servers) are placed. In case these servers are compromised, the intruder still doesn't have direct access to the internal network because of the DMZ. The incorrect answers: While a DMZ does add a layer of security, its purpose is not specifically to secure communication between different internal networks. For securing such communications, techniques like VPNs (Virtual Private Networks), encryption, and internal firewalls are more appropriate. The main goal of a DMZ isn't to provide internet access for internal users, but rather to protect the internal network from external threats. Internet access for internal users can be managed through the use of firewalls and content filtering without necessarily needing a DMZ. To provide a secure environment for hosting public-facing web servers is partly correct in that a DMZ can host public-facing servers, but the primary purpose isn't to provide a secure environment for these servers, but rather to protect the internal network from potential attacks originating from these public-facing servers.

password Authentication Protocol (PAP) Explanation: The Password Authentication Protocol (PAP) is a simple authentication protocol that sends a username and password pair across the network in plaintext (unencrypted). PAP is considered static because it does not involve a challenge-response mechanism or any form of encryption, making it compatible with systems that require a straightforward authentication method without additional complexity.
A. Routing Information Protocol (RIP) is a routing protocol used to exchange routing information within a network, not an authentication protocol.   
C. Challenge Handshake Authentication Protocol (CHAP) is a more secure protocol that uses a challenge-response mechanism for authentication, which is considered dynamic, not static.   
D. Point-to-Point Tunneling Protocol (PPTP) is a VPN protocol that can encapsulate PPP frames into IP datagrams for transmission over an IP-based network. PPTP can use different authentication methods, including PAP and CHAP, but it is not itself an authentication protocol.   

Correct Answer: C) Integrating the Online Certificate Status Protocol (OCSP)

Explanation:

Integrating the Online Certificate Status Protocol (OCSP) should be prioritized to streamline the validation process for digital certificates within the agency. OCSP allows for real-time checks of a digital certificate's revocation status, providing a faster and more efficient method than traditional Certificate Revocation Lists (CRLs). This is particularly important for a government agency where the timely validation of digital identities is critical for maintaining secure communications and operations. OCSP reduces the overhead associated with downloading and parsing CRLs, thereby enhancing the overall performance and responsiveness of the PKI.

Incorrect Answer Explanations:

A) Deploying a Certificate Authority (CA) is a fundamental part of a PKI, responsible for issuing and managing digital certificates, but it does not directly address the efficiency of certificate validation.

B) Implementing Certificate Revocation Lists (CRLs) provides a list of revoked certificates, but CRLs can become large and unwieldy, leading to slower validation times compared to OCSP.

D) Establishing a Registration Authority (RA) assists with the vetting and registration of entities before they are issued certificates by the CA, but it does not improve the efficiency of certificate status checking.

Domain
Communications and Network Security

domain 5 IAM
Adopting a data-centric security model with tokenization

Explanation:

Adopting a data-centric security model with tokenization should be the priority to ensure data security and regulatory compliance across diverse environments. A data-centric approach focuses on securing the data itself rather than the surrounding infrastructure. Tokenization replaces sensitive data elements with non-sensitive equivalents, known as tokens, which have no exploitable value. This method allows the secure processing and storage of proprietary research data while maintaining its usability for analysis and collaboration. Tokenization also helps in meeting various regulatory requirements by reducing the risk of data exposure, as the original sensitive data does not need to be stored or processed in its raw form.

Incorrect Answer Explanations:

A) Centralizing all data storage can create a single point of failure and may not address the regulatory requirements of different countries regarding data residency and sovereignty.

B) Implementing distributed ledger technology like blockchain can provide transparency and immutability but may not be the most efficient or compliant solution for proprietary research data that requires confidentiality and access controls.

D) Utilizing a hybrid cloud model with data residency controls can offer flexibility and compliance with data residency laws, but it does not inherently protect the data itself as effectively as tokenization does within the data-centric security model.

domain 6
 automated breach and attack simulations (BAS) with continuous compliance checks provides the most comprehensive insights. BAS can simulate a wide range of attack scenarios, both external and internal, 24/7, giving a holistic view of the organization's vulnerabilities. Continuous compliance checks ensure that the organization is adhering to all necessary regulatory requirements consistently, not just at a single point in time. This approach not only identifies vulnerabilities but also verifies that the company is meeting its regulatory obligations, making it the most comprehensive and actionable approach for this scenario.

domain 8
positive vs negative list 
 A positive list only allows certain IP addresses, while a negative list blocks them: In cybersecurity, a positive list (also known as a whitelist) is a list of entities that are provided explicit permission to access a specific system or network. For instance, a positive list of IP addresses would mean that only those specific IP addresses on the list are allowed to access a system, while all others would be denied. In contrast, a negative list (often referred to as a blacklist) is a list of entities that are explicitly denied access. A negative list of IP addresses would block access to those specific IP addresses, but all others would be allowed   the concept of positive and negative lists can be applied to email addresses in some scenarios (like email filters), the question specifically asks about the main difference in the context of cybersecurity. The more common application in cybersecurity is with IP addresses, not email addresses
  
 domain 7 
   In a chain of custody log, you should never record any modifications made to the original evidence. The chain of custody must maintain the integrity of the evidence; this means that the evidence should not be altered or tampered with in any way.
  
   Recovery Point Objective (RPO)
   Maximum Tolerable Downtime (MTD), 
   
   The type of information compromised in the breach: The most important factor to consider when reporting a breach is the type of information compromised. This is because different types of data have different legal, regulatory, and compliance requirements when it comes to breach reporting. For example, breaches involving personal identifiable information (PII), health information, or financial data are subject to specific laws and regulations. Understanding the type of data compromised can help assess the potential impact of the breach on individuals and the organization, guiding the subsequent response actions and mitigation strategies
  
  domain 7 - prevent ransomware by backup ??
implementing robust backup and recovery processes: This is the MOST effective way to prevent ransomware attacks. While it may not prevent the initial attack, having regular, secure backups can dramatically mitigate the effects of a ransomware attack by allowing a system or data to be restored without paying the ransom. It's crucial to ensure that backups are not connected to the systems they're backing up, as some ransomware can also infect connected backup systems

 domain name phishing
  cybersquatting and typo squatting threats.
  
  domain 7
  RAID 5: Block level striping with distributed parity, requires at least 3 disks. Combined speed with redundancy.
  
  domain 7
  Unallocated space refers to portions of the hard drive that are not currently assigned to hold data. An intelligent attacker might exploit this by marking a section of the drive as unallocated to conceal malicious software. This area isn't frequently scanned by standard antivirus or anti-malware software, making it a more likely hiding spot for malicious software. However, a comprehensive forensic analysis would include the scanning of unallocated space, increasing the chance of discovering any hidden threats. 

. Slack space is the area at the end of a file, up to the end of the final disk cluster used by that file. It could contain fragments of deleted or old files, including potentially malicious software, but it's not the most common place for a sophisticated perpetrator to hide malware due to its limited space and the fact that some forensic tools will also scan this area. Although the metadata area holds valuable information about system files and can be a useful source of evidence in a forensic investigation, it is not the most likely place to find hidden malware. An advanced attacker would likely avoid placing malware in this area as changes in the metadata can raise flags during system audits and analysis.

. Slack space is the area at the end of a file, up to the end of the final disk cluster used by that file. It could contain fragments of deleted or old files, including potentially malicious software, but it's not the most common place for a sophisticated perpetrator to hide malware due to its limited space and the fact that some forensic tools will also scan this area. Although the metadata area holds valuable information about system files and can be a useful source of evidence in a forensic investigation, it is not the most likely place to find hidden malware. An advanced attacker would likely avoid placing malware in this area as changes in the metadata can raise flags during system audits and analysis.

 Initiating containment procedures to prevent the threat from spreading to other systems is the most appropriate step after detecting an attack. Containment strategies, such as network segmentation, can isolate the affected systems and limit the damage caused by the attack. This is crucial in managing an APT, which often aims to move laterally through a network after gaining initial access. The incorrect answers: Additional intrusion detection systems may help detect further malicious activities in the network, but it's not the immediate next step after detecting an attack. The primary concern should be to contain the ongoing attack to prevent it from spreading to other systems. Although in some cases, powering down a system might be a valid containment strategy, doing so without attempting to capture more details about the attack might lead to the loss of valuable forensic data. Also, in the case of an APT, an immediate shutdown might alert the attacker, who might then alter their tactics, making future detection and mitigation more difficult. Internal communication is an important part of incident management, but it's not the immediate next step after detecting an attack. Such a communication should also be carefully planned and executed to avoid unnecessary panic and confusion among employees. The immediate focus should be on technical measures to contain the threat.

In CISSP Disaster Recovery (DR) context, MTD and MAD are easy to mix up‚Äîbut they answer different survival questions.

üîπ MTD ‚Äî Maximum Tolerable Downtime

Question it answers:

‚ÄúHow long can this business process be down before the organization suffers unacceptable damage?‚Äù

Key points:

Business-driven

Set by senior management / BIA

Includes all impacts: financial loss, reputation, legal, safety

Defines the upper limit for recovery

üëâ Think: absolute pain threshold

üîπ MAD ‚Äî Maximum Allowable Downtime

Question it answers:

‚ÄúWhat downtime limit can IT realistically design recovery solutions to meet?‚Äù

Key points:

IT / technical-driven

Constrained by technology, cost, and complexity

Must always be ‚â§ MTD

Used to design DR strategy and controls

üëâ Think: engineering constraint

üîÅ Relationship (EXAM GOLD)
MAD ‚â§ MTD


If MAD > MTD ‚Üí ‚ùå Business fails (IT can‚Äôt meet business needs)

üîπ Where RTO fits (for exam clarity)

RTO (Recovery Time Objective) is usually set at or below MAD

RTO is what DR plans are actually tested against

RTO ‚â§ MAD ‚â§ MTD

üß† CISSP Memory Trick

MTD = Management Tolerance

MAD = Admin (IT) Design Limit

üìù Quick Example

Online trading platform

MTD: 2 hours (after that ‚Üí regulatory + financial disaster)

MAD: 90 minutes (best IT can guarantee)

RTO: 60 minutes (chosen target)

‚úÖ Passes CISSP logic

‚ö†Ô∏è Common CISSP Traps

MTD is NOT set by IT

MAD is NOT a business decision

You never choose a solution where MAD exceeds MTD


domain 8
 The procurement process involves activities such as creating a business case, researching and selecting vendors, and involving stakeholders in decision-making. This process aligns the requirements of the organization with the capabilities of potential vendors to ensure that the procured goods or services will fulfill the needs of the business. It often involves a formal bidding or negotiation process with potential vendors and requires approval from relevant stakeholders. The incorrect answers: Requirements elicitation is the process of gathering requirements for a system from users, customers, and other stakeholders. While it involves stakeholders, it does not typically involve building a business case or researching vendors. Risk assessment involves identifying, analyzing, and evaluating risks that could potentially impact the success of a project or business goals. It's a critical part of any project management strategy, but it doesn't typically involve building a business case or researching vendors. Development and testing involve the actual creation of a system or product and the subsequent testing to ensure it meets the identified requirements and is free from defects. While stakeholders might be involved in providing feedback during this process, building a business case and researching vendors are not part of this process.

Overall explanation
The correct answer: Agile is a project management approach that emphasizes flexibility, collaboration, and rapid iteration in the development process. In an Agile environment, development work is broken down into smaller pieces, each of which can be developed, tested, and integrated regularly, even several times a day. It is well-suited for CI and CD because it allows for frequent code updates and testing, and encourages the use of automated tools to streamline the integration and delivery process. The incorrect answers: Waterfall is a linear, sequential project management approach that emphasizes strict planning and defined stages of development where one stage should be completed before moving on to the next. This approach doesn't align with the principles of CI/CD, which require frequent iterations, testing, and deployment of code changes. Sashimi is a project management term originating from a variation of the Waterfall model, often used in software development. While the Sashimi model introduces overlapping phases (thereby increasing communication and reducing the isolation of phases as compared to the pure Waterfall model), it's not inherently designed to accommodate the frequent integration and delivery required in CI/CD practices. TOGAF (The Open Group Architecture Framework) is a framework for enterprise architecture that helps organizations design, plan, implement, and govern their IT systems. It is not inherently designed for or tied to software development processes like CI/CD.

Extreme Programming (XP) is a type of software development methodology that emphasizes collaboration between teams, frequent code testing, and constant feedback. One of the key principles of XP is pair programming, where two developers work together on the same code, with one person writing the code and the other reviewing it. 

 The Waterfall model is a sequential software development process, in which progress is seen as flowing steadily downwards, hence "waterfall", through the phases of requirements analysis, design, implementation, testing, integration, and maintenance. Pair programming is not a specific part of the Waterfall methodology. Scrum is an Agile framework that is used for managing knowledge work, with an emphasis on software development. It is designed for small teams (about three to nine people) who break their work into goals that can be completed within time-boxed iterations. While collaboration is emphasized in Scrum,

The correct answer: Cross-Site Scripting. If Rocket Raccoon does not properly filter out HTML tags from the input in the code for the Milano's communication system, he could potentially introduce a Cross-Site Scripting (XSS) vulnerability. XSS vulnerabilities occur when an application includes untrusted data in a new web page without proper validation or escaping, allowing an attacker to inject malicious scripts. The incorrect answers: Buffer Overflow: Buffer overflow vulnerabilities occur when a program writes more data to a buffer than it should, thereby overflowing the buffer's boundary and overwriting adjacent memory. The scenario given does not suggest any misuse of memory space. Insecure Direct Object References: This type of vulnerability occurs when a developer exposes a reference to an internal implementation object. The scenario does not suggest this is the case. Injection: Injection vulnerabilities arise when untrusted data is inserted into a system and then processed. In this scenario, the unfiltered HTML tags would more likely result in a Cross-Site Scripting vulnerability, not an injection vulnerability.

The Delphi Technique is the most appropriate methodology for gathering input and consensus from a diverse group of stakeholders on the most effective security measures to implement. This technique is an iterative process used to collect and distill the anonymous judgments of experts using a series of data collection and analysis techniques. It is designed to combine opinions from diverse groups to converge towards the "correct" response. The process begins with a survey or questionnaire and incorporates multiple rounds of polling, with a facilitator summarizing the anonymous responses from the previous round and asking participants to reassess their views based on this feedback. The goal is to reduce the range of responses and arrive at something closer to expert consensus. This is particularly useful when dealing with complex problems like cybersecurity measures, where diverse stakeholder inputs and consensus are critical. The incorrect answers: While brainstorming sessions can be useful in generating a variety of ideas, they don't always facilitate consensus. They also tend to favor those with more assertive personalities or higher positions, potentially silencing other voices or ideas. Furthermore, ideas generated in brainstorming sessions can sometimes be surface level or uninformed without proper structure and expert guidance, and there's less emphasis on refining and converging ideas, which is essential for addressing complex issues like cybersecurity. A focus group discussion is another way to gather input, but it doesn't usually reach a consensus as effectively as the Delphi Technique. Like brainstorming, focus groups may be dominated by the loudest voices, not necessarily the most knowledgeable. They also typically involve a smaller number of participants, which can limit the diversity of input. Surveys and questionnaires are excellent for collecting information from a large group of people. However, they're less effective at driving consensus among the group members. Without the iterative feedback and adjustment process present in the Delphi Technique, surveys and questionnaires provide limited opportunity for stakeholders to understand others' viewpoints and refine their own based on that understanding.

 The parent table's primary key is seen as the foreign key in the child table. In a relational database, relationships between tables are established through primary and foreign keys. A primary key in one table (parent table) is used as a foreign key in another table (child table) to create a link between the two tables. This relationship allows for efficient data retrieval and helps ensure data integrity. The incorrect answers: The parent table's primary key is seen as the parent key in the child table: This statement is not correct because there is no term like 'parent key' in relational database parlance. The parent table's primary key is seen as the child key in the child table: Again, this statement is incorrect as there is no such term as 'child key' in the context of relational databases. The parent table's primary key is seen as the sibling key in the child table: There is no such term as 'sibling key' in the context of relational databases. The term 'sibling' typically refers to nodes at the same level in a tree data structure, not to keys in a relational database.
 
 Given the scale and complexity of the IT transformation project, it is imperative to have a unified security framework that extends across the entire portfolio. This means a common set of security protocols, standards, and controls that are uniformly implemented across all projects and programs in the portfolio. This provides a consistent security baseline, facilitating inter-project communication and integration, and enabling efficient identification and mitigation of potential security risks. Having a unified security framework also enables efficient resource allocation, since you can centrally manage and optimize the security measures for the entire portfolio, rather than separately for each project. This leads to more efficient use of resources, reduces duplicative efforts, and ensures consistency in the level of security across all sub-projects. The incorrect answers: Although regularly reviewing and updating security controls for each sub-project is important, it doesn't address the overarching need for a unified approach to security across the entire portfolio. In fact, without a unified security framework, you might end up with inconsistent security controls between sub-projects, which could create vulnerabilities and inefficiencies. An IPT can be instrumental in aligning the objectives and efforts of different stakeholders, but its formation doesn't directly ensure the security of the IT transformation project. The IPT would be more concerned with the strategic and operational aspects of the project, while the establishment and enforcement of a unified security framework is a more direct and effective strategy to ensure comprehensive security. Identifying and assessing the potential security risks of the project is a part of the process of ensuring the project's security, but without a unified security framework, the identified risks might not be adequately or consistently mitigated across the entire portfolio. The risk assessment should be done within the context of a comprehensive security framework that spans the whole portfolio.
 
****NISTs*****
NIST CFS 
NIST CSF = Identify ‚Üí Protect ‚Üí Detect ‚Üí Respond ‚Üí Recover
It is NOT a technical standard, a certification nor mandatory (unless referenced by law or contract).
It IS:
	-	A risk-based, voluntary framework
	-	Designed for critical infrastructure, but usable by any org
	-	Mapped to other frameworks (ISO 27001, COBIT, etc.)
	-	High-level, management-friendly
	-	outcome driven
Use the acronym: I P D R R
üëâ ‚ÄúI Prepare DRR (Disaster Ready Response)‚Äù
or
üëâ ‚ÄúI Protect Data, Respond & Recover.‚Äù
You will see this flow repeatedly in questions.
 
  The NIST (National Institute of Standards and Technology) Cybersecurity Framework is a policy framework of computer security guidance for how private sector organizations in the United States can assess and improve their ability to prevent, detect, and respond to cyber attacks. Although it includes risk management elements
	
NIST SP 800-39	- framework any companies can use to assess risks ... "a holistic view rather than a checklist view ", 3 tiers, organization/mission-business/info-system, risks must be addresses in each tiers
NIST 800-37
NIST SP 800-53 is a comprehensive catalog of security and privacy controls for all U.S. federal information systems (except those related to national security) and is directly focused on listing such controls, making it highly relevant for application development.

**NIST Special Publication 800-53A, security labeling and marking of asset**
The term security labeling refers to the association of security attributes with subjects and objects represented by internal data structures within organizational information systems, to enable information system-based
enforcement of information security policies. 
The term security marking refers to the association of security attributes with objects in a human-readable form, to enable organizational process-based enforcement of information security policies


NIST SP 800-53A: 
NIST SP 800-53A provides guidelines for assessing the effectiveness of security controls, and while it is not the primary resource for listing controls, it is related to the implementation of the controls listed in NIST SP 800-53, which itself is applicable for the development process.

üá∫üá∏ NIST (U.S. Government / Industry)
NIST Cybersecurity Framework (CSF)
Risk-based framework organized into Identify, Protect, Detect, Respond, Recover.
Used to assess, improve, and communicate cybersecurity posture.
NIST SP 800-53
Comprehensive catalog of security and privacy controls for federal and enterprise systems.
Answers ‚ÄúWhat controls should we implement?‚Äù
NIST SP 800-37 (RMF)
Defines the Risk Management Framework lifecycle (categorize ‚Üí authorize ‚Üí monitor).
Used heavily in government system authorization (ATO).
NIST SP 800-61
Computer Security Incident Handling Guide.
Defines incident response phases: Preparation, Detection, Containment, Eradication, Recovery.
NIST SP 800-30
Risk assessment methodology focusing on threats, vulnerabilities, likelihood, and impact.
Supports both qualitative and quantitative risk analysis.

NIST SP 800-88 revision 1 provides guidelines for media sanitization 


Establishing appropriate controls for security and availability: The SOC 2 (System and Organization Controls) audit is a type of audit designed to assess a service organization's systems in terms of their security, availability, processing integrity, confidentiality, and privacy. The primary focus of a SOC 2 audit is on the organization's non-financial reporting controls as they relate to the security and availability of a system. The primary indicator that a company has met the requirements of a SOC 2 audit is its establishment of appropriate controls for security and availability. These controls must be designed effectively and operating as intended to pass the audit. The incorrect answers: Although implementing strong password policies is indeed a best practice in cybersecurity and could be part of the controls evaluated in a SOC 2 audit, it is not the primary indicator that a company has met the audit's requirements. SOC 2 audits are broader and include controls that relate to the entire system‚Äôs security and availability, not just the strength of password policies. While important, strong password policies alone do not guarantee a successful SOC 2 audit. Having a comprehensive data backup plan is certainly a vital part of any organization's disaster recovery strategy and can contribute to the availability aspect of SOC 2. Still, it is only one of many aspects of a system that are evaluated in a SOC 2 audit. A successful audit requires an organization to demonstrate effective controls across a range of areas, including security, availability, processing integrity, confidentiality, and privacy. Regular risk assessments are an integral part of maintaining a secure and reliable system. They help organizations identify potential threats and vulnerabilities, assess their impact and likelihood, and prioritize mitigation strategies. Regular risk assessments alone do not indicate that a company has met all the requirements of a SOC 2 audit. The audit examines how well an organization has established and is operating controls in numerous areas, not just its ability to identify and assess risk.

System Organization Controls (SOC) reports
Over the years, audit standards have evolved and matured over the years from SAS70 ‚Üí SSAE 16 ‚Üí SSAE 18.

The SSAE 18 standard defines three types of Service Organization Controls (SOC) reports:

SOC 1 reports are quite basic and focus on financial reporting risks.
SOC 2 reports are much more involved and focus on the controls related to the five trust principles: security, availability, confidentiality, processing integrity, and privacy.
SOC 3 reports are stripped-down versions of SOC 2 reports‚Äîtypically used for marketing purposes.
There are also two types of SOC 1 and SOC 2 reports:

A Type 1 report focuses on the design of controls at a point in time.
A Type 2 report examines not only the design of a control but, more importantly, the operating effectiveness over a period of time, typically a year.
From the types of reports defined, it should be clear that SOC 2, Type 2 are the most desirable reports for security professionals, as they report on the operating effectiveness of the security controls at a service provider over a period of time.

ISO 31000 - ISO 31000 is a family of standards relating to risk
management. The scope of ISO 31000 is to provide best
practice structure and guidance to all organizations
concerned with risk management.
ISMS
COSO - COSO provides a definition to essential enterprise risk
management components, reviews ERM principles and
concepts, and provides direction and guidance for
enterprise risk management.

**NIST RMF 800-37 Rev2** - This guide describes the risk management framework (RMF) and provides guidelines for applying the RMF to information systems and organizations.
Prepare to execute the RMF - In this step, information systems are identified and categorized. 
Select Security Controls
Implement Security Controls
Assess Security Controls
Authorize Information System
Monitor Security Controls



ISO/IEC 27001
Enterprise security governance
ISACA Risk IT Framework - ISACA‚Äôs Risk IT Framework contains guidelines and
practices for risk optimization, security, and business
value. The latest version places greater emphasis on
cybersecurity and aligns with the latest version of COBIT.


Three of the most popular enterprise security architectures are 
Zachman, Sherwood Applied Business Security Architecture (SABSA), and The Open Group Architecture Framework (TOGAF).



 The Graham-Denning model is a framework for identifying and selecting appropriate security controls for an organization. It takes into account the organization's goals, threats, and vulnerabilities to determine the most effective controls. 

 GLBA (Gramm-Leach-Bliley Act)-  Establishing safeguards for financial institutions -  requires financial institutions to develop, implement, and maintain a comprehensive information security program to protect the privacy and security of customers' nonpublic personal information. 
 This includes ensuring customer privacy, implementing cybersecurity measures, and adopting other administrative, technical, and physical safeguards.


 Security Models (Very Exam-Heavy)
Bell-LaPadula

Confidentiality model enforcing No Read Up, No Write Down.

Used in military and classified environments.

Biba Model

Integrity model enforcing No Read Down, No Write Up.

Protects data accuracy and trustworthiness.

Clark-Wilson ... Integrity model using well-formed transactions and separation of duties. Common in commercial and financial systems.

Brewer-Nash (Chinese Wall) .. Prevents conflicts of interest dynamically. ...Access depends on prior access history.
Graham-Denning ....Formal model defining secure creation and deletion of subjects/objects. ...Focuses on access rights management.

üîê Cryptographic & Trust Models ...Public Key Infrastructure (PKI)
Framework for managing digital certificates and trust.
Enables authentication, encryption, and non-repudiation.

Web of Trust
Decentralized trust model used by PGP.
Trust is established through peer endorsements.

üß© Software & Development Models
SDLC (Systems Development Life Cycle)

Structured approach to system development from requirements to disposal.

Security must be integrated in every phase.

DevSecOps

Integrates security into DevOps pipelines.

Emphasizes shift-left security and automation.

üèõÔ∏è Privacy & Data Protection Frameworks
GDPR

EU data protection regulation governing personal data processing.

Emphasizes consent, data subject rights, and breach notification.

HIPAA Security Rule

Protects electronic protected health information (ePHI).

Focuses on administrative, technical, and physical safeguards.
 
 
 
 ------------------------------------------
 
 
 
 Software Configuration Management (SCM) 
 involves tracking and controlling changes in the software, part of which includes maintaining the records of what versions of each software component are part of a specific build or release. SCM tools help manage different versions and ensure that the configuration of the product is known and reproducible. 


CN=Francis Smith, OU=Sales, DC=ISC2, DC=org
The best way for Thor to query the LDAP server for a user, Francis Smith, is to use a distinguished name (DN) that includes not just the common name (CN) but also other attributes that will uniquely identify the user within the LDAP directory structure. 

The DN typically includes the organizational unit (OU), domain components (DC), and other relevant attributes. In this case, specifying the user's CN, their OU (as a department or group they belong to), and the domain components (representing the company's domain) will likely provide the needed specificity to retrieve Francis Smith's user attributes from the LDAP server. 

The Recovery Time Objective (RTO) should be your primary focus given the nature of the hospital's patient records systems. 
The RTO represents the duration of time within which a business process must be restored after a disaster (or disruption) to avoid unacceptable consequences associated with a break in business continuity. 

In this scenario, any significant disruption in access to patient records can lead to life-threatening situations, so the aim should be to restore access to these systems as quickly as possible, making RTO the most critical metric. 

 The Recovery Point Objective (RPO) is the maximum targeted period in which data (transactions) might be lost from an IT service due to a major incident. 
 This is also an important metric, but it may not be as crucial as RTO in this scenario. Though loss of data can also lead to severe problems, the immediate priority for the hospital would be to get the patient records systems up and running again to avoid any immediate threats to patients' lives. 
 
 The Maximum Tolerable Downtime (MTD) is the total amount of time that a business process can be disrupted without causing irreparable harm to the business, but in a critical health-care setting like a hospital where downtime can result in life-threatening situations, the MTD might be close to the RTO. Even so, focusing on achieving a very low RTO can ensure that the MTD is not exceeded. 
 
 Work Recovery Time (WRT) is the duration of time it takes to catch up on work not done during an outage. 
 This is important but is less critical in this scenario compared to RTO, given the urgency to get the patient records systems operational again. The priority in this case is to restore the systems and not necessarily to catch up on work not done during the outage.



While the prudent person rule is related to due care, it is a legal concept that typically applies to the management of another's affairs, especially in a fiduciary capacity. It is not specifically a term used to describe the process of setting security goals for an organization. 

Building consensus is about obtaining agreement from various stakeholders or team members. While gaining agreement on security goals is an important part of the CISO's role, it does not represent the minimum standard of care expected of Jane when she determines those goals. Due care is the term that best fits this requirement.


 Data processor: The role most likely given to the company that runs an application to produce reports about data subjects, 
 
 Data custodian: The data custodian is responsible for the safe custody, transport, and storage of the data and implementing the technical environment and database systems. While a data custodian may assist with managing the infrastructure needed to produce reports, the company running the application specifically for reporting is acting as a data processor. Data controller: 
 
 The data controller decides the purposes for which and the means by which personal data is processed. As the VP of Sales, Naomi would generally fill this role herself because she is determining how the data is used

Data steward: A data steward has an oversight role, focusing on ensuring data governance, data quality, and adherence to data management policies and standards. A data steward does not usually run applications to produce reports; their focus is on the overall quality and proper use of the data within the organization.

Server responds on port 443 and continues to listen for Ken's traffic on that port. When a client initiates a TCP three-way handshake, it uses an ephemeral source port (49,123 in this case) to connect to the server‚Äôs well-known port (443). The server then responds from port 443 to the client‚Äôs ephemeral port (49,123) to complete the handshake. Once the connection is established, data flows between the client‚Äôs ephemeral port and the server‚Äôs port 443. The server continues listening on port 443 for any new connection requests while maintaining the established session separately. The keywords in the answer options is on. 

The incorrect answers:

Server responds on port 49,123 and listens for Ken's traffic on port 49,123: Incorrect because the server never uses or listens on the client‚Äôs ephemeral port. That port belongs to the client.

Server responds with an ephemeral port above 49,000 and listens for Ken's traffic on that port: Incorrect because the server always uses its designated service port (443) for incoming connections, not an ephemeral port.

Server responds on port 49,123 and continues to listen for Ken's traffic on port 443: Incorrect phrasing ‚Äî while the server does communicate to the client‚Äôs ephemeral port (49,123), it does so from port 443, not on it.

The correct answer: The ratio of false positives to false negatives: The best metric to use when evaluating the effectiveness of Intrusion Detection Systems (IDSs) is the ratio of false positives to false negatives. This metric provides insight into the accuracy and precision of the IDS, considering both its sensitivity (which could result in false positives, where benign activity is incorrectly flagged as malicious) and its specificity (which could result in false negatives, where actual malicious activity is not detected). A balanced ratio indicates an effective IDS that neither overburdens staff with false alarms nor misses genuine threats. 


Started the authentication sequence by claiming an identity: By entering his username on the system, Thor has initiated the authentication process by claiming an identity. 
A username is a claim of identity that will typically be followed by providing credentials (such as a password) to verify or authenticate that claim. 

The incorrect answers: 
Completed the authentication by claiming an identity: Thor has not completed the authentication process just by entering a username. Authentication involves verifying the claimed identity, usually through additional factors like a password, biometric data, or a security token. 

Started the authentication sequence with something that he knows: While a username is something Thor knows, it is not in itself sufficient to start the authentication sequence. 
Authentication sequences typically require knowledge of a secure credential like a password or PIN, not just a username, which is considered public information. 

Started the authentication sequence with something that anyone can claim: A username on its own is something that anyone can claim, but the phrase does not fully describe the action Thor has taken. The act of entering a username starts the process by which the system identifies who might be trying to authenticate, not just what could be claimed by anyone.


: Design. During the design phase of the Software Development Life Cycle (SDLC), potential threats are analyzed through threat modeling to understand how they could materialize. This helps in building robust security mechanisms in the design of the system. 


Requirements gathering: While potential risks could be considered during the requirements gathering phase, specific identification of how threats could materialize typically happens during the design phase. 

Development: By the development stage, potential threats should already have been identified during the design phase, and mitigations for these threats should be incorporated into the code being developed. 

Testing/Validation: At this stage, the software is tested for defects, including any vulnerabilities from threats identified during the design phase. However, the actual identification of potential threats and their actualization usually happens earlier, during the design phase.


he ratio of false positives to false negatives: The best metric to use when evaluating the effectiveness of Intrusion Detection Systems (IDSs) is the ratio of false positives to false negatives. This metric provides insight into the accuracy and precision of the IDS, considering both its sensitivity (which could result in false positives, where benign activity is incorrectly flagged as malicious) and its specificity (which could result in false negatives, where actual malicious activity is not detected). A balanced ratio indicates an effective IDS that neither overburdens staff with false alarms nor misses genuine threats. 

The incorrect answers: The number of attacks we detect: While the total number of detected attacks can be an indicator of IDS activity, it does not provide comprehensive insight into the system's effectiveness, as it may include false positives and does not account for missed detections (false negatives). 

Conduct a privacy impact assessment: This is the most accurate response as a Privacy Impact Assessment (PIA) is a systematic process used to evaluate the potential effects that a particular activity or process may have on the privacy of individuals. 

The primary purpose of a PIA is to identify and mitigate privacy risks. PIAs can help ensure that the data mining process is conducted in a secure and ethical manner, by determining the nature of the data, how it will be used, stored and protected. 

The assessment can guide the decision-making process on whether to proceed with the data mining project, modify it, or discard it altogether, all while considering privacy laws and regulations.

 The data owner. In this scenario, the data owner is likely to be the best person to work with to ensure that the files are correctly classified and given the proper protection profile. As the person responsible for the data, the data owner would have the best understanding of the data's sensitivity, value, and the potential impact of its loss or misuse.

The Recovery Time Objective (RTO) should be your primary focus given the nature of the hospital's patient records systems. The RTO represents the duration of time within which a business process must be restored after a disaster (or disruption) to avoid unacceptable consequences associated with a break in business continuity. In this scenario, any significant disruption in access to patient records can lead to life-threatening situations, so the aim should be to restore access to these systems as quickly as possible, making RTO the most critical metric. 

The Recovery Point Objective (RPO) is the maximum targeted period in which data (transactions) might be lost from an IT service due to a major incident. This is also an important metric, but it may not be as crucial as RTO in this scenario. Though loss of data can also lead to severe problems, the immediate priority for the hospital would be to get the patient records systems up and running again to avoid any immediate threats to patients' lives. 

The Maximum Tolerable Downtime (MTD) is the total amount of time that a business process can be disrupted without causing irreparable harm to the business, but in a critical health-care setting like a hospital where downtime can result in life-threatening situations, the MTD might be close to the RTO. Even so, focusing on achieving a very low RTO can ensure that the MTD is not exceeded. 

Work Recovery Time (WRT) is the duration of time it takes to catch up on work not done during an outage. This is important but is less critical in this scenario compared to RTO, given the urgency to get the patient records systems operational again. The priority in this case is to restore the systems and not necessarily to catch up on work not done during the outage.

 Internet - FW1 - DMZ - FW2 - OZ - FW3 - RZ: This configuration provides the most secure setup by having a dedicated firewall (FW) for each network zone transition. Traffic from the Internet comes through FW1 to reach the DMZ, then through FW2 to get to the Operations Zone (OZ), and finally through FW3 to access the Restricted Zone (RZ). 
 
 Each firewall enforces policies specific to the traffic allowed in and out of its respective zone, thus creating a layered security approach with clear boundaries between each zone.



Domain 1: Security and Risk Management
**********************************************************
he Chief Information Security Officer (CISO) is responsible for the overall security strategy and program of an organization. This position sits at the top of the IT security hierarchy in most organizations and is responsible for establishing and maintaining the enterprise vision, strategy, and program to ensure information assets and technologies are adequately protected. 

a Data Protection Officer (DPO) plays a crucial role in overseeing data protection strategy and implementation within an organization, they are typically more focused on ensuring compliance with data protection regulations and laws, such as the EU's General Data Protection Regulation (GDPR). The DPO doesn't usually cover all aspects of an organization's security strategy and program. 

The Chief Information Officer (CIO) is an executive role in charge of the overall strategic direction of the company's IT infrastructure and assets. While the CIO does have a role in security, their focus tends to be broader, covering areas like IT budgeting, planning, and personnel, as well as digital transformation strategies. 

The Chief Risk Officer (CRO) is responsible for identifying, assessing, and mitigating risks across the organization. Although these risks can include those related to information security, the CRO's role is broader and includes other types of risks such as operational, financial, reputational, and strategic risks.


 Implementing a risk management framework provides an organized and structured approach to identifying, assessing, and responding to risks. The aim of an RMF is to enable organizations to better understand the nature of the risks they face, make informed decisions about how to address those risks, and manage them in an ongoing, dynamic way. It provides a holistic view of the organization's risk profile, helps establish risk tolerance, and aids in decision-making processes related to risk treatment. The incorrect answers: Identifying and prioritizing vulnerabilities is an important part of risk management but it is not the primary benefit of implementing a risk management framework. The framework provides a comprehensive approach to managing risks including business, strategic, and operational risks, not just identifying vulnerabilities. Although implementing a risk management framework can help an organization maintain compliance with certain laws and regulations, it's not the primary purpose of an RMF. The primary purpose is to manage risks in a systematic and structured way. While compliance is often a beneficial side effect, it is not the main goal. It reduces the need for security controls is incorrect. Implementing a RMF doesn't reduce the need for security controls but rather it aids in determining which controls are necessary and how they should be applied to mitigate specific risks. The risk management process often results in the implementation of additional controls or the strengthening of existing ones to better manage identified risks.

BIA/BCP/DRP /RMF

Risk = Threat x Vulnerability. Impact - Can at times be added to give a more full picture. 
Risk = Threat x Vulnerability x Impact (How bad is it?). 
Total Risk = Threat x Vulnerability x Asset Value. 
Residual Risk = Total Risk ‚Äì Countermeasures.

Single Loss Expectancy (SLE) = Asset Value (AV) x Exposure Factor (EF)
Annualized Loss Expectancy (ALE) = Single Loss Expectancy (SLE) x Annualized Rate of Occurrence (ARO)

**Asset Value (AV)**: The cost of the asset in a monetary
value, e.g., a CCTV system that costs $2,000

**Exposure Factor (EF)**: Measured as a percentage and expresses how much of the asset‚Äôs value stands to be lost in case of a risk materializing
The EF will always be a percentage between 0 to 100 percent.

**Single Loss Expectancy (SLE)**: Denotes how much it will cost if the risk occurs once. To calculate the SLE, simply multiply the AV by the EF:

**Annualized Rate of Occurrence (ARO)**: Denotes how many times each year the risk is expected to occur. For example, if the voltage spikes excessively three times a year, the ARO is 3.

**Annualized Loss Expectancy (ALE):** Expresses the annual cost of the risk materializing.


**Security assessment** is a thorough evaluation of an organization's information security posture, including its policies, procedures, and technical controls. It involves identifying potential vulnerabilities and risks, and determining the appropriate measures to mitigate those risks. Security testing, on the other hand, involves evaluating the effectiveness of an organization's security controls by simulating attacks and attempting to breach the system. 

**Risk management**, meanwhile, involves identifying, assessing, and prioritizing risks to an organization's assets, and implementing strategies to mitigate those risks. 
**Security policy** review, meanwhile, involves reviewing and updating an organization's security policies to ensure they are still relevant and effective.
**Security testing** involves evaluating the effectiveness of an organization's security controls,

Applicable Risk Controls
**A complete control at minimum is a combination of preventive, detective, and corrective controls

1. Directive Controls
Definition: Controls that guide or mandate desired behavior but do not directly stop incidents.
Example: An information security policy requiring strong passwords.

2. Preventive Controls
Definition: Controls that stop or block security incidents before they occur.
Example: Firewall rules blocking unauthorized network traffic.

3. Detective Controls
Definition: Controls that identify and alert on security events after or during occurrence.
Example: Intrusion Detection Systems (IDS) generating alerts on suspicious activity.

4. Corrective Controls
Definition: Controls that fix or remediate issues after an incident has occurred.
Example: Applying security patches after discovering a vulnerability.

5. Deterrent Controls
Definition: Controls that discourage potential attackers through fear of detection or consequences.
Example: Warning banners stating systems are monitored and logged.

6. Compensating Controls
Definition: Alternative controls used when primary controls are not feasible.
Example: Increased logging and monitoring when encryption cannot be implemented.

7. Recovery Controls
Definition: Controls that restore systems and operations after an incident.
Example: Backups and disaster recovery plans used to restore systems after ransomware.

Key CISSP rule:
A control has two dimensions
What it does ‚Üí Directive / Preventive / Detective / etc. 
Directive/Detective/Deterrent/Corrective/Compenseation/Preventive/Recovery ...D3C2PR
How each is implemented ‚Üí Administrative / Technical / Physical

One-Minute CISSP Decision Tree üß†

1Ô∏è‚É£ Does it tell people what to do? ‚Üí Directive
2Ô∏è‚É£ Does it scare or discourage? ‚Üí Deterrent
3Ô∏è‚É£ Does it block the action? ‚Üí Preventive
4Ô∏è‚É£ Does it detect or alert? ‚Üí Detective
5Ô∏è‚É£ Does it fix after damage? ‚Üí Corrective
6Ô∏è‚É£ Is it a backup control? ‚Üí Compensating
7Ô∏è‚É£ Does it restore operations? ‚Üí Recovery

Then ask:
Policy/process? ‚Üí Administrative
System/software? ‚Üí Technical
Building/people? ‚Üí Physical
**Functional and Assurance** - The control should perform some function (e.g., control the flow of network traffic, only allow authorized employees to enter a building, detect smoke from a fire), ....and there should be some means of verifying/obtaining assurance that the control is working effectively on an ongoing basis.


Patents: Protects inventions for 20 years (normally) ‚Äì Cryptography algorithms can be patented. Inventions must be:Novel (New idea no one has had before). Useful (It is actually possible to use and it is useful to someone). Nonobvious (Inventive work involved).
. A patent is a form of intellectual property that gives its owner the legal right to exclude others from making, using, selling, and importing an invention for a limited period of years. Patents are intended to encourage innovation by granting inventors exclusive rights to profit from their inventions. If someone copies a patented invention, the patent owner can sue for patent infringement

Trademark -  A trademark is a recognizable sign, design, or expression which identifies products or services of a particular source from those of others. Trademark law prevents others from using a deceptively similar mark, but it doesn't prevent others from making the same goods or from selling the same goods or services under a clearly different mark. If someone were to copy your work under a different brand, a trademark would not protect you

Copyright/Copyleft -  Copyright is a legal right that grants the creator of an original work exclusive rights to determine and decide whether, and under what conditions, this original work may be used by others. This includes the right to reproduce, distribute, display, perform, transmit, or transform the work. Copyright infringement can lead to legal ramifications.

Trade Secret - A trade secret is information that is not generally known and is kept secret by a company or individual in order to gain an economic advantage. They are not protected by law in the same way as patents or copyrights. Instead, protection of a trade secret is typically handled through an agreement between parties, such as a non-disclosure agreement. If someone outside of these agreements were to independently develop or reverse engineer a product or idea that you consider a trade secret, you would have no legal recourse to stop them from using, manufacturing, or selling it.

Due care: Due care refers to the level of care that an individual would reasonably be expected to exercise in a particular situation. 
As the chief information security officer (CISO) at ThorTeaches.com, Jane is responsible for determining the security goals for the organization, and she must do so with due care. This means she should take reasonable steps to protect the company's data and systems from threats, aligning with industry best practices and regulatory requirements. 

Due diligence is the investigative process conducted to assess a business transaction. When setting security goals, due diligence is part of the broader process Jane would undertake to understand the risks and requirements of the organization, but it is not the term that best describes the minimum standard required of her. 

Intellectual property refers to intangible assets.  This mostly refers to software, proprietary code, and other digital assets owned by the organization, but can also refer to unpublished books, music, art, inventions, patents, trademarks, trades secrets, or other information that should be protected.

Copyright: Creating a photograph or writing a song automatically grants copyright protection without requiring a formal process. The rights to control copies belong to the creator from the moment of creation. In a copyright dispute, parties must provide evidence proving they were the original creator.

Trade Secrets: These are protected due to their commercial value, limited access, and reasonable security measures. Protection is enforced through an organization‚Äôs internal security protocols.

Patents and Trademarks: Unlike copyrights and trade secrets, patents and trademarks require a formal review process, investigation, and associated costs before protection is granted.

When talking about software, the following intellectual property terms apply ‚Äì try to think about what your organization does for its non-proprietary software usage (e.g. MS Word, etc.):

Site license ‚Äì refers to bulk purchases, for all staff within the organization, done for a specific length, and there is typically a cap on the number of licenses
Per-seat license ‚Äì another bulk purchase (number of licenses is chosen rather than having a cap), or pay-per-copy (usually discounted)
Shareware ‚Äì use is allowed with constraints, free for non-commercial use, commercial use would require payment
Public Chapter (previously ‚Äúpublic domain‚Äù) ‚Äì use is allowed without constraints, for any purpose, including modification and customization.  Support and extra features must be purchased.

DRM is a concept that candidates need to be familiar with.  Here are the elements of a DRM solution:

Persistency ‚Äì Access controls follow the material wherever it goes.  The best example of this is a DVD that carries its encryption wherever it goes.

Dynamic policy control ‚Äì This refers to centralized permissions management typically for an organization that needs to allow the owner of the intellectual property to manage and update rights to access the data.  

Automatic expiration ‚Äì License can expire automatically on a specific date, whether for a specific installation or at a point in time where the software becomes public domain.

Continuous audit trail ‚Äì Captures all activity on the material, views, access, modification, copying, etc.Interoperability ‚Äì This concept refers to the solution fitting into any environment, windows, linux, email, file structure, or access control methods.

A copyleft license is a type of open-source license that allows users to freely use, modify, and distribute a work, provided that any derivative works are also distributed under the same license terms. This ensures that the software remains free for all users, even in modified versions, and mandates that derivative works be shared under the same conditions. Copyleft licenses guarantee freedoms to reproduce, adapt, or distribute a work, distinguishing them from more restrictive copyright licenses. Overall, copyleft promotes the idea of keeping software free and accessible to everyone

Copyright grants exclusive rights to creators over their original works, while copyleft allows users to freely use, modify, and distribute works under the same licensing terms.
Understanding Copyright
Definition: Copyright is a legal framework that provides creators with exclusive rights to their original works, such as literature, music, art, and software. This means that the creator has control over how their work is used, reproduced, and distributed. 


Understanding Copyleft
Definition: Copyleft is a licensing method that allows creators to grant others the freedom to use, modify, and distribute their work, provided that any derivative works are also shared under the same or compatible terms. 

Purpose: The philosophy behind copyleft is to promote collaboration and the sharing of knowledge, ensuring that all modifications and adaptations remain accessible to the community. 

Key Feature: A fundamental principle of copyleft is the "share-alike" requirement, which mandates that any derivative works must be licensed under the same copyleft terms, preventing proprietary restrictions on the modified work. 



What is the FIRST step in protecting a company's trademark?
Conducting a trademark search: Before you can protect a company's trademark, it's important to first conduct a trademark search. This is a critical step to ensure that the trademark you're planning to use is not already registered or in use by another company. Using an existing trademark can result in legal consequences, such as lawsuits and fines. It can also undermine your brand identity and lead to confusion in the marketplace. The first step to protecting your trademark is to make sure it's unique and available for use. 

Registering the trademark with the United States Patent and Trademark Office (USPTO) is an important step in protecting a trademark. However, it is not the first step. Registration can only happen after you've conducted a trademark search to verify that your desired trademark isn't already in use by another company. If you try to register a trademark that's already in use, the USPTO will reject your application. 

Monitoring the use of your trademark is crucial in maintaining its protection. This process involves keeping an eye out for any unauthorized use or infringement of your trademark by other businesses. This is an ongoing process that comes after the initial stages of conducting a trademark search and registering the trademark with the USPTO. You can't monitor the use of a trademark that hasn't been established as yours in the first place. 

Implementing a trademark usage policy is an integral part of protecting a company's trademark. This policy guides how the trademark can be used within the company and by third parties, helping to maintain consistency and prevent dilution of the brand. Nonetheless, this step is downstream from conducting a trademark search and registering the trademark. The policy would be irrelevant if the trademark you plan to protect is already registered or in use by another entity.

vulnerabilities vs exploit vs threats vs risk 


 "Conduct a risk assessment of the company's current IT systems and practices." would be the option to pick here. It is far more beneficial to inventory all assets through a risk assessment, then decide from there how to handle security. Otherwise, a business is taking things into consideration that have no effect on them. The incorrect answers:
  Determine the company's overall goals and objectives for IT security is the first step in identifying the business needs for improving the IT security practices. 
  Understanding the overarching goals and objectives ensures that such feedback is collected and analyzed in the context of the company's strategic direction.
 
The primary objective of any risk management program is to minimize residual risk to an acceptable level. Residual risk is the risk that remains after all risk management strategies have been applied. 
To eliminate business risk: It's unrealistic and impractical to completely eliminate business risk. The goal is to manage risk within acceptable levels. 
To implement effective controls: While this is a part of managing risk 
To minimize inherent risk: Inherent risk is the risk that exists before any controls or mitigation strategies are implemented. 
 
Prudent person rule: While the prudent person rule is related to due care, it is a legal concept that typically applies to the management of another's affairs, especially in a fiduciary capacity. It is not specifically a term used to describe the process of setting security goals for an organization. 

Due diligence: Due diligence is the investigative process conducted to assess a business transaction. When setting security goals, due diligence is part of the broader process Jane would undertake to understand the risks and requirements of the organization, but it is not the term that best describes the minimum standard required of her. 

Building consensus: Building consensus is about obtaining agreement from various stakeholders or team members. While gaining agreement on security goals is an important part of the CISO's role
 
 Due care: While due care is a legal and ethical obligation to act responsibly and is often associated with the prudent person rule, it is more specific to taking the necessary care to protect the company and its assets. In contrast, the prudent person rule is a broader standard that emphasizes the decision-making process and the approach that someone in a position of trust should take, which is why it's the best descriptor for Thor's role. 
  
  Due diligence: Due diligence is the investigation and evaluation process undertaken to understand the risks associated with a business decision or transaction. It's evidence of the prudent person rule in action, but as a term, it specifically refers to the careful examination and research done before concluding agreements or investments, rather than the ongoing risk oversight provided by an executive like Thor.
  
  Building consensus: Building consensus involves uniting a group behind a decision or strategy. It is a crucial part of leadership, especially for a CEO who needs the support of various stakeholders to move the company forward. 
 
 Controls. When defining inherent risk, you're looking at the risk that exists assuming no controls or other mitigation actions are in place. Thus, controls are not a factor typically considered when calculating inherent risk, as they are measures implemented to reduce that inherent risk. The incorrect answers: Likelihood or Probability: This is an essential facet of determining risk. It measures the chance of a given threat exploiting a vulnerability. 
 
 Asset Valuation (AV): The value of the asset is a key component of risk. The more valuable the asset, the higher the potential risk. 
 Impact or Consequence: This is another crucial variable for assessing risk. It measures the potential damage or consequences to the organization if a threat exploits a vulnerability.

Establish baseline standards for all locations, and then add additional standards for locations that require more security. This approach allows for creating a consistent minimum level of security across the organization while acknowledging and addressing the specific regulatory requirements of each location. 

The incorrect answers: Incorporate all of the regulations into one overarching policy that covers all the requirements of all the locations and ensure all locations follow it: This approach might be unnecessarily strict for some locations and may not fully address the specific needs of each location. 

Find the industry best practices and ensure all locations are in compliance with those: Best practices are useful guides, but complying with them doesn't necessarily ensure compliance with specific regulations in each location. 

Find the common requirements that all locations have and implement those: This approach may miss specific requirements that are unique to certain locations. While it creates uniformity, it might not fully comply with all regulations.

Privacy by Design (PbD) ‚Äî CISSP Definition (Plain English) - Privacy is built into systems, processes, and technologies from the beginning, not added later.
is about: Proactive, not reactive, Default privacy protections ..Minimizing data collection ... Embedding privacy into architecture, SDLC, and governance üìå Think architecture principle, not a single control.
2Ô∏è‚É£ The 7 PbD Principles (EXAM-WORTHY)
CISSP doesn‚Äôt always ask all seven, but keywords from them appear constantly.
1Ô∏è‚É£ Proactive, not Reactive ‚Äì prevent privacy issues before they happen
2Ô∏è‚É£ Privacy as the Default ‚Äì no action required by the user to protect privacy
3Ô∏è‚É£ Privacy Embedded into Design ‚Äì part of architecture and SDLC
4Ô∏è‚É£ Full Functionality (Positive-Sum) ‚Äì privacy AND security/business goals
5Ô∏è‚É£ End-to-End Security ‚Äì data protected through entire lifecycle
6Ô∏è‚É£ Visibility & Transparency ‚Äì clear policies and processing
7Ô∏è‚É£ User-Centric ‚Äì respect user privacy choices
Privacy by Default - Configuration state
Privacy by Design - Architectural principle

Where PbD Fits in CISSP Domains
Domain	How PbD Appears
Domain 1 ‚Äì Security & Risk	Privacy governance, policy, legal compliance
Domain 3 ‚Äì Security Architecture	Data flow design, minimization, encryption
Domain 7 ‚Äì SDLC	Privacy requirements, secure design
Domain 8 ‚Äì Software Dev	Secure coding, data handling

Continous Risk Management - Plan Do Check Act / Deming Cycle
Plan - Determine which controls to implement based on the risks identified
Do - Implement the controls
Check -  Check Monitoring and assurance; are the controls operating effectively
Act - Based upon findings during the ‚ÄúCheck‚Äù step, take additional, actions as necessary (react), which leads back to planning
 
 The CIA triad stands for Confidentiality, Integrity, and Availability. 
 
 Purpose of threat modeling
In order to perform proper risk management, it is important to identify the threats and vulnerabilities associated with each asset. Threat modeling methodologies aid in systematically identifying
threats and their severity, which in turn makes risk management more accurate and effective
 They enable: the systematic identification, enumeration, and prioritization of threats related to an asset.
 . Three of the major threat modeling methodologies you need to know about for the exam are STRIDE, PASTA and DREAD.

***STRIDE*** 
***STRIDE*** was developed by Microsoft. Though it was initially developed as a means of assessing threats to applications and operating systems, it can be used in other contexts too. STRIDE is a threat-focused methodology
**Spoofing - threat against Authentication
An attacker pretends to be something or someone to gain unauthorized access
**Tampering - threat againstIntegrity
An attacker modifies data at rest (e.g., in a database) or in transit (e.g.,over the network)
**Repudiation - threat against Nonrepudiation
An attacker performs an action on a system that is not attributable to them
**Information Disclosure - threat against Confidentiality
An attacker can read sensitive information
**Denial of Service - threat against Availability 
An attacker prevents legitimate users from accessing an application/service
** Elevation of Privilege - threat against Authorization
An attacker gains elevated access rights (e.g., administrative/root access)

**DREAD**
**DREAD** is a threat model primarily used to rank the severity of threats. DREAD is often used in combination with the STRIDE model, where STRIDE identifies the threats, and DREAD is then used **to rank the severity** of threats. Five key points are considered and a score for each is determined between 1 and 10 (1 being low-risk, and 10 being high-risk). The score from each of thefive key points is then tallied up and divided by five to produce a final score out of ten. This final score is then used to understand the severity of a threat.
D Damage
Total amount of damage the threat can cause?
R Reproducibility How easily can the threat be replicated? 
E Exploitability
How difficult is it to exploit the threat?
A Affected Users
How many people, inside or outside the organization, will be affected by the threat?
D Discoverability
How easily can the threat be discovered?
  
  **Damage** refers to the potential harm that could be caused if a vulnerability were to be exploited. This impact considers how bad the consequences would be, ranging from minimal disruptions to significant data breaches or system outages. The severity of the potential damage is often considered the most critical factor because it directly relates to the potential consequences an organization could face from a security incident. 
  
  **Reproducibility** refers to how easily and consistently the vulnerability can be exploited. While an easily reproducible vulnerability is concerning, it is the damage caused by the exploitation, not the frequency or ease of reproduction, that holds the highest impact. 
  
  **Exploitability** gauges how easy it is for an attacker to exploit the vulnerability. While this is a crucial factor in the overall risk assessment, it is the potential damage that often holds greater weight in the decision-making process regarding the risk. 
  **Awareness(( measures how well-known the vulnerability is, both to potential attackers and the public. A widely known vulnerability might be seen as a higher risk because many attackers are aware of it, but again, it's the potential damage that results from the exploitation that generally carries the most weight in the assessment.

**Attack Simulation and Threat Analysis (PASTA)**
Process for **Attack Simulation and Threat Analysis (PASTA)**, contrary to STRIDE, is an attacker-focused, risk-centric methodology. It is much more detailed than STRIDE and performs threat analysis from a strategic perspective that includes input from governance, operations, architecture, and development, in both tech and business views

**Define Objectives‚ÄîThis considers the inherent application risk profile and addresses other business impact considerations early.
**Define Technical Scope‚ÄîThe philosophy behind this stage is that you can‚Äôt protect what you don‚Äôt know. It‚Äôs intended to decompose the technology stack that supports the application components that realize the business objectives identified from Stage 1.
**Application Decomposition‚ÄîThis stage focuses on understanding the data flows among application components and services in the application threat model.
**Threat Analysis‚ÄîReviews threat assertions from data within the environment as well as industry threat intelligence that is relevant to service, data, and deployment model.
**Vulnerability and Weakness Analysis‚ÄîIdentifies the vulnerabilities and weaknesses within the application design and code and correlates to see if it supports the threat assertions from the prior stage.
**Attack Modeling‚ÄîThis stage focuses on emulating attacks that could exploit identified weaknesses/vulnerabilities from the prior stage. It helps to also determine the threat viability via attack patterns.
**Risk and Impact Analysis‚ÄîThis stage centers around remediating vulnerabilities or weaknesses in code or design that can facilitate threats and underlying attack patterns. It may warrant some risk acceptance by broader application owners or development managers.

**Social engineering** 
Manipulation of people‚Äôs actions through intimidation and/or deception .. it is a prevalent means of attack against organizations Best way to combat social engineering is through awareness/education/training

**intimidation (involves inducing fear in order to manipulate someone into a specific course of action) ... blackmail
**deception (involves tricking someone in one manner or another) ...lying 
**rapport (building a gradual relationship with a victim in order to take advantage of it down the line) ...pretending to be from the IT team
**Phishing -  where an attacker sends many emails with the hope that the target will open an email and click on a link or open a file that leads to a malicious action ... email 
**Spear phishing -  a targeted form of phishing that typically focuses on certain individuals or groups of individuals ... sending fake invoice to payable team
**Whaling Like spear phishing, this is also an email attack and targets the big fish‚Äîthe whales‚Äîin an organization. Typically, targeting people like the CEO, COO, and CFO
**Smishing - Smishing is a form of phishing that targets mobile phone users. Typically, an attacker purporting legit company sends a fraudulent text/SMS message to a potential victim, 
**Vishing- Vishing is another form of phishing, and the name refers to the way it is typically presented to a potential victim‚Äîvia voice over IP (VoIP) phone systems to their phones
**Pretexting involves the attacker creating a scenario, almost like a script, that very ingeniously and subtly spurs the victim into action. a‚Äúfriend‚Äù texting you with news about an unfortunate incident that‚Äôs left them stranded someplace. Ultimately, a request is made for money, sensitive information, or both.
**Baiting -  a form of social engineering that preys on people‚Äôs curiosity via the use of physical tools, like USB drives. Usually, the attacker will drop some USB drives in a building parking lot, 
**Tailgating or piggybacking -  the action of following a person who is authorized to enter a restricted area through a door and thus gaining unauthorized access. The difference is that in tailgating the attacker possesses a
badge that is fake but looks real. In piggybacking, the attacker doesn‚Äôt have any badge at all.

Apply supply chain risk management (SCRM) concepts
**Risk management methodologies should be applied to all vendors, suppliers, service providers
**The process of identifying, assessing, mitigating, and monitoring risks introduced by third parties, vendors, suppliers, and service providers across the entire lifecycle.
**CISSP frames SCRM in domain 1 as: Governance responsibility, Risk ownership by the organization, Policy-driven, contract-enforced ...  not purely technical risk
**CISSP Considers ‚ÄúSupply Chain‚Äù examples : Software vendors (OS, apps, SaaS), Cloud providers, MSPs / MSSPs, Hardware manufacturers, Open-source components, Logistics and maintenance vendors
**Supply chain risk management ensures third-party risks are governed through due diligence, contractual controls, and continuous monitoring.

1. Identify
Who are the vendors?
What do they access?
What data do they handle? üìå Vendor inventory is critical.
2. Assess
Security posture
Compliance (SOC 2, ISO 27001)
Financial stability
Geographic / geopolitical risk üìå Use due diligence, not trust.
3. Mitigate
Contract clauses, SLAs / SRAs, Right-to-audit
Security requirements ...Segmentation & least privilege üìå Contracts are security controls.
4. Monitor (Continuous)
Ongoing assessments
Audit reports
Incident notifications
Performance metrics üìå One-time assessments = ‚ùå wrong answer.

SCRM vs Third-Party Risk (Trick!)
Third-party risk	Risks from any external party
SCRM		Focused on supply chain dependencies

**SCRM mitigation controls(Know These)**
Administrative - Vendor policies, contracts, NDAs, SLA/SLR
Technical	Network segmentation, access controls
Legal	Right-to-audit, breach notification
Operational	Monitoring, audits, performance reviews

**Other various supply chain risk mitigations**
**Software Bill of Materials - A comprehensive list of components, libraries and modules that are used to build a software product, often detailing versions, sources and dependencies. 
With this list, we can check what changes have been made in new versions. This can help us detect things like backdoors and othervulnerabilities.
**Third-party Assessment and Monitoring - Evaluating and continuously monitoring the security practices and performance of third-party vendors or suppliers.
**Minimum Security Requirements - Predefined baseline security standards that vendors must meet.
**Service-level Requirements - Specifications set in contracts that dictate the expected performance, availability, and responsiveness of a service provided by a vendor.
**Silicon Root of Trust - A secure cryptographic identity embedded in hardware, ensuring that the hardware starts in a trusted state and that the firmware loaded onto it is genuine
Physically Unclonable Function - A hardware feature that uses the unique physical characteristics of semiconductor devices to generate cryptographic keys, ensuring that each device has a unique and unclonable identity. 

**SLR, SLA, and Service Level Reports**
1**SLR ‚Äî Service Level Requirements - Business-defined minimum service expectations (availability, RTO, response time).
CISSP framing: Input to risk assessment,  Defined by business, not vendor, Occurs before SLA
With the acquisition of a service, additional organizational requirements must be considered, and this is done through a document called an SLR.
Detailed service descriptions, Detailed service level targets, Mutual responsibilities
.....it defines the security services and service level targets that each potential supplier can be evaluated agains


2**SLA ‚Äî Service Level Agreement - A contractual agreement that commits a provider to meet defined service levels.
CISSP framing: Legal / administrative control, Risk transfer & risk mitigation
SLAs are addendums to the contract and are therefore enforceable. SLAs often include expectations and stipulations such as **Service Levels (performance levels)**, **Governance**‚Äîthe customer and the service provider know
who is responsible for what, **Security** ‚Äîexpected security controls put in place by the service provider that speak to the topic of accountability
and responsibility. Accountability can never be outsourced,  Compliance with all laws and regulations, and Liability/Indemnification when any element of the SLA is not met or is below threshold standards


3** Service Level Reports (SLRpt / SLM Reports)- Operational evidence showing whether SLAs are being met.
CISSP framing: Monitoring and assurance, Supports audits and RMF ‚ÄúMonitor‚Äù step, Detective control
-Service level reports are issued by a vendor or service provider to a client and provide insight and information about the service provider‚Äôs ability to deliver services as defined by the SLA.
- The service level report compares anticipated and agreed upon service levels with actual service levels and documents the effectiveness of security controls, which allows the customer‚Äîthe owner‚Äîto gain assurance that expectations are being met. such ... Achievement of metrics defined in the SLA, issues identified, Reporting channels, Third-party SOC2 type2 reports, which provide independent verification and assurance

**Order matters ‚Äî CISSP loves order questions
SLR ‚Üí SLA ‚Üí Service Level Reports   (SLA is based on SLR, not the other way around.)
(Step	RMF / PDCA Phase / RMF mapping )
SLR	- PLAN  - Categorize /Select / Define SLRs
SLA	- DO - Implement / Establish SLAs  - SLA ‚â† Risk Transfer - SLA transfers financial accountability, not risk ownership.
Reports -	CHECK assess / Review service reports  - Define SLRs and enforce SLAs with reporting
Adjust SLA / Controls -	ACT / Monitor Continuous service reporting - SLA without reporting = false sense of security.

Item	Control Type
SLR	Administrative (governance)
SLA	Administrative / Legal
Reports	Detective / Administrative

SLR = What we need
SLA = What we agree
Reports = What we prove

SLRs define business service expectations, SLAs contractually enforce them, and service level reports provide ongoing assurance as part of continuous risk monitoring.

Exam Keywords to Circle ‚úèÔ∏è If you see keywords like:
Vendor, Supplier, Outsourcing, Cloud provider, Third-party access, Contracts
‚û°Ô∏è Think SCRM

**Risks associated with the acquisition of products and services from suppliers and providers**
**Product Tampering - Unauthorized alteration or modification of a product after manufacturing, but before the product reaches the consumer. 
As an example, an attacker could intercept a keyboard delivery, solder a keylogger inside it, and then repackage it. 
**Counterfeits - Unauthorized replicas or imitations of products that are made to deceive consumers into thinking that they are purchasing the real thing. Counterfeits may result in reduced
performance, hazards, regulatory violations, inappropriate security, or increased vulnerabilities.
**Implants - Hardware or software components stealthily inserted into products to perform unauthorized activities, such as espionage or data theft. Implants can result in sensitiveinformation being sent to unauthorized entities and unauthorized access to systems. 

**Methods and Techniques to Increase Awareness, Training, and Education**
**Awareness is to change cultural sensitivity to a topic or issue
Examples of awareness include internal phishing campaigns, lunch and learns, and awareness posters hung in visible places.
**Training provides specific skills needed to perform tasks for security or tech spec in a role
Examples of training might include a firewall administrator learning how to write firewall rules or a security guard learning how to respond to different situations related to protecting a
building and the assets within.
**Education helps people understand fundamental concepts and therefore develop decision-making skills and abilitie

Methods and Techniques to Provide Awareness and Training (e.g., social engineering, phishing, security champions (promoters), gamification)



Enforceable via penalties

Domain 2: Asset Security
**********************************************************


**Asset security includes the concepts, structures, principles, and controls we are aimed at protecting ..
**Assets‚Äîanything that represents value to the organization.

**Asset classification policies, procedures, and processes help achieve proper protection of assets
**Classification is driven by the value of the asset
**Protecting assets should always be based on the value of the asset, and therefore, for security to be an enabler, protecting assets should always be cost-effective. As the value of an asset
increases, so does the effort invested in protecting it. Less valuable assets might not warrant costly protection

**Once the asset captured, inventoried, and classification system is in place and asset owners have been identified, security can then work with owners to assign assets an appropriate classification level that determines
how the assets are protected.

**owners are ultimately accountable for ensuring their assets are classified and thus protected appropriately.
**It's not uncommon for some owners to challenge ownership to avoid being accountable, governance committee must set the tone from the top, stating that owners must own the asset, and the security function is there to support the implementation of suitable controls.

**Information Classification Benefits The information classification process provides several benefits,
**Identification of **critical information**: identifies information that the organization considers critical to business success.
**Identification of **sensitivity** to modification: classification helps identify data that must only be modified in specifically authorized ways.
**Commitment to protect valuable assets and Commitment to confidentiality where applicable
**The value to assets can be represented through confidentiality (sensitivity), integrity (accuracy and meaningfulness), and availability (criticality). 

**For classification to be done properly, it needs to be driven by the owners and an asset classification committee or working group, comprised of qualified representatives from different areas of the organization which  can provide a more objective classification process without bias
** Est a consistent process to classify assets. This could be achieved through a consistent asset classification scoring system
** Asset classification is ongoing. Because asset classification helps identify the appropriate controls for a given asset, and the nature of assets changes over time
** The classification of an asset also driven by . Laws, regulations, industry standards, privacy requirements, company policies, and related guidance lifecycle and archiving and retention requirements 

**Classification versus Categorization**
**Classification refers to a system of classes, ordered according to value
**Categorization refers to the act of sorting assets into defined classes
**Ideally, all assets should be categorized into a classification system to allow them to be protected based on value

Classification Examples:  Top secret, secret, confidential, sensitive but unclassified, and unclassified ...Financially sensitive ... Company restricte .. Proprietary ... PII/PHI

**Labeling refers to the classification of the asset and is **system-re**adable**
	-	Association of security attributes with subjects and objects represented by internal data structures, Enables **system-based enforcement**
	-	Metadata, Barcodes, QR codes, RFID tags, GPS tags
**Marking refers to the handling instructions of the asset and is **human-readable**
	-	Association of security attributes with objects in a human-readable form, Enables **process-based enforcement**
**Should be consistently applied to all assets within an organization, Labeling should be cost-effective, Enables system-based enforcement

**Owners can delegate responsibility for an asset, but they always remain accountable for the protection of the asset. In other words, accountability cannot be delegated to anyone else. The owner can delegate the
responsibility, but accountability remains with the owner.

Types of asset owners ..Data owners
   Process owners
   System owners
   Product owners
   Service owners
   Hardware owners
   Applications owners
   Intellectual property owners


**CISSP Data Roles**
Data Controller -üëâ Determines why and how personal data is processed (GDPR-focused role).
üìå Exam scenario: Decides purpose and legal basis for PII processing.

Data Owner - üëâ Accountable for the data; defines classification, access rules, and risk tolerance.
üìå Exam scenario: Decides who may access the data.

Data Custodian -üëâ Implements and maintains controls defined by the data owner (backups, access enforcement).
üìå Exam scenario: Configures permissions and performs backups.

Data Steward -üëâ Ensures data quality, consistency, and compliance with policies and regulations.
üìå Exam scenario: Validates data accuracy and regulatory adherence.

Data Processor -üëâ Processes data on behalf of the data controller under contractual terms.
üìå Exam scenario: Cloud provider handling customer PII.

Data User -üëâ Authorized individual who accesses and uses data according to policy.
üìå Exam scenario: Employee viewing customer records.

Data Subject -üëâ Individual to whom the personal data belongs.
üìå Exam scenario: Customer exercising GDPR rights.

Steward ‚â† Custodian
Steward = data quality & compliance
Custodian = technical handling
Owner vs Custodian, Owner decides, Custodian enforces


Manage **data life cycle** -  2.4.1 Information Life Cycle
Data must be protected at each stage of its life cycle. The concept of the information life cycle is founded on the principle that proper controls should be in place at the time
of creation and throughout the life cycle. Immediately upon creation, collection, or update, information should be assigned a classification (by the owner), which then drives
all other activities, such as storage, use, sharing, archiving, and final disposal or destruction, to protect information throughout its life cycle. 

Stages
Create - Generation of new digital content, or the alteration/updating/modifying of existing content
Store - Committing digital data to some sort of storage repository, which typically occurs nearly simultaneously with creation
Use - Data viewed, processed, or otherwise used in some sort of activity, not including modification
Share - Information made accessible to others, such as company users, customers, and partners
Archive - Data leaves active use and enters long-term storage 
Destroy - Data is permanently destroyed using physical or digital means (e.g., crypto shredding)


2.4.2 Data Destruction
**Data remanence refers to residual representation of information even after attempts to securely delete or remove the data 
**Categories of sanitization‚Äîdestruction, purging, clearing
**Secure removal of data in the cloud
**Defensible destruction** means being able to prove that there‚Äôs no possible way for anyone to recover data that has been securely destroyed. Data owners are responsible for
ensuring the proper sanitization of the data assets they own

Destroy - Physical destruction of media; this is the most effective means of sanitization.
Purge - Logical/physical techniques used to sanitize; data cannot be reconstructed.
Clear - Logical techniques used to sanitize; data may be reconstructed. This is the least effective means of sanitization.
NIST SP 800-88 revision 1 provides guidelines for media sanitization 




pg216 - pg 243 marked end of D2 on Destination book need notes ...............................





PROM (Programmable Read Only Memory) ‚Äì Can only be written once, normally at the factory.
APROM
EEPROM
EPROM
DRAM
SDRAM

EEPROM, or Electrically Erasable Programmable Read-Only Memory, is the type of memory used in flash drives. This is a type of non-volatile memory used in computers and other electronic devices to store relatively small amounts of data. It can be electrically erased and reprogrammed, which is why it's used in flash drives. These can be repeatedly written to and erased without wear and can retain data without power. The incorrect answers: SDRAM (Synchronous Dynamic Random-Access Memory) is a type of DRAM that is synchronized with the clock speed that the microprocessor is optimized for. This maximizes the number of memory accesses the microprocessor can make. While used in computing systems, it isn't the memory type used in flash drives. Programmable Read-Only Memory (PROM) is a type of ROM that can be programmed using a special device. This programming is a one-time operation and once done the data stored in PROM cannot be changed. It's not used in flash drives because flash drives require memory that can be rewritten and erased. Dynamic Random-Access Memory (DRAM) is a type of RAM that stores each bit of data in a separate capacitor within an integrated circuit. The capacitors leak charges, hence they must be refreshed constantly. Because of this volatility and the required constant power supply, DRAM is not used in flash drives, which need to retain information even without power.

 tangible or intangible assets

Statistical techniques are considered to be the most effective methods for de-identifying personal data. 
Techniques can include noise addition, permutation, data swapping, and more complex methods like differential privacy. The goal is to ensure that the data, when released, does not contain information that can be linked back to an individual, making it the most effective method for de-identifying personal data. 

Redaction is a method used to remove sensitive information from a document or data set. However, it's not the most effective method for de-identifying personal data as it can still leave other potentially identifying information intact. Additionally, in some cases, redacted data can be re-identified through inference. While encryption can protect data from unauthorized access, it doesn't necessarily de-identify the data. Moreover, using a weak cipher could make the encrypted data vulnerable to decryption. This option also assumes that data recipients have the decryption key, which could potentially expose the original, identifiable data. 

Replacing names with random values, commonly referred to as pseudonymization, replaces identifiers with other values. While it can provide some level of de-identification, it can still leave data susceptible to re-identification if the pseudonymized data can be linked back to the original data via a "lookup table" or other method.


 COBIT 2019: COBIT, which stands for Control Objectives for Information and Related Technologies, is primarily a framework focused on governance and management of enterprise IT. Although COBIT can help organizations align IT with business strategies and manage IT-related risks, it is less likely to be directly used for listing specific security controls for application development compared to other standards that provide detailed control catalogs. 
 
 ISO 27002: ISO 27002 is a well-established international standard that provides best practice recommendations on information security controls, making it a suitable reference for listing security controls in application development. 
 
 NIST SP 800-53: NIST SP 800-53 is a comprehensive catalog of security and privacy controls for all U.S. federal information systems (except those related to national security) and is directly focused on listing such controls, making it highly relevant for application development. NIST SP 800-53A: NIST SP 800-53A provides guidelines for assessing the effectiveness of security controls, and while it is not the primary resource for listing controls, it is related to the implementation of the controls listed in NIST SP 800-53, which itself is applicable for the development process.




 System interconnectivity and poor security management. High system interconnectivity can increase the risk exposure of IT assets, especially if combined with poor security management. If security management practices are deficient, potential vulnerabilities might not be effectively identified or mitigated, making the interconnected systems more susceptible to security breaches or data loss. Security management can also encompass keeping data secure during disasters, which would include having backups. 
 
 The incorrect answers: System interconnectivity and poor controls over data sensitivity: Although this could increase risk, poor security management generally poses a greater overall risk as it affects all aspects of security, not only those related to data sensitivity. 
 
 System interconnectivity and lack of system backups: While lack of backups could be a significant issue in case of data loss, the immediate risk due to lack of proper security management can be higher. 
 
 System interconnectivity and inadequate physical security: Physical security is crucial, but in a highly interconnected system, poor security management practices can potentially expose the system to more immediate and expansive risks.
 
 
 Conflict of interest: Naomi, as the VP of Sales, is responsible for managing the customer data. By deciding to run a report that involves analyzing customer stability based on the length of time they stay in the same house, and doing so using her desktop, she may inadvertently create a conflict of interest. This is especially true if the information is sensitive, and running the report on her desktop could lead to a potential breach of trust or security. As a data owner, she should be aware of the proper channels and procedures for handling PII and should avoid situations where her personal handling of data could conflict with the company's data protection policies or legal obligations regarding customer privacy. Specifically, this may breach any agreements between the business and customers on what data will be used and how. 
 
 Data controller: The data controller is an entity that determines the purposes and means of processing personal data. In many organizational contexts, the VP of Sales would be considered a data controller since they decide how customer data is used. However, in this scenario, Naomi's actions involving the processing of PII on her personal desktop raise concerns about a potential conflict of interest or breach of data governance practices. 
 
 Data processor: A data processor processes personal data on behalf of the data controller. While Naomi's actions involve processing data, the primary concern here is not her role in processing but the possibility that her actions may lead to a misuse of customer data or an insecure handling of sensitive information, which depicts a conflict of interest. 
 
 Data steward: A data steward manages the integrity, usability, and security of the data based on the organization's guidelines. Although Naomi has stewardship over the data being the VP of Sales, the scenario raises issues about personal handling and potential conflicts of interest rather than issues related to data quality or integrity management.

Domain 3 Security Architecture and Engineering
**********************************************************
: Elliptic curve cryptography (ECC): ECC is considered the most secure method of public key cryptography, because it provides the same level of security as RSA and DSA, but with much shorter key lengths. Shorter keys are quicker to process, and thus ECC is more efficient. The strength of ECC lies in the complexity of the elliptic curve logarithm problem, which makes ECC more difficult to crack than RSA or DSA. Therefore, it is commonly used in resource-constrained environments, such as mobile devices. The incorrect answers: RSA (Rivest‚ÄìShamir‚ÄìAdleman): RSA is one of the earliest and most widely used public key cryptosystems. It's used for secure data transmission and digital signatures. The security of RSA relies on the difficulty of factoring large numbers into primes. However, RSA requires longer key lengths than ECC for the same level of security, which makes RSA less efficient. DSA (Digital Signature Algorithm): DSA is a public key algorithm that's used to create digital signatures. It's part of the U.S. Federal Information Processing Standards (FIPS), and its security is based on the discrete logarithm problem. Like RSA, DSA requires longer key lengths than ECC to provide the same level of security, making it less efficient. Symmetric key cryptography: This is not a method of public key cryptography, but a different type of cryptography. Symmetric cryptography uses a single key to both encrypt and decrypt information. While symmetric key cryptography is efficient and fast, it's not as secure as public key methods like ECC, RSA, or DSA because it requires the secure distribution of the key to both parties.


**The Security Architecture and Engineering CISSP domain contains the concepts, principles, structures, and standards used to design, implement, monitor, and secure various architectures such as systems, applications, operating systems, equipment, networks, and those controls used to enforce various appropriate levels of security

**focuses on the different processes, standards, frameworks, and structures to design and implement secure architectures and how, in order to achieve that, the security function needs to be involved at the start of the engineering life cycle and throughout each of the subsequent phases

**security architecture. - Security policies, knowledge, and experience must be applied to protect this architecture to the level of value relating to the individual components and to the overall architecture

**the word architecture implies many components that work together to allow that architecture to be used for the purposes for which it was intended

**enterprise security architecture. -  the entire organization can be protected by breaking the enterprise into different components and protecting each component. What makes up any company or enterprise? Typical components include people, technology, processes, functions, information, hardware, and networks.

** Security considered from the beginning leads to the best outcome, and it is the most cost-effective approach. It leads to what is known as security by design, which means that security should be embedded from the beginning and not just as an afterthought.

** Regardless of the framework, model, or methodology used, the risk management process should be used to identify the most valuable assets and risks to those assets, and to determine appropriate and cost-effective security controls to implement this.

** SOME Examples of Secure Design Principles:
 Threat modeling (discussed in section 1.10)
   Least privilege (discussed in section 1.8.2)
   Defense in depth (discussed in section 3.4.9)
  Secure defaults - Any default settings a system has should be secured to the extent possible, so no compromise is facilitated   
  Fail securely - If a system or its components fail, they should do so in a manner that doesn‚Äôt expose the system to a potential attack
   Separation of duties (discussed in section 1.8.2)
   Keep it simple and small (KISS)  - Remove as much complexity from a situation as possible and focus on what matters most,  Smaller attack surface,   Less errors and vulnerabilities, testing, easier troubleshooting
   
**Zero trust or trust but verify -  Zero trust essentially means trust nothing, and it is based upon the premise that organizations should not automatically trust anything internal or external to enter their perimeter
   (more pg 250-254)
	-	"trust but verify" really means being able to authenticate users and perform authorization based on their permissions to perform activities on the network so they can access the various resources. 
	-	Micro-segmentation of networks, Granular enforcement of perimeter ingress/egress points, based upon identity, user location, and other data to determine whether to trust the user, device, or application seeking access to enterprise resources.
	- 	inherent trust should not exist. Simply because a device is connected to a network does not mean the device should have access to anything on the network.
	-	Access to network resources should only come as a result of confidence gained through proper authentication, authorization, and accounting (AAA) of users, devices, and services.
	-	 user‚Äôs identity (user authentication), associated user devices (device verification), and the services they access (service authorization and associated accounting, which is achieved by logging).
	- 	elements like: Strong user authentication / Authentication and Authorization of services /  Logging and monitoring
	-	principles like: understanding the architecture, identities, health of (users, devices, and services), Authenticate everywhere, Focus your monitoring on devices and services, and which to design for ZTrust

	-	due to the growth in reliance on third-party services, trust should be verified through additional assurance mechanisms like Audits, Ongoing monitoring, SOC reporting, Contracts/agreements, like SLAs and SLRs
   
   
**Privacy by Design - Privacy by Design is premised on the belief that privacy should be incorporated into networked systems and technologies by default and designed into the architecture Shared responsibility    
	-	Privacy as proactive and preventive, not reactive and remedial. Privacy by Design (PbD) should anticipate and prevent privacy shortcomings before they occur. PbD does not wait for risks to privacy to arise, nor does it 	attempt to resolve privacy breaches once they‚Äôve taken place
	-	 Privacy as the default setting. Like many firewalls that include implicit deny as the default treatment of all traffic, PbD seeks to ensure the automatic‚Äîdefault‚Äîprotection of personal data in IT systems and business practices
	-	Privacy embedded into design. PbD is considered from the inception of a system, application, or process and is included in the architecture, design, and development of the system, application, and process. 
	-	**Full functionality within a given solution**. PbD seeks a ‚Äúwin-win‚Äù situation for all parties involved and attempts to accommodate all interests, goals, and objectives rather than demand trade-offs that reduce overall effectivenes
	-	End-to-End Security. PbD, having been considered from the inception of a system, application, or process, should securely extend through the life cycle of the data involved.
	-	Visibility and Transparency. PbD calls for visibility and transparency of all components and processes related to the technology or business practice being used.
	-	 Respect for User Privacy. PbD ultimately requires the individual to be treated with the utmost respect and care.

**Shared Responsibility 
	-	defines which security responsibilities belong to the cloud provider and which belong to the customer.
	-	consumers and providers must act on these responsibilities and define clear contracts and agreements, which can then be implemented through appropriate policies, procedures, and controls.
	-	CISSP tests : Accountability, Risk ownership and Legal and governance implications
	-	You can outsource operations ‚Äî you CANNOT outsource responsibility,  Even if the cloud provider causes the issue, the organization remains accountable.
	-	defined and understood, especially since accountability may never be delegated or otherwise transferred, regardless of the customer provider relationship.
	-	High level:
		-	Cloud Provider (Always Responsible For) -Physical security of data centers, Power, HVAC, Physical network infrastructure, Hypervisor (usually)
		-	Customer (Always Responsible For) - Data classification, Access control, Identity management, Configuration, Compliance
	-	By Service Model (Exam Favorite)
		-	IaaS Customer responsible for: OS patching, Firewall rules, Applications Data, 
		-	IaaS Provider responsible for: Physical servers, Hypervisor, Facilities
		-	PaaS Customer responsible for: Applications, Data, User access
		-	PaaS Provider responsible for: OS, Runtime, Platform services
		-	SaaS Customer responsible for: Data, User access, Configuration settings, 
		-	SaaS Provider responsible for: Application, OS, Infrastructure
	-	SaaS is NOT zero customer security duties ‚úÖ Identity, access, data remain customer's accountability

**Break some links in the Cyber Kill Chain.
	- 	framework describing the stages of a cyberattack, originally developed by Lockheed Martin, used to understand, detect, and disrupt attacks at multiple points.
	-	CISSP need to Mapping controls to attack phases, to where to break the chain and to Choosing preventive vs detective vs corrective controls
	-	7 stages of CKC
		**	1. Reconnaissance - involves identifying a target, gather information that will be useful to the attacker
				stage controls: Reduce attack surface, info disclosure
		**	2. Weaponization - Malware/exploit prepared, building an exploit that aims to take advantage of any vulnerabilities identified in the previous step.
				stage controls: Secure SDLC, patching
		**	3. Delivery - Payload delivered (email, USB, web), involves the attacker launching their attack. Common delivery methods include sending malicious
				stage controls: Email security, user training
		**	4. Exploitation - 	Vulnerability exploited, the attacker executes their malicious code on the target‚Äôs systems.
				stage controls : Hardening, patching
		**	5. Installation - 	Malware installed, comes immediately after exploitation, and the attacker now has software installed on the target‚Äôs systems.
				stage controls : Endpoint protection
		**	6. Command & Control (C2) - Attacker establishes control, allows the attacker to remotely control their malware running within the target‚Äôs systems. The attacker can move laterally and install backdoors to advance
				stage controls : Network monitoring
		**	7. Actions on Objectives - Execute utimate goals such as Data exfiltration, lateral movement, or encrypting the target‚Äôs files in a ransomware attack.
				stage controls : DLP, IR
	- 	Stopping the attack at ANY stage of the CKC is a success
	-	CISSP prefers : Earlier stages ‚Üí cheaper, less damage, Defense in depth ‚Üí multiple chances to stop attacker
	-	CISSP must see CKC stage controls ..is NOT only prevention but expects detection + response
	-	DO NOT memorizing steps without control mapping, Always ask: Which control stops this stage?



**Secure access service edge - We discuss secure access service edge when we talk about edge computing in section 3.5.15.

3.2 Understand the fundamental concepts of security models (e.g., Biba, Star Model, Bell‚ÄìLaPadula)
3.2.1 Security Models
**A model is a representation of something real.
**A security model is a representation of what security should look like in an architecture.
**Some of these models include Bell‚Äì LaPadula, Biba, Clark‚ÄìWilson and Brewer‚ÄìNash (also referred to as the Chinese Wall model). 
**These are simple models that provide the basis‚Äîthe fundamental means‚Äîfor building confidentiality or integrity into architectures that require these core principles. 
**Like any model, security models represent what security needs to look like.


3.2.2 Enterprise Security Architecture
**An architecture is a group of components that work together.
**Security architecture involves breaking down a system to its components and protecting each component based upon its value.
**Frameworks existto serve as guidelines. THREE of the most popular enterprise security architectures 
**Though each differs a bit in structure and terminology, they each basically do the same thing to protect any architecture are:
	-	**Zachman**
		**The Zachman Framework is a taxonomy (classification schema) that organizes enterprise architecture artifacts by perspective and interrogatives.
		**Tells you how to organize and describe architecture
		**Focuses on answering basic questions like how, where, who, when, and why 
		**Directing those questions to the various company teams (e.g., designers, owners, architects, strategists, engineers, operators) and acquiring their feedback
		**outdated ...as it merely focuses on science of sysmatic classification (taxonomy) and organization of enterprise security.
		
	-	**Sherwood Applied Business Security Architecture (SABSA)** 
			**a business‚Äëdriven security architecture framework that starts with business requirements and risk, then derives security services and controls.
			**Built specifically for security, ‚úÖ Strong alignment with risk management,‚úÖ Very popular in CISSP questions
			**CISSP keywords in the question: Business drivers, Risk appetite, Security metrics, Business alignment
			
	-	**The Open Group Architecture Framework (TOGAF).** 
		**focuses on efficient resource utilization and cost minimization while having a modular structure increasing its adoption, a content framework providing consistency, and a style that allows architectural flexibility.
		** is a methodology and framework for developing, implementing, and governing enterprise architecture.
		**‚úÖ It includes process, ‚úÖ governance, ‚úÖArchitecture  Lifecycle management
	


‚úÖ Mnemonic
Zachman = WHAT exists, Classification & taxonomyOrganizing architecture artifacts
TOGAF   = HOW to build it, Security‚Äëdriven architectureAligning security to business
SABSA   = WHY security exists, Enterprise architecture processBuilding and governing architecture

Zachman = Structure
TOGAF   = Process
SABSA   = Security + Business
	
If the question says‚Ä¶
‚ÄúClassification, viewpoints, completeness‚Äù ‚Üí Zachman
‚ÄúBusiness‚Äëdriven security, risk appetite‚Äù ‚Üí SABSA
‚ÄúArchitecture process, governance, lifecycle‚Äù ‚Üí TOGAF
   
**Security models:**
-	**ABOUT Who can read/write what ..What property is being protected ..Confidentiality or Integrity or Availability (less common in classic models)
-	**Two most common types: **lattice-based** or **rule-based**
	-	A good way to envision a **lattice-based model** is to think of a ladder, where a framework and steps exist that look a bit like layers, going up and down. In other words, a lattice-based model is a layer-based model. It requires layers of security to address the requirements. 2 main lattice-based models exists: **Bell‚ÄìLaPadula** and **Biba**.
	-	Lattice-based models do include rules, but those rules are confined to layers within the model; hence, the term lattice-based is more applicable.
		**Bell‚ÄìLaPadula - The Bell‚ÄìLaPadula model is a formal state machine model designed to **preserve confidentiality** in multilevel security systems (e.g., military or government environments).
			-	BLP ...**No Read Up and No Write Down**, protects **Confidentiality ONLY"
			-	**When Bell‚ÄìLaPadula is the right answer**: ‚úÖ Confidentiality‚Äëfocused questions, ‚úÖ Government / military environments, ‚úÖ Data classification and clearance levels, ‚úÖ Mandatory Access Control (MAC)
			-	Core rules: No Read Up (ss‚Äëproperty) and No Write Down (*-property).
			-	Goal: prevent disclosure of higher‚Äëclassified data to lower levels.
		**Biba - formal access control model designed to preserve integrity, ensuring that data is not improperly modified in a system.
			-	Biba ...**No Write Up and No Read Down**, protects **Integrity ONLY"
			-	Biba is opposite of BLP
			-	**When Biba is the right answer**: ‚úÖ Focus on data integrity ‚úÖ Preventing unauthorized modification ‚úÖ Trusted vs untrusted data sources ‚úÖ Financial, transactional, or safety‚Äëcritical systems
			-	Core rules: No Read Down (simple integrity) and No Write Up (*-integrity).
			-	Goal: prevent corruption of high‚Äëintegrity data by lower‚Äëintegrity subjects.			
		**Lipner implementation is not a model; it is an implementation that combines the best features of Bell‚ÄìLaPadula and Biba, protects both confidentiality and integrity
		**Star-Rules (BLP): Normal Star  = write up allowed ... Strong Star = same level only
		**Invocation property says (Biba): a subject cannot invoke a subject at a higher integrity level. = no write up
-	**All other models are **rule-based**, meaning specific rules dictate how security operates. is a set of rules that mediate access between subject and objects. 
		-	Information Flow - models track the flow of information and can help uncover vulnerabilities and insecurities like covert channels.
		-	Covert channels are unintentional information communications/disclosure paths; two types exist: storage and timing. 
				**timing -huge/sudden pizza order delivery hinting something big... when they exist, confidentiality may be compromised
				**storage - sensitive info intentionally remain in memory of a laptop
		-	Clark‚ÄìWilson - focuses on enforcing integrity in commercial transaction systems. Well‚Äëformed transactions
				**Prevent authorized subjects from making bad changes
				**Maintain consistency of the system
				**Prevent unauthorized subjects from making any changes (this is the only of the three that Biba addresses)
				**Rules of integrity: Well-form transaction(Good, consis**tent, validated data.), Separation of Duties (One person shouldn‚Äôt be allowed to perform all tasks related to a critical function.) and Access Triple (subject cannot directly access object but must go through a program that enforces access rules)
		-	Brewer‚ÄìNash (Chinese Wall) -   one primary goal: Preventingconflicts of interest.
				**AKA the 'cinderella' or 'chinese wall' model, it's core rule is dynamic separation and protect from Conflict of interest
				**stipulates and ensures that information flows between subjects and objects are only allowed if the information does not provide a conflict of interest
				**ie. Development and Production departments in an organization, should not be able to influence each other or even allow access between each other.
		-	Graham‚ÄìDenning - integrity-focused rule-based model that specifies rules allowing a subject to access an object.
		-	Harrison‚ÄìRuzzo‚ÄìUllman - like GD focusing integrity via a finite set of rules to assign access rights of a subject to an object. It adds the ability to add generic rights to groups of individuals.
	
	
**3.2.5 Certification and Accreditation   ..... pg 276 in Destin**
	-	**Certification** is the **comprehensive technical analysis** of a solution to confirm it meets the desired needs.
	-	**Accreditation is **management‚Äôs official sign-off** of certification for a predetermined period of time, then the certifcation/accreditation processes repeated
	-	An architectures..especially security architectures‚Äîare built, products are often purchased from vendors. Security today often relies on solutions and mechanisms provided by vendors. 
	-	would need an independent and objective measurement system that vendors can use for evaluation and purchasing purposes, which is called **evaluation criteria systems**, 3 system well known:
		-	Trusted Computer System Evaluation Criteria (TCSEC)**	‚Äîalso known as the **Orange Book**
				**was written as part of a series of books known as the ‚Äúrainbow series,‚Äù published by the US DOD in the ‚Äô80s. different books each with a different color
				**There‚Äôs one book called the Light Blue Book that deals with password guidelines, while the Red Book deals with network security, and the Orange Book focuses on measuring security products.
				**Focus on Confidentiality only, Military / government systems, Mandatory Access Control (MAC) and BLP model, Old, rigid, confidentiality-focused.
				**it only measures single-box type of architectures; it does not map well to networked environments.
				**Functional levels Ratings: D ‚Üí C ‚Üí B ‚Üí A (A1 highest)
		-	The European-equivalent of TCSEC called **Information Technology Security Evaluation Criteria (ITSEC)**
				**Focus on all 3 CIA triad (broader than TCSEC and includes network environment), Separates functionality from assurance
				**Improvement over TCSEC but still fragmented internationally, Flexible security targets
				**"E" for Levels of assurance Ratings: E0 ‚Üí E6 (E6 highest) and also "F" Functional level rating which same as the Orange Book
		-	An ISO standard, called the **Common Criteria (ISO/IEC 15408)** 
				**Common Criteria is an international evaluation standard that assesses security functions and assurance levels using **Protection Profiles, Security Target and Evaluation Assurance Levels**
				**International standard replacing TCSEC and ITSEC
				**Focus on CIA triad and Customizable security requirements
				**4 Evaluation processes : 
					**Protection Profile** (PP) ‚Äì ‚ÄúWhat are needed‚Äù -  lists the security capabilities that a type or category of security products should possess
					**Target of Evaluation** (TOE) - a vendor desires their target product to be rated according to the Common Criteria,  under eval of functional and assurance security capabilities
					**Security Target** (ST) ‚Äì ‚ÄúHow this product meets it‚Äù - from the vendor's perspective‚Äîeach of the firewall's security capabilities that match up with capabilities outlined in the Protection Profile.
					**Evaluation Assurance** Levels (EAL 1‚Äì7, 7 most rigorously tested, EAL7/6/5 too high-maintenance/cost/risks, typically EAL3 for OS and EAL4 for Firewall), same in lifespan until major product changes
				**It‚Äôs called the Common Criteria because several countries joined together with a common goal: to create a common measurement system that could be trusted globally. 
				**For example, if a German-made product is rated, the rating can be trusted by US-based companies, because the rigorous rating process is independent and objective and globally applicable. 
				**To make this possible, globally dispersed, independent, Common Criteria‚Äìlicensed organizations evaluate and rate products. 
			
	Evaluation criteria systems 
		**Timeline Trick**
			Orange ‚Üí Europe ‚Üí World
			TCSEC ‚Üí ITSEC ‚Üí Common Criteria
		**Scope Trick**
			TCSEC = Top Secret focus
			ITSEC = Improved triad
			Common Criteria = Custom Criteria


3.3  Select controls based upon systems security requirements
3.3.1 Security Control Frameworks

**Security control frameworks aid with the control selection process.
**Security control frameworks provide guidance, based upon best practices.
**Features from multiple frameworks can be used to meet the needs of an organization.
**When considered, especially mitigating controls, control frameworks can be utilized to aid with the control selection process. Control frameworks provide comprehensive guidance, based upon best practices
**Understand the major frameworks at a high level, especially ISO 27001/02, which is an internationally recognized framework
		**ISO 27001
		**ISO 27002
	

COBIT
ITIL
NIST SP 800-53
PCI DSS
COSO
HIPAA
FISMA
FedRAMP
SOX
	
**The Ring Model: 4 ring model that seperates Users (Untrusted) from the Kernel (Trusted). The full model is slow and rarely used; most OS‚Äô only use rings 0 and 3. The applications are at layer 3. There is a new addition to the Ring Model: Hypervisor mode is called Ring -1 and is for VM Hosts. Ring -1 sits below the Client kernel in Ring 0.	


3.4 Understand security capabilities of information systems (IS) (e.g., memory protection, Trusted Platform Module (TPM),encryption/decryption)
3.4.1 RMC, Security Kernel, and TCB

**Security within information systems always pertains to **subjects and objects**.
**The **Reference Monitor Concept (RMC)** is a concept.
**Implementation of the RMC is known as a **security kernel**.
**A security kernel should consist of three properties, or characteristics: **completeness, isolation, and verifiability**.
**The term **Trusted Computing Base (TCB)** refers to all the protection mechanisms within an architecture; the TCB is the totality of protection mechanisms within an architecture.

**subjects and objects**
	-	Subject: Active entities A subject is a person, process, program, or anything similar that actively tries to access an object.
	-	Object: Passive entities An object is anything that is being passively accessed by a subject, like a file, server, process, or hardware component.
**Reference Monitor Concept (RMC)**
	-	The RMC is simply the concept of a subject accessing an object through some form of mediation that is based on a set of rules, with this access being logged and monitored. 
	-	there‚Äîas a subject is accessing an object‚Äî based on a set of rules, and this activity is logged and monitored.
	-	Need features like: Must mediate all access, Be protected from modification, Be verifiable as correct and Always be invoked

**Security Kernel**
	- 	The implementation of the reference monitor concept (concept only) is known as a **security kernel**, that is must be an implementation of a system that actually controlling access 
	-	When implemented, **a viable security kernel should contain three properties or characteristics**
			-	completeness - it is impossible to bypass the mediation/security kernel, subject must always go through the security kernel when accessing the object.
			-	isolation - relates to the mediation rules and specifically ensures that the mediation rules are tamper-proof. Only authorized individuals should be able to change these rules.
			- 	verifiability - relates to the aspect of assurance. It means being able to verify that the security kernel is functioning correctly by means of logging and monitoring, and other forms of testing.

**Trusted Computing Base (TCB)**	
	-	Refer to all the protection mechanisms within a system, within an architecture; the TCB is the totality of protection mechanisms within an architecture.
	-	all the security controls that are implemented to protect an architecture 
	-	Examples of components that would be within the TCB include all the system kernel, hardware, firmware, and software processes that make up the security system.
	
3.4.2 Processors (CPUs)
**A central processing unit (CPU) is the brain of a computer; it processes all of the instructions and ultimately solves problems.
**CPU processing involves an ongoing, four-step process: Fetch, Decode, Execute, and Store.
	- Fetching instructions and data- 
	- Decoding instructions-
	- Executing instructions-
	- Storing results-
**A CPU operates in one of two states: the supervisor state or the problem state - operating modes for the processor that restrict the operations that can be performed by certain processes.
	- Supervisor State (higher privillege)-  Typically, where the system kernel runs, allowing full access to all of the instructions and capabilities of a CPU
	- Problem state (lower privilege) - Limited access to CPU instructions, this is the standard operation mode of the CPU, means does the "solve problems" job

3.4.3 Process Isolation
**Prevents interactions that could result in negative consequences
**Two primary methods: memory segmentation and time-division multiplexing
**From a security perspective, process isolation is a critical element of computing, as it prevents objects from interacting with each other
**the actions of one object should not affect the state of other objects and their resources.... acomplished by **Memory segmentation and Time-division multiplexing**
	-	Time-division multiplexing - more to the CPU. With time-division multiplexing, process isolation is determined by the CPU allocates very small slots of time to each process.
	-	Memory segmentation - all about separating memory (RAM)segments from each other to protect the contents, including processes thatmay be running in those segments. - each segment accessible by one application only 

3.4.4 Types of Storage
**Two types of storage: primary and secondary storage
**Primary storage is small, fast and Volatile‚Äîdata is lost when device gets powered off (RAM, CPU register, cache)
**Secondary storage is slow and non-volatile (Disks, optical, virtual memory or tapes storage)
Faster -----------------------------------> Slower
Register -> Cache ----> RAM -----> Disks 
Small cap  -------------------------------> Large cap

3.4.5 System Kernel
**Core of the operating system that has complete control over everything in the system
**The system kernel and the security kernel are NOT the same thing.
**Relies on privilege levels for smooth and safe operation
**As noted, the system kernel drives the operating system. The security kernel is the implementation of the reference monitor concept.
**security perspective, it‚Äôs critical to protect the system kernel and ensure that it is operating correctly, and privilege levels aid in this regard.

3.4.6 Privilege Levels
**Privilege levels establish operational trust boundaries for software running on a computer.
**User mode results in lower trust and only allows access to a small subset of system capabilities.
**Privileged mode, also known as kernel mode, results in higher trust and allows access to more system capabilities.
**The ring protection model describes a form of CPU layering that is designed to protect critical elements of a computing system, Right level 0-3
	-	Ring 0 *system kernel* is the most trusted and thus the most secured ring which runs critical system related processes such as firmware
	-	The idea behind the model is that each ring communicates with the adjacent ring via system calls, protect from malware
	-	The outer rings can only communicate with the inner rings via the most trusted system calls.
	-	Ring 1 *drivers**
	-	Ring 2 *library*
	-	Ring 3 *user programs*
** Firmware is software that provides low-level control of hardware systems; it‚Äôs the code that boots up hardware and brings it online. may be modified or exploitable

3.4.7 Middleware
**Middleware acts as an intermediary between two applications.
**Middleware is a layer of software that can speak the languages of two disparate applications and thereby facilitate communication between them.
**Example would be bank mainframe and web app tied in together ...A translator‚Äîmiddleware‚Äîmust be present. Middleware is an intermediary that allows disparate applications to communicate with each other.

3.4.8 Abstraction and Virtualization
**Abstraction refers to the underlying complexity and details of a system being hidden. ... like driver dont think about complexity of car mechanism when driving 
**Examples of abstraction include driving a car and computing. 
**Virtualization extends the computing example further.
**Virtualization - Carrying the concept of abstraction further, virtualization is the process of creating a virtual version of something to abstract away from the true underlying hardware or software.
** A hypervisor serves as a layer of abstraction between underlying physical hardware and virtual machines (VMs).

3.4.9 Layering/Defense-in-Depth
** Protection of an asset is best accomplished through the implementation of multiple control layers.
** is the protection of a valuable asset should never rely on just one control. If that control fails, the asset would be unprotected.
** This is the concept of layered security, where multiple layers of controls exist. If there‚Äôs a failure at one layer, controls at other layers can effectively protect whatever valuable asset, like sensitive research and development data.

3.4.10 Trusted Platform Modules (TPM) - incorporates the international standard denoted by ISO/IEC 11889
** A trusted platform module (TPM) is a piece of hardware that implements an ISO standard, resulting in the ability to establish trust involving security and privacy.
** A TPM is an independent component of a computing system and functions similarly to a black box.
** Binding and sealing are important elements that help a TPM maintain integrity.

** A TPM is a chip that performs cryptographic operations like key generation and storage in addition to platform integrity. hardware‚Äîusually installed on the motherboard‚Äîthat
** For example, when a machine boots the TPM can be used to identify if there has been any tampering of critical system components, in which case the system wouldn‚Äôt boot.
** TPM is a black box, meaning that commands can be sent to the TPM, but information stored within the TPM cannot be extracted. 
** TPMs do not rely on an operating system or components external to the device for processing instructions;

** every TPM chip is unique because a unique and secret endorsement key is burned into the chip during production. An endorsement key is a special purpose RSA key that remains hidden and can only be used
for encryption, which allows for TPM authentication.

**Binding** ‚Äì A cryptographic operation in which data is encrypted in such a way that it is tied (bound) to a specific TPM‚Äôs hardware and software configuration. For example, encryption keys that are
stored on a TPM can be bound to it, ensuring that keys are only accessible by that specific TPM and that the system‚Äôs integrity has not been compromised.

**Sealing** ‚Äì A cryptographic operation that involves encrypting data. However, unlike binding, sealing is not tied to the TPM‚Äôs state or configuration. Instead, sealing is used to only allow the data to be
decrypted in certain conditions, such as in the presence of certain software or after user authentication.

3.5 Assess and mitigate the vulnerabilities of security
architectures, designs, and solution elements
3.5.1 Vulnerabilities in Systems

** A single point of failure is something that exists in a system or security architecture, when a failure is realized, will result in negative operational impact of the whole system....
** Redundancy can help alleviate the risk associated with a single point of failure, and it should be implemented where it is cost-justifiable.
** Bypass controls are a potential vulnerability or new source of risk, but they are intentional..... example such as reset home wireless router to factory default
** The risks associated with bypass controls can be mitigated using other compensating controls like ... segregation of duties, logging and monitoring, and physical security.
** Time-of-Check Time-of-Use (TOCTOU), also known as a race condition, represents a short window between two events, typically when something is used and when authorization for that use is checked.
** Example of race condition ... A user or process attempts to ‚Äúrace in‚Äù and make changes to a system before another check to confirm that access is still appropriate.
** Frequent access or authorization checks can reduce the risk of race conditions.
** Emanations are unseen elements leaking out of systems that might reveal confidential and valuable information if captured and analyzed with the proper equipment. wifi, magnetic waves, shoulder surfing

** Shielding (tempest), white noise, and control zones can prevent emanations from being captured.
	-Shielding (TEMPEST) - Walls, Faraday cages, copper-lined envelopes, and othermethods of preventing sensitive information from leaking out or being intercepted. TEMPEST is a specification that covers techniques for shielding equipment to prevent emanations from being detected
	-White Noise -  Strong signal of random noise emanated where sensitive information is being processed
	-Control zone- Preventing access or proximity to locations where sensitive

3.5.2 Hardening
**Hardening is the process of looking at individual components of a system and then securing each component to reduce the overall vulnerability of the system.
**What drives hardening decisions?
**The most important question to ask is, ‚ÄúWhat is this system meant to do?‚Äù That will guide the hardening effort. If a system is supposed to act as a web server, then it shouldn‚Äôt have fifty different ports open and services installed, as that heavily increases an attacker‚Äôs chances of breaching it.
**  If a vendor checklist does not exist, Center for Internet Security (CIS) and similar organizations publish hardening guidelines, which are great starting points and can then be customized as needed.
** Each time a system is deployed, a hardening procedure should be followed, and after each hardening process the resulting configuration should be verified to confirm the system is working as expected.

3.5.3 Risk in Mobile Systems
**Mobile device management (MDM) and mobile application management (MAM) solutions help organizations secure devices and the applications that run on them.
**Mobile device management solutions should particularly focus on securing remote access using a VPN and end-point security as well as securing applications on the device through application whitelisting.
**small-form factor computing devices that are unbelievably powerful for their size and are typically carried in pockets and purses. The fact that they‚Äôre small and powerful can allow them to store and access so much data and their mobility presents significant risk to most organizations.

**Control and management - What is the primary difference between MDM and MAM?
	-	MDM software allows a security administrator to perform tasks like enforcing different security controls or even wiping a device remotely
	-	Mobile application management (MAM) software can secure applications that interact with corporate data. Note that oftentimes the two are included within a single application.
	-	MDM and MAM can be combined with policy enforcement, application of device encryption, and related policies to adequately protect mobile devices if they are lost or stolen.

What are ways to reduce risk associated with mobile devices and workers?

Policies: One of the best ways to reduce risk related to mobile devices is using policies, like: Acceptable Use, Personal Computers, BYOD/CYOD (Bring Your Own Device/Choose Your Own Device), and Education, Awareness, and Training.

Process related to lost or stolen devices: Typically, this involves notification of IT and security personnel as well as a means by which the device can be remotely wiped. Note that remotely wiping is dependent upon the device being connected to the internet and a savvy attacker can easily prevent this from happening.

Remote access security: VPN and 2FA capabilities should be enabled by default, to prevent a mobile device from being used to connect to a remote network in an insecure manner 

Endpoint security: Antivirus/malware, DLP, and similar MDM-provisioned software should be installed on mobile devices just like standard computing equipment. Additionally, the concept of hardening should be employed to minimize the potential attack surface of the devices.

Application whitelisting: Organizations should control which
applications users may install on their mobile device through
application whitelisting and not allow them to install anything not
present on the approved application list.

3.5.4 OWASP Mobile Top 10
**The Open Web Application Security Project (OWASP) Foundation is an organization that is driven by community-led efforts dedicated to improving the security of software, including software and
applications that run on mobile devices. 
**The OWASP Top 10
**The OWASP Mobile Security Testing Guide is a manual for mobile application security testing and reverse engineering for mobile security testers.

the globally recognized OWASP Top 10 and OWASP Mobile Top 10 lists that are based on data from a variety of sources like security vendors and consultancies, bug bounties, and numerous organizations located around the world.

The key element collected
in every case is the Common Weakness Enumeration (CWE) and
associated software or hardware that contain the CWE. In addition
to collected data, OWASP surveys members of the community to
identify potential new categories for inclusion in the Top 10.

OWASP Mobile Top 10 adheres loosely to OWASP Top 10 methodology, with the focus and categories being on mobile applications.

What are the top vulnerabilities on mobile devices?

In addition to the valuable information provided in the OWASP Mobile Top 10 list, the OWASP Foundation has developed a security standard for mobile applications that helps with security testing and reverse engineering. It‚Äôs called the Mobile Application Security Testing Guide (MASTG). Another interesting OWASP project is the Mobile Application Security Verification Standard (MASVS), which helps guide secure development and testing for mobile
applications.

3.5.5 Distributed Systems
**Distributed systems are systems that are spread out and can communicate with each other across a network. The internet is a great example of a distributed system.

**Distributed file systems are systems where files are spread across multiple hosts and made available via sharing across a network.

**Grid systems are interconnected systems that are usually working together to solve a specific and usually very complex problem.

A great example of the world‚Äôs largest distributed system is **the internet**. A company network is an example of a distributed system. Although there is significant value
in connecting the systems within an organization and then connecting the organization to the internet, there are also significant risks,

What is an underlying risk related to distributed file systems (DFS)?
**Distributed systems are a number of different systems that are networked together and can communicate with each other

**Distributed file systems (DFS) take the concept of distributed systems a step further by allowing files to be hosted by multiple hosts and shared and accessed across a network. DFS software helps manage the files being hosted and presents them to users as if they‚Äôre stored in one central location.

Grid Computing
**Grid computing is like distributed systems as it still relates to systems that are connected together, but the thinking behind grid systems is that they‚Äôre usually connected via a very high-speed connection to serve a greater purpose than simply passing the occasional email or file back and forth.
**A grid systems are multiple systems working together to solve very complex problems that require more computing power that one system can provide; so, a number of systems are interconnected into a grid and programmed to work in unison to solve difficult problems.
**Example , users downloaded "SETI at Home" screen saver on their home computer and processed these little chunks of data. And in doing so they essentially created the world‚Äôs largest distributed grid computer.
** data integrity and data validation can be comprised 

3.5.6 Inference and Aggregation
**Data warehouse
	-	To perform data analytics from a number of different data sets in different systems, with the hope of identifying interesting bits of information. 
	-	Each Data set are its own island , DW is bringing them to one location for easy analyze and search and trending  
	-	Security risks to  availability. If the data warehouse goes down, access to valuable data insights could be lost.
	-	Also to the fact that if someone gains unauthorized access to the data warehouses, they could have access to significant amounts of valuable information ... all at once
	-	Need redundancy and fine-grained access control which are more complex in DW
	
**Big data
	-	Similiar useage to DW but difference in variety, volume, and velocity.
	-	**Variety** refer to that any kind of data from any source can be found within a big data repository, not just relational data can be stored only data in a clean table format, with rows and columns as in DW
	-	**Volume** refers to the size of the data sets. With a data warehouse, storage is typically restricted to the storage capacity of a single system; with big data, storage spans multiple systems. ie. Hadoop
	-	**Velocity** refers to the fact that data can be ingested and analyzed very rapidly in big data‚Äîeven faster than is possible with data warehouses.
	-	Big data tools: include Hadoop, MongoDB, and Tableau.
	
**Data Mining and Analytics
	-	Through the analysis of seemingly disparate data, otherwise invisible relationships and little nuggets of valuable information can be gleaned.
	-	Aggregation pulls data into one location. 
	-	Inference tries to infer things; it tries to identify bits of information in the data.
	-	inference, especially unauthorized inference, can create a significant risk to an organization.
	-	Unauthorized inference can take information from the hands of key decision makers or secure systems and expose it to an entire organization, the competition, or even to the enemy.
Aggregation -	Collecting, gathering, or combining data for the purpose of statistical analysis
Inference - 	Deducing information from evidence and reasoning rather than from explicit statements

Understand the difference between aggregation and inference and how inference can be mitigated

Reduce Risk of Inference and Aggregation One method to reduce the risk of unauthorized inference is using ‚Äúpolyinstantiation,‚Äù which allows information to exist in different classification levels for the purpose of preventing unauthorized inference and aggregation

3.5.7 Industrial Control Systems (ICS)

**Industrial control system (ICS) is a general term used to describe control systems (HW and SW) related to industrial processes and critical infrastructure: factory, power/nuclear,manufacturing plants, 
**Three primary types of ICSs: Supervisory Control and Data Acquisition(SCADA), Distributed Control System (DCS), Programmable Logic Controller (PLC)
**Due to their inherent complexity and the things they help control/manage, ICS can be quite vulnerable to attack; the best way to reduce risk to ICS is to keep them offline‚Äîto ‚Äúair gap‚Äù them from direct or indirect access to the internet.
** Operational technology (OT) is a broader term than ICS. It refers to the hardware and software technology used to monitor and control physical processes, devices, and industrial systems. ICS is a subset of OT that focuses on the control and automation of industrial processes.

Reduce Risk in Industrial Control Systems
Understand the risk associated with ICS and how best to reduce this risk

One of the best ways to protect ICS is keeping them offline, also
known as ‚Äúair gapping‚Äù or creating an ‚Äúair gap.‚Äù What this simply
means is that ICS devices can communicate with each other, but
the ICS network is not connected to the internet or even the
corporate network in any way. So, even if someone does try to
connect to these ICS systems from the internet or corporate
network, they‚Äôll be unable to do so.

Patching Industrial Control Systems
	-	Understand the implications of patching ICS or alternative ways to mitigate risk if patching is not possible
	-	Industrial control systems by their very nature are difficult to maintain, especially where security is concerned. Often patching of ICS has been avoided, as patching these critical systems may cause unintended consequences and downtime.
	-	Strong configuration management processes, good patch management and backup/archive plans, and so on should be in place and used when and where possible.
	-	When patching ICS systems is not possible (or not possible to the degree needed), additional mitigating steps can be taken to reduce the risk and impact of disruption of critical infrastructure:

Understand each type of ICS at a high level
	-	Implement nonstop logging and monitoring and anomaly detection systems to rapidly detect nefarious activities within ICS networks.
	-	Conduct regular vulnerability assessments of ICS networks, with particular focus on connections to the internet or direct connections to internet-connected systems, rogue devices, and plaintext authentication.
	-	Use VLANs and zoning techniques to mitigate or prevent an attacker from pivoting to other neighboring systems if the ICS is breached.
	-	Additionally, privileged access management and privilege task automation tools can potentially be deployed to help manage risks associated with legacy systems often found within ICS.
	
Three major types of Industrial Control Systems (ICS)
**SCADA (Supervisory Control and Data Acquisition)
	-	System architecture that comprises computers, networking, and proprietary devices as well as graphical interfaces for management of the entire system
	-	Used to manage	small and large-scale industrial, infrastructure, and facility processes
**DCS (Distributed Control System)
	-	Process control system that monitors, controls,and gathers data from components like controllers, sensors, and other devices typically found in large processing facilities.
	-	DSC mostly can only be controlled locally ...Unlike SCADA which includes local and remote management capabilities
**PLC (Programmable Logic Controller)
Industrial computer, specifically used for the control of manufacturing processes Key features include high reliability, ease of programming and diagnosis of process problems. Often networked
with other PLC devices and SCADA systems

3.5.8 Internet of Things (IoT)
	-	Internet of Things (IoT) refers to all the devices, like home appliances, that are connected to the internet.
	-	IoT devices, by their nature, are risky. Reducing their risk involves making different purchase decisions, taking every precaution when installing and keeping the technology up to date.
	-	Risks: built-in computer and networking, never updated, cheap, insecurely configured password, vunlerable to network open doors
	-	Prone to malware and DDOS attacks if not secured 
	-	Reduce Risk of Internet of Things (IoT) by planning, Make sure the technology remains up to date, connect them to a segregated part of the network, and ensure adequate protection is built around it and ensure you scan that network for vulnerabilities and mitigate those accordingly., and segement off the network from regular production networks to be safe ...

3.5.9 Cloud Service and Deployment Models
**Characteristics of cloud computing: on-demand self-service, broad network access, resource pooling, rapid elasticity and scalability, measured service, multitenancy 
**Cloud service models: IaaS, PaaS, SaaS, CaaS, FaaS
**Cloud deployment models: Public, Private, Community, Hybrid Protection and privacy of data in the cloud should be carefully considered.

Cloud Computing Cloud computing allows individuals and organizations to access and use computing resources 
**like servers,, storage, databases, networking, software, and more) over the internet, on a pay-as-yougo basis. 
**This enables users to access data and applications from anywhere, without having to maintain physical hardware and software infrastructure themselves. 

Cloud computing
A cloud can be a private, public, or hybrid model. It can also allow greater or smaller control to fall on the client or the cloud service provider. It all depends on what the goals are. So many options and variations exist. Some of the most common characteristics of a cloud provider are:

**On-demand self-service
**Broad network access
**Resource pooling
**Rapid elasticity and scalability
**Measured service
**Multitenancy

Cloud service models	-	There are three primary service models used in the cloud:
**Software as a Service (SaaS) provides access to an application that is rented‚Äîusually via a monthly/annual, subscriber-based fee‚Äîand the application is typically web-based.
**Infrastructure as a Service (IaaS) is an environment where customers can deploy virtualized infrastructure servers, appliances, storage, and networking components.
**Platform as a Service (PaaS) is a platform that provides the services and functionality for customers to develop and deploy applications.
**Containers as a Service (CaaS). It allows for multiple programming language stacks, like Ruby on Rails or node.js, to name a couple, to be deployed in one container. 
**Function as a Service (FaaS). It describes serverless and the use of microservices to accomplish business goals inexpensively and quickly.

Understand cloud service provider and cloud customer responsibilities, depending upon the cloud service model in use
Regardless of the nature of shared responsibility, one thing is always constant when talking about the relationship between the cloud service provider and the cloud customer: the cloud customer is always accountable for their data and other assets existing in a cloud environment.

Cloud Deployment Models - Several cloud deployment models exist

The first model is public cloud, and the name implies who can
access it‚Äîeverybody, the public. A public cloud is in the cloud
service provider‚Äôs data center and consumers are simply accessing
it as a service (e.g., Gmail). It‚Äôs accessible by anyone.

A private cloud is only accessible by a single customer, so it‚Äôs
private to that customer. If it‚Äôs an on-premises private cloud, this
means it‚Äôs in the customer‚Äôs own data center;

A community cloud is a cloud that is used by a group of users that
share common needs or interests, like a group of hospitals, for
example. One of the largest community clouds is GovCloud.

The last cloud deployment model is a hybrid cloud. A hybrid cloud
is any combination of the three previously mentioned, and it is
usually a combination of a public and private cloud.

Protection and Privacy of Data in the Cloud - What should be a primary concern of an organization considering a move to the cloud?
In addition to implementing strong access controls, strong encryption practices should be used when and where necessary to properly secure this data. This is especially true when an organization makes the initial decision to move from legacy, on-premises infrastructure to that of a cloud provider. In cases like this, best practices indicate that data should be encrypted and secured locally and then migrated to the cloud.

Cloud computing roles
Multiple computing roles relate to cloud computing: cloud consumer, cloud provider, cloud partner, and cloud broker.
	-	Cloud Service - Customer/Consumer	-	Individual or organization who is accessing cloud services
	-	Cloud Service Provider	-	The organization that is providing cloud services/resources to consumers
	-	Cloud Service Partner	-	The organization which supports either the cloud provider or customer (e.g., cloud auditor or cloud service broker)
	-	Broker	-	Carrier, Architect, Administrator, Developer, Operator, Services Manager, Reseller, Data Subject, Owner, Controller, Processor, Steward

Accountability versus responsibility
Accountability can never be delegated or outsourced. It can't be outsourced or passed down, and it always remains with the owner.

Responsibility, 
on the other hand, can be outsourced, and this often happens to a great extent when working with a cloud service provider.

Compute in the cloud Hypervisors, virtual machines (VM), containers, serverless
A hypervisor, also known as a virtual machine manager/monitor (VMM), is software that allows multiple operating systems to share the resources of a single physical machine. On the other hand, a virtual machine (VM) resembles a computer, but everything is emulated using software.

One of the characteristics of cloud computing is resource pooling, which describes the relationship between the fundamental hardware that makes up the compute, storage, and network resources and the multiple customers that utilize those resources. Cloud customers can access compute resources through:

Monolithic vs. Microservices vs Serverless Computing
Monolithic
**All functionality of a monolithic application is wrapped together as a single unit, whereas with microservices and serverless, functionality is more defined and self-contained in smaller or individual units.
** typically comprise of a back-end database, an application and a user interface. Correspondingly, this implies a single large code base, and changes to an application may require updates to all three areas. 
Microservices (Container)
**a function that exists and operates as one unit, microservices exist and function as separate units that are loosely coupled via API calls.
**The fact that an application is composed of multiple, loosely coupled components allows for better overall understanding of the application, and functional components can be reused across multiple applications.
Serverless(FaaS)
**The term serverless takes the basic premise of microservices‚Äîhyper focused, independent pieces of functionality coupledtogether through APIs‚Äîand extends it to the cloud

3.5.11 Cloud Forensics
**Focus is on the forensic process in cloud computing environments
**Typically, more complex than on-premises forensic investigations
**Virtual disks and VM images are often analyzed as part of cloud forensics

The following table shows the type of forensic evidence that can be acquired based on the cloud model being used:

SaaS
Consumers must rely entirely on CSP
PaaS
For underlying infrastructure, consumers must rely entirely on CSP
Consumer is responsible for any application layer code they deployed and application logging
IaaS
Consumers can perform forensic investigations on their VMs
Investigation of network traffic, access to snapshots of memory, or the creation of hard disk images may require investigative support by the CSP

NIST published a document in August 2020 entitled ‚ÄúNIST Cloud Computing Forensic Science Challenges‚Äù that summarized research in this area by members of the NIST Cloud Computing Forensic Science Working Group identified 9 main areas of challenge categories related to cloud forensics as outlined below:
1.   Architecture
2.   Data collection
3.   Analysis
4.   Anti-forensics
5.   Incident first responders
6.   Role management
7.   Legal
8.   Standards
9.   Training

3.5.12 Cloud Computing Roles
**Multiple computing roles relate to cloud computing: cloud consumer, cloud provider, cloud partner, cloud broker
**The cloud consumer is always accountable for their data stored in the cloud .. Responsibility can be delegated to other cloud computing roles.
**Data controller = owner of data = cloud customer = accountable -  regardless of the terms of the SLA, accountability always remains with the asset owner.
**Cloud Service Provider - Organization that is providing cloud services/resources to consumers
**Data processor = processor of data = cloud provider or other agent of the customer = responsible
Cloud Service Partner - Organization which supports either the cloud provider or customer (e.g., cloud auditor or cloud servicebroker)
**Cloud service broker representatives provide service aggregation services to customers. Carrier, Architect, Administrator, Developer, Operator, Services Manager, Reseller, Data Subject, Owner, Controller, Processor, Steward

3.5.13 Cloud Identities (pg383)
**Third-party identity provider is a trusted organization that manages user identities and related attributes for purposes of authentication and authorization.
**Identity federation involves protocols, standards, practices, and policies that support identity portability and trust relationships among unaffiliated resources and organizations.
**SPML enables the automation of adding users to multiple cloud services.
**On-premise IAM solutions include Microsoft Active Directory and LDAP based.
**Cloud-based IAM solutions include those offered by Amazon, Google, and many other cloud vendors.
**Identify as a Service (IDaaS) refers to cloud-based IAM services.
**Identity moves to the cloud; access control follows federated trust, not local accounts.

**Identity and Access Management (IAM) in any context can be challenging, and especially so in the cloud, with one of the most significant challenges being that of provisioning users to multiple disparate resources spread across multiple cloud services.

**Identity as a Service (IDaaS) - One of their primary advantages is supports scalability, availability, and centralized control, but introduces third-party and trust risks.
IDaaS is a cloud-based IAM model where identity lifecycle, authentication, authorization, and federation are handled by a third-party provider (e.g., Azure AD, Okta, Ping).
Centralized identity management
Supports SSO, MFA, and federation
Integrates on-prem and cloud apps
Reduces password sprawl and admin overhead

**Federated identity (FIM) -  operates in a similar context to IDaaS.Purpose: Automates identity provisioning and deprovisioning
Create, modify, delete user accounts
**FIM extends the functionality of IDaaS to include multiple resources and organizations. Standards, protocols, and technologies that support FIM include Services Provisioning Markup Language (SPML), Security Assertion Markup Language (SAML), OAuth and OpenID


SPML (Service Provisioning Markup Language)
Purpose: Automates identity provisioning and deprovisioning
Create, modify, delete user accounts
Synchronizes identities across systems
XML-based (older, but exam-relevant)
üìå Exam clue: ‚ÄúAutomated user account provisioning across cloud services‚Äù
SPML = account lifecycle management


**Security Assertion Markup Language (SAML)  is an XML-based, OASIS standard that utilizes security tokens that contain assertions about a user. SAML facilitates service requests made by users to service providers in the form of requests to identity providers, which‚Äîif the user is authenticated/authorized‚Äîresult in SAML assertions allowing the user access to the service.
Purpose: Authentication & SSO across trust boundaries
Uses XML assertions
Common in enterprise SSO
Identity Provider (IdP) ‚Üî Service Provider (SP)
üìå Exam clue to be SAML:‚ÄúUser authenticates once and accesses multiple SaaS apps‚Äù
üìåSAML handles authentication, not authorization logic
SAML - Authentication

**OAuth (Authorization) - OAuth (authorization) is a Federated Identity Management (FIM) open-standard protocol that typically works in conjunction with OpenID (authentication). OAuth provides users and applications with ‚Äúsecure delegated access‚Äù via access tokens versus credentials
Authorization for APIs ‚Äî OAuth (and OpenID Connect) OAuth 2.0
Purpose: Authorization delegation
Grants access tokens
Used by APIs and microservices
No password sharing
üìå Exam clue: ‚ÄúAllow a mobile app to access cloud resources on behalf of a user‚Äù
üìåOAuth = what you can access, not who you are  ...  

Protocol	Primary Function	Used For	CISSP Keyword
SPML	Provisioning	Account lifecycle	User management
SAML	Authentication	SSO, federation	Trust assertions
OAuth	Authorization	API access	Token delegation
OIDC	Authentication	Cloud/mobile login	Identity claims

A company wants automatic account creation and removal when employees join or leave. - SPML
Users authenticate once and access multiple SaaS apps without re-entering credentials. ‚úÖ SAML

A mobile app accesses a cloud API without storing user passwords.
‚úÖ OAuth

OAuth is used to authenticate users.
‚ùå Wrong
‚úî OAuth authorizes access; OIDC authenticates users

üß† Memory Trick (CISSP-Friendly)
SPML ‚Üí Staff Provisioning
SAML ‚Üí Single Sign-On
OAuth ‚Üí API Access
OIDC ‚Üí OAuth + Identity
üèÅ CISSP Bottom Line
Cloud IAM = federation + tokens + lifecycle automation
Identity is centralized, not local
Protocols solve different IAM problems
Always map the protocol to what the question is really asking


3.5.14 Cloud Migration (pg 386)
**Cloud migration involves benefits and risks that should be carefully considered 
**One significant risk of cloud migration is vendor lock-in
**Security in the cloud should be understood thoroughly, and organizations should work closely with the cloud service provider to implement security that follows best practices.

Cost shifting ...
Benefits of such a move include shifting costs from a capitalization expense (CapEx) model, where networking and computer equipment is owned by the organization, to an operations expense (OpEx) model where
compute, storage, and networking costs are borne by the cloud service provider and paid for by the organization on an ‚Äúas needed‚Äù basis.

Focus shifting ...
This shift, though not necessarily a huge cost-saver, can result
in considerable efficiencies gained. Additionally, it‚Äôs in a cloud
service provider‚Äôs best short- and long-term interest to provide
reliable technology and support to its customers. This further shifts
the load away from the organization so it can focus on core
business activities.

efficiencies and scalability ...
Moving to the cloud makes applications, services, and
data accessible from anywhere, using virtually any type of device,
as long as an internet connection is available

security 
igration to the cloud also facilitates the centralization of data,
which can further facilitate safe storage and backup of data.

Among Cloud migration risk, what is one of the most important things to consider? ..................

most important considerations of cloud migration
relates to vendor lock-in‚Äîthe notion that once migrated, an
organization is ‚Äústuck‚Äù with the cloud service provider and will be
unable to move elsewhere

Security related to cloud migration must be carefully
considered and addressed. 

3.5.15 Edge Computing
Edge computing is a distributed computing approach that can
reduce latency, speed up response times, and increase bandwidth
availability. Instead of processing all of the data in a central data
center, much of the processing is done closer to the source of the
data, often on the devices themselves, or on a local server

 some important concepts related to edge computing.
Ingress
Ingress traffic is traffic entering a network. In edge computing,
ingress traffic is often generated by users who are accessing
services that are hosted at the edge

Egress
Egress traffic is traffic exiting a network. In the context of edge
computing, this is generally data sent from services at the
edge, either back to users, or to another network.

Peering
Peering is the interconnection between separate networks for
exchanging traffic, This approach allows them to exchange
traffic without going through the Internet. ISPs often have
agreements between themselves to make it easier to send
data to one another.

Secure Access Service Edge
Secure access service edge (SASE), pronounced sassy, is a suite of
technologies that is often looked upon as the future of wide area
networks (WANs).

It combines network security and wide area
networking into a cloud-based service. It aims to get data and
services as close to the end users as possible

3.5.16 XSS and CSRF
**The topic of assessing and mitigating vulnerabilities in web-based systems is important, because the prevalence of web-based
applications only continues to grow.
**Cross-site scripting (XSS) attacks
**An XSS attack involves a malicious script that is injected into a trusted website that a visitor‚Äôs browser then downloads and executes
	-	Two primary forms of cross-site scripting (XSS): stored/persistent XSS and reflected XSS

**A cross-site request forgery (CSRF) relies on persistence facilitated by cookies in browsers
**With XSS, the target of attack is the user‚Äôs browser; with CSRF, the target of attack is the web server

One-line memory anchor
XSS = attacker injects script that runs in the user‚Äôs browser
CSRF = attacker tricks a user‚Äôs browser into sending a valid request

Cross-Site Scripting (XSS)
What it is (CISSP definition): Injection attack where malicious script executes in the victim‚Äôs browser under a trusted site‚Äôs context.
Key CISSP focus
Client-side attack
Steals cookies, tokens, sessions
Exploits trust in the website, not the user

Two Testable Types of XSS
üéØA. Stored (Persistent) XSS
Malicious script is stored on the web server (DB, forum post, comment)
Executes every time a user loads the page
Example: User posts <script>stealCookies()</script> in a comment field.
 Exam memory trick:  Stored = Saved = Server-side persistence

üéØB. Reflected (Non-Persistent) XSS
Script is reflected immediately in the response
Delivered via URL, form input, search field
Requires user interaction (click link)
Example: https://site.com/search?q=<script>attack()</script>
Exam memory trick:
Reflected = Returned immediately

üõ° XSS Mitigations (CISSP-favorite answers)
Input validation
Output encoding
Content Security Policy (CSP)
HTTPOnly cookies

Cross-Site Request Forgery (CSRF / XSRF)
What it is: Attack where a logged-in user is tricked into executing an unwanted action without knowing it.
üéØ Key CISSP focus
Uses valid session/cookies
No script injection required
Exploits trust in the user
Example: User clicks hidden link that submits a fund transfer form.
üß† Memory trick: CSRF = ‚ÄúYou click it, browser sends it‚Äù

üõ°CSRF Mitigations
Anti-CSRF tokens
SameSite cookies
Re-authentication for sensitive actions
CAPTCHAs (secondary control)

XSS vs CSRF ‚Äî Exam Comparison Table
Feature				XSS				CSRF
Attack target		User‚Äôs browser	User‚Äôs session
Script injection	‚úÖ Yes			‚ùå No
Uses victim‚Äôs auth	Sometimes		‚úÖ Always
Steals data			‚úÖ Yes			‚ùå Usually performs actions
Primary threat on	Website			User

üö® CISSP Exam Traps
‚ÄúThe attacker injects JavaScript that runs in the user‚Äôs browser.‚Äù ... ‚úÖ XSS
‚ÄúThe user unknowingly performs an action while logged in.‚Äù ... ‚úÖ CSRF
‚ÄúAnti-CSRF tokens protect against XSS.‚Äù .... ‚ùå Wrong ‚Äî they protect against CSRF, not XSS.
‚ÄúHTTPOnly cookies prevent CSRF.‚Äù ....‚ùå Wrong ‚Äî they mitigate XSS session theft, not CSRF.

3.5.17 SQL Injection (pg 400)
**Structured Query Language (SQL) is the language used for communicating with databases.
**SQL Injection is a method of attack that utilizes SQL code and commands. can be used for modification, corruption, insertion, or deletion of data in a database.

**dynamic website**, meaning that web pages can be created dynamically using data from the database, based upon user requests and interaction with the website.  a persistent
connection to the database is required, but a web user should never be able to directly interact with the back-end database. However, SQL Injection makes that possible.
**web server passed unvalidated information directly to the database server.
**Using SQL Injection, instead of normal username .. a bit of malform SQL code is entered to trick SQL server to act abmornally 

**Input validation is the best method to prevent SQL Injection attacks from being successful.
Unvalidated data should never be passed directly from a web server to a database server.
**input sanitized, or otherwise made to conform to expected formatting standards
**Additionally, the use of things like prepared statement/parameterized queries and stored procedures can also help protect against SQL Injection attacks.
	-	Prepared statement or parameterized query (PS/PQ) is to think of a template of SQL code, where variables are used and passed to the query later. The separation helps prevent the intent of a query from being changed
	-	Stored procedures are like the PS/PQ but they are statements stored inside the database already compiled then ready to be invoked by the application

common SQL commands
CREATE ..SELECT FROM ..GRANT COMMIT
ALTER INSERT INTO... REVOKE ROLLBACK
DROP UPDATE SAVEPOINT TRUNCATE DELETE RENAME MERGE LOCK TABLE

3.5.18 Input Validation
**No input validation can lead to numerous web application vulnerabilities being exploited.
**Server-side input validation reduces web-based vulnerabilities and the risk of XSS and SQL Injection attacks from succeeding.
**Whitelist input validation only allows acceptable input.

What is the best way to mitigate web-based vulnerabilities and what types of attacks can be mitigated or prevented
**Server-side input validation‚Äîchecking the contents of input fields ‚Äîis one of the best ways to prevent XSS and SQL Injection attacks from succeeding.
**Allow list (Whitelist)** input validation only allows acceptable input that consists of very well-defined characteristics, e.g., numbers, character, or both, size, or length, to name a few formats and standards.
**Deny list (blacklist)** input validation where malicious characters can be discarded as they are considered signs of an attack, i.e., if the = or - characters are met in a ‚ÄúFirst Name‚Äù field, they can be safely discarded as a person‚Äôs first name wouldn‚Äôt need to include = or -.

3.6 Select and determine cryptographic solutions (pg 409)
3.6.1 Introduction to Cryptography

**The most critical aspect of cryptography is key management 
**Cryptographic systems can provide up to five services: confidentiality, integrity, authenticity, nonrepudiation, and access control 
**Cryptography is used extensively and is often all around us in many different contexts
**Cryptography is derived from two Greek words‚Äî crypto and graphia; crypto means ‚Äúsecret‚Äù or ‚Äúcovert‚Äù and graphia means ‚Äúwriting.‚Äù So, the foundation and meaning of cryptography is ‚Äúsecret writing‚Äù‚Äîcreating a cipher.

With any cryptographic system, one (or a combination of services) denoted in the following table can be achieved:
-	Confidentiality:	Confidentiality helps prevent unauthorized disclosure of information and to make data available to only those authorized to view it.
-	Integrity:	Integrity ensures that information has not been manipulated or changed by unauthorized individuals without our knowledge; it helps identify unauthorized or unexpected changes to data.
-	Authenticity:	Authenticity allows verification that a message came from a particular sender.
-	Nonrepudiation:	Nonrepudiation prevents someone from denying prior actions. There are two flavors of nonrepudiation:
		Nonrepudiation of origin: the sender cannot deny that they sent a specific message.
		Nonrepudiation of delivery: the receiver cannot deny that they received a specific message
-	Access control:	Cryptography enables a form of access control; by controlling the distribution of ciphertext and the corresponding decryption key to only certain people, control over the decryption and, therefore, access to data can also be controlled

high-level overview of the evolution of cryptography
Manual	-	Caesar Cipher
Mechanical	-	The Spartan Scytale,
Electro-mechanical	-	Enigma machine, Japanese Red and Purple cipher
Electronic	-	Most current way... software-based like PGP, and common algorithms include DES, AES, and RSA.
Quantum	-	future/exprimental 

3.6.2 Cryptographic Terminology
**Cryptography involves its own nomenclature. practice of securing communications to prevent attackers from reading or manipulating information.
**Important terms to be familiar with include: initialization vector (IV)/nonce, confusion, diffusion, avalanche, key space
	
	-	initialization vector (IV)/nonce 
	-	Plaintext -	Plaintext, also known as cleartext, is simply data that is readable by anyone.
		Plaintext (in this case, "CISSP is awesome") is provided as input into a cryptosystem, and a cryptographic algorithm transforms it into ciphertext. 
	-	Encrypt/ Encryption -	Encryption is the process of converting plaintext into ciphertext using a cryptographic algorithm and a key/crypto variable.
	-	Key/Crypto Variable -	A crypto variable is also referred to as a key. When a given piece of plaintext is encrypted with a key, the key determines how the algorithm processes the plaintext to produce ciphertext. Once plaintext has been encrypted with a key, the only way to decrypt the ciphertext is with the appropriate key.
					-	The only way this ciphertext can be transformed back into plaintext by a recipient is through the use of a compatible cryptosystem and the same cryptographic algorithm.
	-	Key space -	The term key space refers to the unique number of keys that is available based on the length of the key. For example, a 2-bit key has a total of four possible or unique keys: 00/01/10/11
	-	Work factor -	an estimated amount of time or effort required by an attacker to break a cryptosystem. The higher the work factor, the more secure the cryptosystem.
	-	Key Clustering -	describes what happens when two different keys generate the same ciphertext from the same plaintext. This is something that should be avoided in good  cryptographic algorithms design
	-	Confusion -	focuses on hiding the relationship between the key and the resulting ciphertext.
	-	Diffusion -	focuses on hiding the relationship between the plaintext and the resulting ciphertext.
	-	Initialization Vector (IV)/ Nonce -	is a random number that is used in conjunction with the key and fed into a cryptographic algorithm when encrypting plaintext, need to prevent patterns in the resulting ciphertext.
	-	Avalanche -	looks at the degree of confusion and diffusion that an algorithm provides. resulting small changes from key or plantext should result in BIG/Avalanche changes of ciphertext

3.6.3 Substitution and Transposition (pg 419)

**Encryption involves methods known as substitution and transposition.
**Encryption is accomplished through the manipulation of bits ‚Äî1s and 0s‚Äîvia synchronous or asynchronous means.
**Patterns must be avoided. 
**When implemented and used correctly, one-time pads are the only unbreakable cipher systems. 
**Bits are encrypted/decrypted as stream ciphers or block ciphers.
	
Substitution Transposition	-Characters are replaced with a different character
GUBBINS > JXEELQV
-	is a method of encryption where every plaintext character is replaced/substituted with a different character to create ciphertext.



Transposition	-The order of characters is
rearranged
GUBBINS > BINBUGS
-	is a method of encryption where every plaintext character is shifted around/rearranged based on a given key.
-	Rail Fence (Zigzag)-	the text is transposed by writing it in a table where each row represents a rail, following a zigzag pattern.

Cryptography --> Art of converting plain text to cipher text 
Cryptanalysis --> Art of breaking the cipher
Cryptology --> Science of Cryptography and Cryptanalysis
Cryptosystem --> Implementation of code/cipher in Hardware or Software

Synchronous versus Asynchronous
**The bits of 0s and 1s are manipulated via synchronous or asynchronous methods.
**Synchronous involves working with bits synchronized through some type of timing mechanism, for example, a clock, while encryption/decryption takes place immediately.
	 - 	A timing element is involved
	 -	Encryption/decryption requests are performed immediately

**Asynchronous involves working with collections of bits, and the input is typically dictated by the user or some other element that requires input.
	-	Dictated by some other element or entity that requires input
	-	Encryption/decryptionrequests are processed in batches (queued)

**Frequency analysis	-	The activity of trying to determine keys based upon letter usage patterns is known 
simple substitution and transposition do not hide patterns in monoalphabetic ciphers. Frequency analysis can easily detect patterns in them, which can then lead to the determination of the key.

Substitution‚Äîpolyalphabetic ciphers
By using polyalphabetic ciphers, frequency analysis becomes much more difficult because patterns are reduced significantly.
The prefix poly means many, so with polyalphabetic ciphers, multiple alphabets are created and used.

Substitution‚Äîrunning key ciphers
Running key ciphers has been used since World War II. To utilize the running key cipher, the same "book" must exist at both ends of the communication channel.

Substitution‚Äîone-time pads
With a one-time pad, after every message is encrypted, the key is changed and never reused. One-time pads are the only unbreakable cipher systems.

Stream versus block ciphers
All symmetric and asymmetric algorithms in cryptography
work with bits, not letters. Once a message has been turned
into bits, two options exist with regards to how those bits
are encrypted and decrypted.

The two types of ciphers that exist are known as stream ciphers and block ciphers. Variables like speed and where it makes sense to use the block as opposed to stream help determine which cipher algorithm to use.
	-	Stream	-	Encrypt/decrypt data one bit at a time ... faster suitable for networking and hardware level
		-	Plaintext bits that need to be encrypted are combined with bits generated by a keystream generator.
		-	The bits are combined using a logical operation called "exclusive or," or XOR. The result from each XOR operation becomes the ciphertext.
		-	The most commonly used stream cipher is Rivest Cipher 4 (RC4).
	-	Block	-	Encrypt/decrypt blocks of bits at a time (typically 64-bit blocks) .... slower but have a high diffusion rate and are very resistant to tampering.
			These are five block cipher modes you need to know for the exam:  (pg 438 / 439)
				Electronic Codebook (ECB)
				Cipher Block Chaining (CBC)
				Cipher Feedback (CFB)
				Output Feedback (OFB)
				Counter (CTR)

3.6.4 Steganography and Null Ciphers
Steganography is hiding information of a particular type
within something else (like a sound file hidden in a picture).
A null cipher involves hiding a plaintext message within
other plaintext.

Steganography Null Cipher
Plaintext is hidden within
something else (e.g., a picture)

Null Cipher	-	Plaintext is mixed with a large amount of nonciphertext

3.6.5 Symmetric Cryptography (pg 442)
Symmetric key cryptography is fast.
Key distribution and scalability are major disadvantages.
Out-of-band communication can facilitate key distribution.
Know symmetric algorithms from weakest (DES) to strongest
(AES).

Understand advantages and disadvantages of symmetric cryptography
key distribution is a glaring and inherent
weakness, especially if the parties involved in
communication are separated by any amount of distance.
Out-of-band communication can be used to overcome
this weakness,

A simple formula can be used to determine
the number of keys required for a given number of people
to securely communicate with each other:

n * (n‚Äì1) / 2 = number of keys
n = number of people

Advantages :Fast/efficient / Strong
Disadvantages : 
Key distribution
Scalability
No authenticity, integrity, or nonrepudiation

3.6.6 Asymmetric Cryptography (pg 452)
Asymmetric cryptography 
**Asymmetric cryptography solves the key exchange problem associated with symmetric cryptography. It enables digital signatures, digital certificates, authenticity, and nonrepudiation (of origin and delivery) and utilizes key pairs consisting of a public key and a private key.
**Enables digital signatures, digital certificates, authenticity, and nonrepudiation (of origin and delivery)
**Utilizes key pairs consisting of a public key and a private key
**Two primary types of hard math problems: factoring and discrete logarithms
**Popular asymmetric algorithms include RSA (uses factoring) and Elliptic Curve (ECC, uses discrete logarithms)
Advantages 
	-	Solves key exchange problem
	-	Enables digital signatures and other services, like authenticity (proof of origin), confidentiality, and access control
	-	Solves scalability
Disadvantages
	-	Significantly slower
	-	Requires large key sizes
**To obtain authenticity or proof of origin‚Äîidentify with certainty who a message came from
	‚Äî	A sender should encrypt the message using the sender's private key.
	-	Anybody with the sender's public key can decrypt the message and therefore know without a doubt who sent the message
	-	As the sender is the only person having access to their private key.

Hard math problems
Factoring and discrete log asymmetric algorithms depend on using very large prime numbers. When using such large numbers, it is very difficult to work backward to determine the original integers.
 -	Factoring and discrete log asymmetric algorithms depend on using very large prime numbers. When using such large numbers, it is very difficult to work backward to determine the original integers.

Asymmetric algorithms
The following table summarizes asymmetric algorithms:
	-	Rivest, Shamir, and Adleman (RSA)	-	Uses factoring mathematics for key genertion.
	-	Elliptic Curve(ECC)					-	Uses discrete algorithms mathematics for key generation. ECC uses shorter keys than RSA to achieve the same level of security, which means ECC is faster and more efficient.
	-	Diffie‚ÄìHellman Key Exchange			-	Uses discrete algorithms mathematics for key generation is primarily used for the exchange of symmetric keys between parties

3.6.7 Hybrid Key Exchange (pg464)
**Diffie‚ÄìHellman (DFH) Key Exchange (uses discrete logarithms) is an asymmetric algorithm used primarily for symmetric key exchange.
**Hybrid cryptography blends the advantage of symmetric cryptography‚Äîextremely fast‚Äîwith the advantage of asymmetric cryptography‚Äîsolves the key distribution problem.

**symmetric key cryptography is the best, when **speed and bulk processing are required**. 
It is the only type of cryptography that can host the speeds required for being able to encrypt and decrypt fast enough, such as VPN traversing data over the network via session keys, one per session
DFH process
1. user a and b each picked a prime number
2. each user multiply their prime number by 2 and send then exchange that with the other user
3. each user multiply that given number by their own originial prime and that product would be equal for both user

Hybrid cryptography
Hybrid cryptography solutions employ the advantages of symmetric and asymmetric cryptography.
**Symmetric algorithms are used for bulk processing and speed‚Äîfor anything that requires frequent encryption and decryption and where both need to be done very quickly, 
**while asymmetric algorithms are used to exchange symmetric keys.
example of Hybrid cryptography:
	1.	Alice wants to send Bob a very large message and she can only use symmetric cryptography
	2.	Alice knows that for Bob to be able to decrypt using the exact same symmetric key
	3.	To share the symmetric key securely with Bob, Alice knows that she can encrypt it with Bob‚Äôs public key send this to Bob.
	4.	Bob can then use his private key to decrypt that message with the symmetric key sent by Alice
	5.	Once decrypted, Bob will then have the same session key, which will allow him to quickly decrypt and read Alice‚Äôs very large message
 
3.6.8 Message Integrity Controls (pg 470)
**Message integrity checks (MIC) help to ensure the integrity of a message between the time it is created and the time it is read.
**A MIC works by creating a representation of the message, which is sent with the message.
**Message integrity checks are based upon math, some more complex‚Äîand therefore more effective‚Äîthan others.
**The use of simple math can result in a collision, meaning two different messages can result in the same representation.
**Hashing is very effective as a MIC and works the same way, regardless of the length of input; the result is always a fixed length digest, based on the hashing algorithm used.
**The birthday paradox best illustrates how collisions should be avoided to maintain integrity. ...more people entered the room, more with same birthdate . end hashed values are the same with two different input 
**Cryptographic Hash Functions
Purpose: Detect any change to data
Examples: D5: 128-bit digest,SHA-1 (always 128): 160-bit digest (always 160), SHA-2: 224/256/384/512-bit digests, SHA-3: 224/256/384/512-bit digests
		-	Fixed-length output	-	Any length input always equals the same length output.
		-	One-way	-	it is not possible to determine the input of a hashing algorithm by inspecting theoutput.
		-	Sensitive to bit changes (avalanche effect)	-	It should be very hard to find two inputs that hash to the same output.
Exam hook:	Hash = integrity ONLY (no identity, no secrecy)
‚úî Correct answer: Cryptographic hash or HMAC
‚ÄúVerify software download integrity from a public website.‚Äù
‚úÖ Best answer: Cryptographic hash (SHA-256)

**Message Authentication Code (MAC)
Purpose: Integrity + authentication (shared secret)
Examples: HMAC-SHA256
	-	Sender and receiver share a secret key
	-	Protects against tampering AND spoofing
üìå Exam hook:	MAC requires shared secret ‚Üí symmetric
MAC uses shared keys ‚Üí no non-repudiation

**Digital Signatures
Purpose: Integrity + authentication + non-repudiation
Examples: RSA, ECDSA signatures
	-	Signed with sender‚Äôs private key
	-	Verified with public key
üìå Exam hook: Integrity + non-repudiation ‚Üí digital signature
A digital signature uses include having the same legal significance as a written signature, code signing to verify the integrity and authenticity of software, and nonrepudiation (of origin and delivery).
The process of creating a digital signature is quite easy and fundamentally involves two steps:
	1.	The sender hashes the message, which produces a fixed-length message digest.
	2.	The sender encrypts the hash value with the sender's private key.
‚úî Correct answer: Digital signature

**Checksums / CRC
Purpose: Error detection (not security)
Examples: CRC32
	-	Detects accidental corruption
	-	Easily forged
üìå Exam hook:	Checksums ‚â† cryptographic integrity
CRC detects errors, not attacks.
‚ÄúDetect accidental corruption in a low-risk internal system.‚Äù
‚úÖ Best answer: CRC

**TLS / IPsec Integrity Protection
Purpose: Transport-level message integrity
Examples: TLS MAC, IPsec AH/ESP
	-	Protects data in transit
	-	Often combined with encryption
üìå Exam hook:	TLS/IPsec protect integrity during transmission

üëâ Integrity answers ‚ÄúWas the message changed?‚Äù
**What is the primary goal underlying the use of message integrity controls (MICs)?
	-	Integrity-	‚úÖ Yes	-Detects modification
	-	Authenticity-	‚ö†Ô∏è Partial	-Only with HMAC
Is the message altered?
‚Üí Integrity needed

Need identity or non-repudiation? ‚Üí Digital Signature
Shared secret environment? ‚Üí HMAC
Public verification? ‚Üí Hash
Data in transit? ‚Üí TLS / IPsec
Accidental errors only? ‚Üí CRC

CIA Triad Mapping
Integrity: Hash, MAC, Signature
Authentication: MAC, Signature
Non-repudiation: Signature only

3.6.10 Digital Certificates (pg 484)
**Digital certificates bind an individual to their public key.
**All certificate authorities conform to the X.509 certificate standard.
**The ‚Äúroot of trust‚Äù or ‚Äútrust anchor‚Äù is the foundation of all digital certificates and is represented by a root certificate authority.
**Digital certificate best practices suggest that public/private key pairs be periodically replaced, which means the associated digital certificate is also replaced.
**When a private key has been compromised, a digital certificate should be revoked by the issuing certificate authority.
**With certificate pinning, when a certificate from a web server is trusted, each subsequent visit to the site does not include a request for a new copy of the certificate.

Digital certificate replacement and revocation are summarized here:
	-	Replacement	-	Regular replacement of expired certificates	
	-	Revocation	-	Replacement of certificate when associated private key has been compromised
		Revocation confirmation methods, on the other hand, are: 
			-	Certificate Revocation List (CRL)	-	Client downloads and searches the list of serial numbers of all revoked certificates from the CA
			-	Online Certificate Status Protocol (OCSP)	-	Client queries CA for revocation status of specific certificate serial number

With certificate pinning, when a certificate from a web server is trusted, each subsequent visit to the site does not include a request for a new copy of the certificate.
Finally, a certificate's life cycle includes a number of distinct phases:
Enrollment
Issuance
Validation
Revocation
Renewal

A digital certificate is an electronic credential that binds an identity to a public key, issued and digitally signed by a trusted Certificate Authority (CA).

üëâ Certificates do NOT encrypt data themselves
üëâ They enable trust, authentication, and secure key exchang

What Digital Certificates Provide (and Do NOT)
‚úÖ They PROVIDE
Authentication (proof of identity)
Integrity (certificate hasn‚Äôt been altered)
Nonrepudiation (when used with digital signatures)
Trust (via CA chain)

‚ùå They DO NOT PROVIDE
‚ùå Confidentiality (encryption happens later using keys)
‚ùå Access control
‚ùå Authorization

Certificate Chain of Trust (VERY CISSP-Important)
Root CA (self-signed, trusted)
   ‚Üì
Intermediate CA
   ‚Üì
End-entity certificate (server/user)

Trust flows top ‚Üí down
Browsers trust Root CAs
Intermediates reduce risk to Root
üìå Exam trap: Trusting an intermediate without trusting the root = ‚ùå

üîÑ Certificate Lifecycle (Know This Order)
Key generation
CSR (Certificate Signing Request) / Enrollment 
CA validation
Certificate issuance
Deployment
Renewal / Revocation

üìå Exam trick: Revocation ‚â† expiration

üõë Revocation Methods (Highly Testable)
	-	***1Ô∏è‚É£ CRL (Certificate Revocation List) - Client downloads and searches list of serial numbers of all revoked certificates from the CA
			Periodic list download
			Can be stale
			Offline capable
	-	***2Ô∏è‚É£ OCSP (Online Certificate Status Protocol)	-	Client queries CA for revocation status of specific certificate serial number
			Real-time status
			Faster, more current
			Availability dependent

üìå Exam question clue:
‚ÄúReal-time status‚Äù ‚Üí OCSP
‚ÄúOffline or cached‚Äù ‚Üí CRL

üß™ Common CISSP Exam Traps
‚ùå Trap 1: Certificate = Encryption
Wrong. Certificates only bind identity to public key.
‚úî Correct answer: Certificates support authentication, not encryption.


Root of Trust
Root of trust, or the trust anchor, is the foundation of digital certificates‚Äô integrity. In the case of every digital certificate, ultimately a root CA‚Äôs key is used to sign the certificate, but oftentimes intermediary CAs (Subordinate CAs) act as proxies and sign and issue digital certificates on behalf of the root CA 

From a security perspective, the reliability of the entire system is dependent on the security of the root CA‚Äôs private key. If this key were ever compromised, significant and farreaching global damage could take place, and the entire system could fall apart

3.6.11 Public Key Infrastructure (PKI) (pg 497)
**Public key infrastructure (PKI) is the basis for keys to be distributed and owners of public keys to be verified.
**The standard used to create all digital certificates is X.509.
**PKI consists of several components: certificate authority (CA), registration authority (RA), intermediate/issuing CA, certificate DB, certificate store.
**The root of trust in any PKI is the CA, which ultimately issues certificates.

Major components
Certificate Authority (CA)	-	Root of trust
Registration Authority (RA)	-	Identity proofs on behalf of CA
Intermediate/ Issuing CA	-	Issues certificates on behalf of CA
Validation Authority (VA)	-	Responds to revocation queries on behalf of CA - either use protocols, Revocation Methods (Highly Testable) or OCSP (Online Certificate Status Protocol)
Certificate DB				-	List of certificates issued by CA and revocation list

**Even without a PKI it is still possible to encrypt and send data, but you cannot verify the identities of the other participating parties, it‚Äôs impossible to entirely trust the digital identity of another entity or person.

3.6.12 Key Management 
**Proper key management is paramount to the security of any cryptographic system.
**Kerckhoffs‚Äô principle
**Key management activities: generation/creation, distribution, storage, change/rotation, disposition/destruction, recovery
**Kerckhoffs‚Äô principle - If a key is secure, the underlying cryptographic system is secure. In other words, an attacker can know the ciphertext, the algorithm, the IV (initialization vector), and everything else about the system, and if the key remains secure, the system is secure.

Proper key management is paramount to the security of any cryptographic system and contains numerous key management activities, which are summarized in the following table:
	-	Key Creation/Generation	-	The key generation/creation process must have  attributes: fully automated process, keys are randomly chosen from all key space, asymmetric keys are much longer than symmetric keys	
	-	Key Distribution	-	practice of securely distributing keys. Methods used could include: out-of-band distribution , key wrapping using key encrypting keys (KEK)
	-	Key Storage	-	most critical , two types of systems for storage: Trusted Platform Module (TPM) / Hardware Security Module (HSM)
	-	Key Change /Rotation	-	 refers to how often encryption keys should be replaced.
	-	Key Destruction / Disposition	-	Key disposition refers to how keys are handled, especially data in cloud, 2 method : crypto shredding, key destruction
	-	Key Recovery	-	Key recovery refers to techniques used to recover a key. Three primary techniques exist: split knowledge, dual control, and key escrow.

S/MIME
S/MIME is a standard for public key encryption and provides security services for digital messaging applications. It requires the establishment or utilization of public key infrastructure (PKI) in order to work properly.

The basic security services offered by S/MIME are:

Authentication
Nonrepudiation of origin
Message integrity
Confidentiality
MIME does not address security issues, but security features were developed and added to MIME to create S/MIME.

S/MIME adds features to email messaging, including:
Digital signatures for authentication of the sender
Encryption for message privacy
Hashing for message integrity and nonrepudiation of origin



----------------------------------------------------------------------------------
 https://blog.balancedsec.com/archive?sort=new
https://andreakaiserclouds.com/my-cissp-journey-how-i-prepared-and-passed-the-exam/
https://www.linkedin.com/pulse/how-i-passed-cissp-my-experience-using-lean-study-strategy-saifee-uzaue/
https://www.youtube.com/watch?v=_nyZhYnCNLA


 
 list of domains: 

1. Security and Risk Management
2. Asset Security
3. Security Architecture and Engineering
4. Communication and Network Security
5. Identity and Access Management (IAM)
6. Security Assessment and Testing
7. Security Operations
8. Software Development Security

*************************************************************************************
Clark‚ÄìWilson Model
Goal: ‚úÖ Integrity (Commercial Focus)
Used in: Banking, accounting, business applications
Key Ideas

No direct user access to data
Uses:

Constrained Data Items (CDI)
Transformation Procedures (TP)


Focus on:

Separation of duties
Well‚Äëformed transactions
Auditing



‚úÖ Strong real‚Äëworld integrity
‚úÖ Practical and process‚Äëdriven
üö´ Less about labels and classifications
üìå Mnemonic:

Clark‚ÄëWilson = Clerks & Workflows
----------------------------------------------------------------------------------
Brewer‚ÄìNash (Chinese Wall)
Goal: ‚úÖ Prevent Conflict of Interest
Used in: Legal firms, consulting, finance
Rule

Access depends on prior access history
Once you access Company A‚Äôs data, you‚Äôre blocked from Company B‚Äôs competing data

‚úÖ Dynamic access control
‚úÖ Prevents insider conflict
üö´ Not about CIA triad directly
üìå Mnemonic:

Chinese Wall = Conflict Wall
----------------------------------------------------------------------------------
attice-Based Access Control
Goal: ‚úÖ Formal information flow control
Used in: MLS (Multi‚ÄëLevel Security) systems
Concept

Every subject and object has a label
Lattice defines allowed dominance relationships

‚úÖ Formal basis for Bell‚ÄëLaPadula
‚úÖ Highly structured
üìå Mnemonic:

Lattice = Levels + Labels
----------------------------------------------------------------------------------
Graham‚ÄìDenning Model
Goal: ‚úÖ Secure creation & deletion of subjects/objects
Focus

Access rights management
Who can:

Grant access
Delete users
Transfer rights



‚úÖ Administrative control model
üö´ Not about data flow directly
üìå Mnemonic:

Graham‚ÄëDenning = Admin actions
----------------------------------------------------------------------------------
Harrison‚ÄìRuzzo‚ÄìUllman (HRU)
Goal: ‚úÖ Access rights safety
Key Question

Can a system ever reach an unsafe state where rights leak?

‚úÖ Theoretical model
‚úÖ Proves access control problems can be undecidable
üìå Mnemonic:

HRU = ‚ÄúAre Rights Unsafe?‚Äù
----------------------------------------------------------------------------------
 Ultra‚ÄëFast Memory Tricks (Exam Gold)
üîî Bell‚ÄëLaPadula
‚úÖ Bell rings when secrets leak
üëâ Protects Confidentiality
üîß Biba
‚úÖ Biba breaks bad data
üëâ Protects Integrity
üè¶ Clark‚ÄëWilson
‚úÖ Accountants love workflows
üëâ Integrity + Separation of Duties
üß± Brewer‚ÄëNash
‚úÖ Walls stop conflicts
üëâ Conflict of Interest
ü™ú Lattice
‚úÖ Security ladder
üëâ Levels & dominance

üéØ CISSP Exam Strategy
When you see a question:
1Ô∏è‚É£ Ask: Confidentiality or Integrity?
2Ô∏è‚É£ If confidentiality ‚Üí Bell‚ÄëLaPadula
3Ô∏è‚É£ If integrity (especially business) ‚Üí Biba or Clark‚ÄëWilson
4Ô∏è‚É£ If conflict of interest ‚Üí Brewer‚ÄëNash

HARD CISSP practice questions #1: All CISSP domains - 125Q   ... notes


***************************************





The Graham-Denning model is a framework for identifying and selecting appropriate security controls for an organization. It takes into account the organization's goals, threats, and vulnerabilities to determine the most effective controls. 

**links or attachments to targets.

 A unique key pair is generated for every user and the private key must remain secret correctly describes how asymmetric encryption is practically applied. In asymmetric encryption, each user has a pair of keys - a private key that must be kept secret and a public key that is made public. This key pair is unique for every user. The public key is used to encrypt the data, and only the corresponding private key can decrypt it. It's essential to understand that the private key should never be shared as it could lead to the decryption of the information by unauthorized individuals, compromising the confidentiality of the data. 
 
 The incorrect answers: In an asymmetric encryption setup, each participant has a key pair, not just one user. The private key is kept private, while the public key is distributed to anyone who needs to send an encrypted message to the key owner. 
 
 The fundamental idea behind asymmetric encryption is that the public key is shared openly, allowing anyone to encrypt messages with it. 
 
 The corresponding private key, which is required to decrypt these messages, is kept secret by its owner. Everyone in the organization uses the same key pair for communication is a misrepresentation of asymmetric encryption. In reality, every participant in the communication has their unique key pair. If the entire organization used the same key pair, it would be similar to using symmetric encryption, which increases the risk of compromise and makes secure individual communication impossible.


AES (Advanced Encryption Standard): This is an example of symmetric cryptography where the same key is used for both encryption and decryption of data.  
RSA: This is an asymmetric cryptographic algorithm used for encryption and digital signatures. It uses a pair of keys: a public key and a private key. 
SHA (Secure Hash Algorithm): It's a cryptographic hash function, not an encryption method. It produces a fixed-size output (hash) from a given input. 
PGP: (Pretty Good Privacy): While PGP is known for encrypting emails, it uses a combination of both symmetric and asymmetric cryptographic techniques. However, by itself, PGP isn't a type of cryptography but a protocol that employs various cryptographic methods.

 Spectre is a hardware vulnerability that affects microprocessors that perform branch prediction. On most personal computers, the operating system is responsible for managing hardware and software resources, including the processor. In response to the discovery of the Spectre vulnerability, operating system developers have released updates that contain mitigation techniques to prevent potential attacks. These techniques primarily involve changing the way the processor handles speculative execution, a feature that is exploited by Spectre attacks. By regularly updating your operating system, you ensure that you have the most recent protection mechanisms against known vulnerabilities, including Spectre.

concerned about potential data leakage through covert channels.
Standardizing system response times regardless of the input is the most effective method to mitigate the risks associated with covert timing channels. These channels rely on variations in response times to transfer information illicitly. By ensuring that the system response time remains constant, regardless of whether a username or password is correct or incorrect, you effectively shut down this covert channel. The incorrect answers: Implementing stringent access controls and authentication measures is an important security practice and can help to reduce the risk of unauthorized access, but this measure does not directly address the issue of covert timing channels, which can be exploited even when strong access controls are in place. Regularly monitoring and analyzing network traffic for anomalies is another crucial aspect of network security. This can help detect abnormal activities that might indicate a covert channel, but it may not necessarily prevent the use of timing channels, especially when the time differences exploited are subtle. Encrypting all data in transit within the network is a valuable security measure to protect the confidentiality of data, but this does not prevent the exploitation of timing channels, which are based on variations in system response times rather than the content of the data being transmitted.


Physical security measures and access controls: While these are important to ensure the security of a processing site, they are the most basic level of security measures. They primarily focus on the physical environment, such as securing entry points and monitoring access to the site. They do not address digital threats or vulnerabilities that can arise within the IT infrastructure. The incorrect answers: Network security measures and firewalls: This level of security is more advanced than just physical security measures. Network security measures include firewalls, intrusion detection systems, and other tools to protect the organization's network from unauthorized access and potential cyber threats. Data encryption and backup procedures are essential for protecting sensitive information and ensuring business continuity in case of data loss or system failure. These measures add a layer of security that goes beyond physical and network security. Regular security assessments and audits are critical for identifying vulnerabilities and ensuring that security measures are effective in safeguarding an organization's assets. This level of security is more comprehensive than the other options, as it involves ongoing evaluation and improvement of security measures.

 CPU utilization and network utilization: The most likely pair of metrics that will indicate a distributed denial-of-service (DDoS) attack is in progress is CPU utilization and network utilization. DDoS attacks typically involve overwhelming a target's network with a flood of internet traffic, which can lead to high network utilization. This in turn can cause increased CPU utilization as the system tries to process the incoming requests. The incorrect answers: Memory utilization and network utilization: While high network utilization is a strong indicator of a DDoS attack, memory utilization may not necessarily be affected in the same way as CPU or disk resources, as DDoS attacks primarily overwhelm network capacity and processing power rather than memory. CPU utilization and disk activity: DDoS attacks are focused on saturating network resources and causing service disruption, rather than causing high disk activity. Therefore, monitoring disk activity is less likely to indicate a DDoS attack compared to network utilization. Disk activity and network utilization: Disk activity is not typically the primary indicator of a DDoS attack, as these attacks target network resources. While high network utilization is relevant, disk activity does not correlate as directly with DDoS attacks as CPU utilization does.

Periodically, Mark sends a random number he has chosen encrypted using the work server's public key and waits for the server to reply with the same number encrypted with his public key: This method is a good way to prevent a Man-In-The-Middle (MitM) attack because it utilizes asymmetric encryption to ensure that the communications are secure between Mark and the work server. By using the server's public key to encrypt the number, only the server, with its private key, can decrypt it. When the server replies using Mark's public key, only Mark can decrypt the message with his private key. This process helps to confirm that the parties in the transaction are legitimate and that there has not been interference by a third party. The incorrect answers: Mark and the work server should create and send a digital signature of every packet: While using digital signatures for every packet can provide strong security by ensuring the integrity and authenticity of the communications, this method may not be the most efficient, as it can produce significant overhead and delay due to the computational cost of signing every packet. Mark and the work server should create and send a digital signature of every nth packet: This approach can leave packets without digital signatures vulnerable to interception and manipulation, as every packet is not verified. It creates a gap in the security model, which could be exploited by an attacker. Mark sends the current time (including milliseconds) to the work server and waits for the work server to reply with the time it received Mark's message, together with the server's own time when it sent the reply: Sending timestamps can help detect replay attacks or delays in the network that might indicate tampering, but it does not encrypt or secure the data, so it would not be effective as a standalone measure against a MitM attack.

PKI (Public Key Infrastructure). PKI uses a combination of public and private cryptographic keys to ensure message integrity, sender authentication, and non-repudiation. Message integrity ensures the message hasn't been altered, sender authentication verifies the sender's identity, and non-repudiation ensures the sender cannot deny having sent the message. The incorrect answers: Symmetric cryptography: While it provides message encryption, it doesn't inherently provide sender authentication or non-repudiation. Hashing: Hashing can ensure message integrity by detecting changes in the content of the message, but it does not provide sender authentication or non-repudiation. Linear cryptanalysis: This is a method used to break cryptographic algorithms and doesn't provide the benefits mentioned in the question.

Second line of defense: Dee, in her role as a compliance manager, forms the second line of defense in her organization's risk management and control framework. The second line of defense typically includes functions that oversee risk management and compliance with external regulations and internal policies. As a compliance manager, Dee helps to ensure that controls are in place to meet compliance standards and works to identify and mitigate risks related to non-compliance. The incorrect answers: First line of defense: The first line of defense is typically composed of operational managers and staff who are directly responsible for maintaining control over the day-to-day business activities and processes. They are the ones who implement risk controls and procedures as a part of their regular job functions. Third line of defense: The third line of defense usually consists of internal audit functions. Their role is to provide independent assurance to the organization's board and senior management regarding the effectiveness of the company's governance, risk management, and internal control processes. Fourth line of defense: The concept of a fourth line of defense is not commonly recognized in standard risk management frameworks. Some organizations may informally refer to external auditors, regulators, or other external parties that provide another layer of oversight as a "fourth line of defense," but this is not a standard classification within the three lines of defense model.

Water: Removes the ‚Äúheat‚Äù leg of the fire triangle by lowering the temperature. IFire Triage = Heat, Fuel, Oxygen
s the safest suppression agent, but for Data Centers: Water + hardware = dead hardware. Should always be a last resort and electricity could always be cut before water is used.



**********
Biba; Tim can read and Harvey cannot; but Harvey can write comments and Tim cannot: The Biba model focuses on ensuring the integrity of the information, which is achieved through two main principles: the "no read down" rule and the "no write up" rule. Given that the documents are at a mid-level clearance (Confidential), under the Biba model, Tim, who has a low-level clearance (Enhanced Reliability), would be able to read the documents (no read down) but not write to them (no write up) to prevent contamination of the higher integrity data. Harvey, with a high-level clearance (Secret), would not be able to read the documents (to maintain his higher integrity level and prevent potential downgrading of information), but he would be allowed to write comments (no write down), as his higher integrity level ensures that he would not compromise the data's integrity. The incorrect answers: Biba; both Tim and Harvey can read; but only Harvey can write comments: This choice is incorrect because, according to the Biba model's "no read down" rule, Harvey should not be able to read data at lower integrity levels than his own to prevent him from accessing potentially less reliable data. Bell-LaPadula; Harvey can read, and Tim cannot; but Tim can write comments and Harvey cannot: The Bell-LaPadula model is primarily focused on maintaining confidentiality, not integrity. Its "no read up" and "no write down" rules serve to prevent unauthorized access to information based on clearance levels and would not allow a lower clearance individual to write to a higher clearance document. Brewer-Nash; both Harvey and Tim can read Jada's papers and write their comments; but neither can read or write to their marketing documents at the same time: The Brewer-Nash model, or Chinese Wall model, is designed to prevent conflicts of interest by separating access to data based on conflict classes. It does not handle data integrity in the same way as the Biba model and does not apply clearance levels to read and write operations as described in the scenario.

The correct answer: "It is a set of rules and guidelines used to ensure the security and integrity of data.": This is the correct answer. The Clark-Wilson model is indeed a set of rules and guidelines used to maintain the integrity of data in a system. It introduces the concepts of well-formed transaction and separation of duties to prevent unauthorized or malicious actions. This model focuses on certifying that applications and processes are in place to change and manage access to data, making it particularly useful for business environments where data integrity is crucial. The incorrect answers: "It is a mathematical model used to describe the behavior of physical systems.": This statement is not correct with respect to the Clark-Wilson model. While mathematical models are used in various fields to describe the behavior of systems, the Clark-Wilson model is a security model aimed at data integrity, not a mathematical model used to describe physical systems. "It is a database management system designed to store and organize large amounts of data.": This statement is also not correct. The Clark-Wilson model is not a database management system. While it can be applied to systems that involve databases to ensure data integrity, it is not a tool or system for managing databases. "It is a security model that focuses on the separation of duties and the use of well-defined interfaces.": While the concept of separation of duties is a part of the Clark-Wilson model, this statement is not entirely accurate. The Clark-Wilson model is more comprehensive, providing a framework for implementing a wide range of integrity controls in addition to separation of duties. It also includes the concept of well-formed transactions, which involves checking data integrity before and after a transaction, rather than focusing on well-defined interfaces.

Provide a constrained interface, so that commands are shown but dimmed if the user does not have sufficient privileges: Implementing the Clark-Wilson security model involves using the principle of well-formed transactions and separation of duties. A constrained interface is a way to enforce these principles. It allows the application to show users all the commands they may potentially execute while making it clear which commands they are not authorized to perform (dimmed options). This helps prevent unauthorized access to functions and ensures data integrity by restricting users to only the transactions they are permitted to execute. The incorrect answers: Provide a drop-down menu showing all possible subcommands: Simply providing a drop-down menu with all possible subcommands does not enforce any of the Clark-Wilson integrity constraints. It does not differentiate between transactions that a user can and cannot perform based on their privileges. Ensure users cannot read down to a classification below their security clearance level: This principle is related to the Bell-LaPadula model, which is focused on maintaining confidentiality in a system with different levels of classification. It primarily deals with preventing "read down" operations, where a higher-classified user might read information at a lower classification. The Clark-Wilson model is focused on data integrity, not confidentiality. Ensure users cannot write down to a classification below their clearance level: This is also a principle from the Bell-LaPadula model, known as the "no write down" or "ss-property" rule. It prevents users from writing information to a lower classification level, potentially causing a data spill. The Clark-Wilson model is concerned with integrity and does not deal with classification levels.


***********
Phishing attack: Ken has most likely been the victim of a phishing attack. Phishing attacks typically involve the attacker sending emails that appear to be from a legitimate source with the intent to deceive the recipient into providing sensitive information. In this case, Ken believed the email was from Dee and clicked on a fraudulent link that led to a fake website designed to capture his username and password. The incorrect answers: Pharming attack: Pharming redirects a website's traffic to a fraudulent website without the user's knowledge, which can occur through malware on the user's system or by exploiting vulnerabilities in DNS servers. Since Ken was actively directed to the fake site by clicking on a link in an email, this is more indicative of phishing rather than pharming. Poisoning attack: Poisoning attacks often refer to techniques that corrupt or falsify data, such as cache poisoning in DNS or ARP (Address Resolution Protocol) poisoning on networks. Ken's situation does not involve data corruption but rather deception through a fake website. DNS Pharming attack: DNS pharming is a type of attack that involves compromising DNS servers to redirect users to malicious sites even when they type in the correct website address. Ken‚Äôs case involves an email link, which suggests a direct phishing attempt rather than DNS pharming, where the attack occurs at the DNS level.

Digital signatures: Digital signatures are the best method for ensuring non-repudiation because they provide proof of the origin and integrity of the data. A digital signature confirms that a document or message was signed by the purported sender, making it difficult for that sender to deny having sent the message. This is achieved by using a combination of a private key to sign the message and the corresponding public key which can be used by others to verify the signature. The incorrect answers: Strong complex passwords: While strong complex passwords are important for securing access to systems and data, they do not provide non-repudiation. Passwords authenticate a user's identity but do not log the actions that the user takes in a way that cannot be repudiated later. Collision resistant hashes: Collision-resistant hashes ensure that a given input will always result in the same unique hash output, making it difficult to find two different inputs that produce the same hash value. However, hashes alone do not provide non-repudiation because they do not inherently tie the hash to a specific user's identity. Symmetric encryption: Symmetric encryption uses the same key for both encryption and decryption. While it secures the data during transmission, it does not prove who sent the message since anyone with access to the key could have encrypted it. Therefore, it cannot be used to ensure non-repudiation.

Spamming. Spamming is typically not part of a reconnaissance attack. Reconnaissance, in the context of cybersecurity, refers to the preparatory phase where an attacker seeks to gather information about a target prior to launching an attack.Spamming would increase awareness of an attack either through logs, reduced performance, or other factors. This would lead to detection. The incorrect answers: Service detection: This can be part of a reconnaissance attack as knowing what services are running on a server can give attackers valuable information about potential vulnerabilities. Scanning: This is a common part of reconnaissance attacks. Scanning can involve checking for open ports, running services, and other elements that might expose a system to attack. Sniffing: Network sniffing can also be part of a reconnaissance attack to monitor and capture data packets in order to gain information about the system that could be useful in an attack.

: Provide non-repudiation, protect confidentiality, and protect integrity. Cryptography provides multiple benefits, including: Non-repudiation: Ensures that a party cannot deny the authenticity of their signature on a document or the sending of a message. Confidentiality: Ensures that only authorized parties can access the data. Integrity: Ensures that data has not been altered in transit. The incorrect answers: Provide non-repudiation and protect confidentiality: This misses the benefit of ensuring data integrity. Protect both confidentiality and integrity: This misses the benefit of non-repudiation. Protect confidentiality, integrity, and availability: While cryptography can help protect confidentiality and integrity, it doesn't directly ensure availability, which usually depends on system resilience and redundancy.


MITRE ATT&CK framework. The MITRE ATT&CK framework is a comprehensive knowledge base of adversary tactics and techniques based on real-world observations.
in watering hole attacks, such as "Drive-by Compromise" (T1189) and "Exploit Public-Facing Application" (T1190), are covered in the framework.
 The MITRE ATT&CK framework includes multiple techniques related to malware, such as "Command and Scripting Interpreter" (T1059),
 Social Engineering (T1586): Social engineering is a technique listed under the "Initial Access" tactic in the MITRE ATT&CK framework. 
 "Obfuscated Files or Information" (T1027), and "Software Deployment Tools" (T1072). Attackers employ malware to achieve different objectives, such as gaining unauthorized access, stealing data, or disrupting operations.

FIRST principle that should be considered when assessing and implementing secure design principles in network architectures?
: Least privilege: This principle dictates that every user or process should have the minimum privileges necessary to perform its task and nothing more. Applying the principle of least privilege is the first and foundational step in designing secure network architectures. It helps to reduce the potential damage from accidents or malicious actions, as users or processes can't affect systems or data beyond their scope of necessity. The incorrect answers: Confidentiality: While it is crucial to ensure that unauthorized individuals cannot access sensitive data, confidentiality is not the first principle that should be considered when designing secure network architectures. Only after setting up access controls based on the least privilege principle, can you effectively implement measures to maintain confidentiality. Integrity: Integrity, ensuring the accuracy and consistency of data, is a key principle in secure network design. However, before ensuring integrity, it is more important to establish who should have access to the data (based on least privilege), which directly impacts both integrity and confidentiality. Availability: While maintaining system and data availability is important, it is not the first principle to consider. Similar to confidentiality and integrity, effective availability controls can only be established after implementing the least privilege principle, as understanding who has access to systems and data under what conditions is fundamental to maintaining their availability.


 Message Authentication Code (MAC): In the context of hashing and cybersecurity, a Message Authentication Code (MAC) refers to a short piece of information used to authenticate a message and to ensure the integrity and authenticity of the message. A MAC is computed from a message and a secret key, and any change to the message will result in a different MAC. It provides assurance that the message is from the claimed sender (authentication) and that it hasn't been tampered with during transmission (integrity). The incorrect answers: Media Access Control: While MAC can also stand for Media Access Control, this refers to a unique identifier assigned to a network interface controller (NIC) for use as a network address in communications within a network segment. This context is related to networking and not directly to hashing or cryptography. Most Accessible Control and Multi-Application Control: These options are not standard terms in the context of hashing, cryptography, or networking.

D4
**********************************************************
Physical Layer: This is the lowest (first) layer in the OSI model. The Physical layer deals with the physical aspects of the data transmission, including the hardware devices, cables, connectors, signals, binary transmission, and other aspects related to the physical transmission of data. It does not interpret any data and works simply to send and receive a raw bit stream over the physical medium. 
Example: Cables, hubs, fiber, radio signals, NICs
Threats: Wiretapping, jamming, cutting cables, EMI

Data Link Layer: The Data Link layer is the second layer in the OSI model. It provides reliable transit of data across a physical network link and handles error correction from the physical layer, packet framing, network topology, and Media Access Control (MAC). It ensures that the data sent by the Network layer of one device can be read by the Network layer of another device. 
Example: Ethernet, ARP, VLANs, switches, Wi‚ÄëFi MAC layer, NAC
Threats: MAC spoofing, ARP poisoning, VLAN hopping

Network Layer : The Network layer is the third layer in the OSI model. Its primary function includes addressing, routing, and (on many networks) the control of congestion of data. This layer is responsible for deciding which physical pathway data should take based on network conditions, priority of service, and other factors.
Example: IP, ICMP, routers, OSPF, BGP
Threats: IP spoofing, route hijacking, ICMP tunneling 

Transport Layer: The Transport layer is the fourth layer of the OSI model. It provides transparent transfer of data between end systems and is responsible for end-to-end error recovery and flow control. This layer ensures complete data transfer. Protocols like TCP and UDP operate at this layer.
Example: TCP, UDP, ports (80, 443, 3389)
Threats: SYN floods, port scanning, session reset attacks

Session Layer ‚Äî ‚ÄúManages sessions, connections, and dialogs.‚Äù
Example:  Session tokens, NetBIOS, RPC, SQL session management. Anything involving session IDs, authentication tokens, or dialog control is Session layer.
Threats: Session hijacking, replay attacks, fixation attacks

Presentation Layer ‚Äî ‚ÄúFormats, encrypts, and translates data.‚Äù
Example: SSL/TLS encryption, data compression, character encoding
Threats: SSL stripping, weak encryption, downgrade attacks

Application Layer ‚Äî ‚ÄúWhere users and software interact with the network.‚Äù
Example: HTTPS, DNS, SMTP, FTP, SNMP
Threats: Phishing, DNS poisoning, email malware, API abuse





Packet switching - Cheap, but no capacity guarantee, very widely used today. Data is sent in packets, but take multiple different paths to the destination. The packets are reassembled at the destination.

staeful vs stateless firewall ...

you are considering adding an additional security layer to your network defense strategy. What additional security measure can best protect your organization's network from state exhaustion attacks that may cause your stateful firewalls to crash?
: Implementing a DDoS mitigation solution would be the most effective approach to protecting against state exhaustion attacks. These solutions can identify and block DDoS attacks, including those designed to overwhelm firewall state tables, by filtering out malicious traffic before it reaches the firewall, thereby preserving the firewall's resources. In addition, these solutions can provide a response to a broad range of DDoS attack types, making them a more comprehensive solution for this specific threat. TThe incorrect answers: Proxy servers can add a layer of security by mediating traffic between internal and external networks, effectively hiding internal IP addresses, and can provide content filtering capabilities. But, they may not be as effective in preventing state exhaustion attacks on firewalls, which are designed to overwhelm the firewall's connection state memory. Intrusion prevention systems can detect and block known malicious activities on the network, but their effectiveness against state exhaustion attacks, which exploit legitimate protocol behavior, can be limited. Load balancing can distribute network traffic evenly across multiple firewalls to maximize throughput and optimize resource use. This approach may help in managing high volumes of traffic, but it may not directly address the threat of state exhaustion attacks aimed at overwhelming the connection state memory of firewalls.

In order to properly respond to a network attack, it is essential to first identify the type of attack that is occurring. This will allow the appropriate countermeasures to be implemented and ensure that the attack is effectively mitigated. The incorrect answers: While shutting down the affected network may be a necessary step in certain situations, it should not be the FIRST step taken. Without identifying the type of attack, it may be impossible to determine whether shutting down the network is the appropriate response. Notifying relevant parties is an important step in responding to a network attack, but it should not be done before identifying the type of attack. Without this information, it may be difficult for the relevant parties to effectively respond to the attack. Implementing countermeasures should not be done before identifying the type of attack. Without this information, it may be impossible to determine the appropriate countermeasures to implement, and the attack may continue to be successful.


Last nine bits, 386: The subnet mask /23 indicates that the first 23 bits are the network portion and the remaining 9 bits are for the host addresses. To find the specific server address within the subnet, Mark would look at the last 9 bits of the address. The value "386" is derived from converting these last 9 bits from binary to decimal. However, it's important to note that the complete IP address must still be presented in standard IPv4 notation, which is in the form of four octets. Here, the "386" represents the combined decimal value of the third octet's last bit and the entire fourth octet. The correct IPv4 address would actually be in the format of 47.152.x.y, where x and y are derived from the binary representation of the last 9 bits. The decimal value of "386" relates to the binary representation of the host part of the address (binary '000000001 10000010'), which corresponds to '1.130' when broken down into two octets. Therefore, the full server address is 47.152.1.130

The hold down timer is a feature that could have significant implications for network performance. While it's intended to allow routes to stabilize before the routing table is updated, if set incorrectly or inappropriately for the network's conditions, it could lead to inefficiencies. For example, it could cause delays in route updates if it's set too long, or frequent, unnecessary updates if set too short. As a result, it's a critical feature to evaluate when looking to improve the performance and efficiency of a network using RIP. The incorrect answers: It is true that RIP uses UDP Port 520, but this is more of a technical detail and doesn't have as significant an impact on overall network performance as other factors.

The correct sequence for the TCP Three-way Handshake is: -Client sends TCP packet with SYN raised: The client initiates the handshake by sending a synchronize (SYN) packet to the server, indicating a desire to establish a connection. -Server responds with packet with ACK and SYN raised: The server acknowledges (ACK) the client's SYN packet and sends its own SYN packet, indicating it's ready to establish the connection. -Client responds with packet with ACK raised: The client acknowledges the server's SYN packet, completing the three-way handshake and establishing the connection

 Baseband transmission sends digital signals over a single channel using the entire bandwidth for one signal at a time. This allows simple, reliable data transmission but limits how far the signal can travel before it weakens.


The incorrect answers:
High data rates: Baseband can be fast, but speed is not its defining feature. The key limitation is distance.
Shared communication medium: That describes broadband, which divides bandwidth into multiple channels for simultaneous use.
Strong security protocols: Security depends on higher-level network controls, not on the transmission method itself.


The firewall will allow malicious attacks: If the firewall is designed to fail open and it exceeds its threshold of handling more than 10,000 concurrent sessions, it will stop blocking traffic, which includes potentially malicious attacks. With 50 application servers each allowing 201 sessions to be opened, the total number of concurrent sessions would be 10,050 (50 servers * 201 sessions), just over the threshold. This means the firewall's default behavior when overwhelmed will be to let all traffic through, including what would normally be blocked under normal operating conditions. The incorrect answers: The firewall will allow only the previously allowed traffic: A firewall that fails open when overwhelmed will not discriminate between previously allowed traffic and new traffic. It will allow all traffic to pass, not just the traffic that was previously allowed. The firewall will block malicious attacks: In this scenario, since the firewall fails open, it will not continue to block malicious attacks once it is overwhelmed with more than 10,000 concurrent sessions. The firewall will block any further use of the previously allowed traffic: A firewall that fails open will not block further use of any traffic because it allows all traffic through when it fails, regardless of whether it was previously allowed or not.


Send log entries to a separate server on UDP port 514: Sending log entries to a separate server on UDP port 514 is the most appropriate method for increasing controls over accountability, as this is the standard port for Syslog, a protocol used widely for sending event notification messages across IP networks to event message collectors (Syslog servers). By directing logs to a separate server, Linda can ensure that the logs are not tampered with on the source device and that there is a central place for log review and analysis, enhancing accountability. The incorrect answers: Send log entries to a separate server on UDP port 162: UDP port 162 is the standard port for SNMP Trap, which is used for sending alert messages from network devices to a management station. It is not primarily used for log transfer and hence is not the most appropriate choice for improving log accountability. Keep log entries on a different partition on the hard drive: Keeping logs on a different partition on the same physical hard drive does not increase controls over accountability significantly, because if the system is compromised, an attacker could gain access to the logs and alter or delete them, compromising the integrity of the logs. Send log entries to a separate server on TCP port 1433: TCP port 1433 is the default port for Microsoft SQL Server. Sending log entries to a server on this port would imply that they are being sent to a database server, which is not the standard practice for log transfer. Moreover, using a database server does not necessarily increase controls over accountability compared to dedicated log management systems.

Given the severe implications of a SYN flood attack on a TCP-based web server architecture. In such an attack, the attacker initiates a large number of TCP connections but never completes the three-way handshake. This leaves the server holding a vast number of partially open connections, consuming resources, potentially leading to a Denial of Service (DoS) as the server might be unable to handle legitimate requests. The impact on a web server - a crucial part of the infrastructure that often faces the public internet - could be enormous, leading to significant downtime and potential loss of business. The incorrect answers: Fraggle attacks against UDP are indeed a concern, but they may not be as critical in this context. These attacks use UDP and not ICMP, making them potentially more successful, but the damage might be limited given the context. This scenario mentions a gaming server, and while downtime is a problem, the implications may not be as far-reaching as an attack on the web server infrastructure. Smurf attacks utilize ICMP, not UDP, rendering this option less pertinent. These types of attacks might pose a risk to your infrastructure, but as UDP is used for live video streaming in this context, they don't apply directly. Also, ICMP blocking is common, which may reduce the potential impact of this type of attack. Sequence number prediction attacks on TCP connections are a concern, but in this context, the data being transmitted is from IoT sensors. It's important to protect IoT data, but this threat may not be as critical in terms of potential damage to the infrastructure as a SYN flood attack on the web server architecture. Modern TCP/IP stack implementations also use random sequence numbers, making these types of attacks more difficult.

Kerberos is not a decentralized protocol, it's actually a centralized authentication protocol. The central component of the Kerberos protocol is the Key Distribution Center (KDC), which is trusted by all entities in the network (both clients and services). The KDC is responsible for authenticating users and issuing tickets that clients can use to authenticate to services. This centralization allows for efficient management and tighter control over the authentication process, but it also means that the security of the entire system depends on the security of the KDC. The incorrect answers: Using a trusted third party to authenticate users is a key characteristic of Kerberos. In a Kerberos-enabled network, the Key Distribution Center (KDC) serves as the trusted third party. The KDC verifies the identity of users and services, issues tickets that users can present to services for authentication, and mediates secure key exchange between users and services. Using symmetric key cryptography is also a correct characteristic of Kerberos. Symmetric key cryptography is a type of encryption where the same key is used for both encryption and decryption. In Kerberos, after the initial authentication process, the KDC issues a ticket to the client. This ticket, which is encrypted with a symmetric key that only the KDC and the desired service know, can then be presented to the service for authentication. Single sign-on (SSO) is a user authentication process that allows a user to provide credentials once and gain access to multiple related systems or services. With Kerberos, after a user has been authenticated by the KDC, they can use the tickets issued by the KDC to authenticate to multiple services without having to re-enter their credentials. This not only improves usability but also reduces the potential for exposure of credentials.

The correct answer: Tunnel management is the process of establishing, maintaining, and terminating L2TP tunnels, which involves negotiation between the L2TP client and server, as well as handling any errors or issues that may arise during the tunnel's lifetime. This makes it the most complex component of L2TP. The incorrect answers: Encapsulation is the process of wrapping data in a protocol-specific format to be transmitted over a network, which is a relatively straightforward process in L2TP. Authentication is the process of verifying the identity of a user or device, which can be done using various protocols such as PPP or IPSec in L2TP. The handshake is the initial exchange of information between the L2TP client and server, which is used to establish a connection and agree on the parameters of the tunnel. This is a relatively simple process compared to tunnel management.


To identify potential attacks on our internal network: The main purpose of an Intrusion Detection System (IDS) is to monitor network and system traffic for suspicious activity and known threats, effectively identifying potential attacks on the internal network. The IDS analyzes traffic to detect various types of malicious activities, including security policy violations, threats, and any signs of compromised systems. The incorrect answers: To block traffic seen as malicious: While blocking malicious traffic is related to intrusion prevention, it is the primary function of an Intrusion Prevention System (IPS), not an IDS. An IPS is designed to not only detect but also prevent or block identified threats in real-time. To identify network misconfigurations: Identifying network misconfigurations is not the main purpose of an IDS. While an IDS might detect activity that could stem from a misconfigured network, its primary role is to identify potential security breaches or attacks, not to audit the network for configuration issues. To alert on true negatives: Alerting on true negatives would mean alerting on events that are not attacks, which is the opposite of what an IDS is intended to do. IDS aims to alert on true positives (actual attacks or suspicious activities) and minimize false positives (benign activities mistaken as attacks) and false negatives (missed actual attacks).

Eight: In a /29 subnet, there are 8 total IP addresses, ranging from 201.225.231.120 to 201.225.231.127 when considering the subnet's binary representation. However, in typical subnetting practice, the first address (201.225.231.120) is reserved as the network address, and the last address (201.225.231.127) is reserved as the broadcast address, leaving 6 usable IP addresses for hosts. Nonetheless, since the question specifies that the maximum number of servers that can use the NAT at the same time is being asked and does not account for network and broadcast addresses, the answer would be 8, implying that Dynamic NAT has been set up to use all addresses in the allocated address pool without the typical reservations. However, it is important to clarify that in a production environment, you would typically reserve the first and last address of the subnet and have only 6 usable addresses for devices. The incorrect answers: Five: The calculation for a /29 subnet results in 8 total IP addresses. Subtracting the network and broadcast addresses, which are traditionally reserved and not used for host assignments, would typically leave 6 usable IP addresses. Since the question implies that all 8 addresses are used for Dynamic NAT, stating 5 would be an undercount, making it an incorrect answer. Six: Under normal circumstances, a /29 subnet, which contains 8 IP addresses in total, would have 6 usable addresses after reserving one for the network address and one for the broadcast address. But the question, as framed, suggested using all addresses in the /29 subnet range, including typically reserved ones, for Dynamic NAT. Sixteen: A /29 subnet mask does not allow for sixteen usable IP addresses. It only provides 8 total addresses. With the standard practice of reserving the first address as the network address and the last as the broadcast address, this would generally leave 6 usable addresses. The number sixteen far exceeds the capacity of a /29 subnet and is not a correct answer.

This address cannot be reached by the firewall: The IP address 127.0.0.1 is known as the loopback address, which is used by a computer to direct traffic to itself. It is not a valid destination for external traffic, as it is used for internal communication within the host. Thus, a firewall rule permitting traffic to these ports on the loopback address does not make sense, because the firewall typically manages traffic between distinct hosts on a network, not traffic sent to a host from itself. The incorrect answers: This address is missing the subnet mask: While it's true that IP addresses can be associated with a subnet mask, the concern with a rule involving 127.0.0.1 is not about subnetting. The loopback address is universally recognized and does not require a subnet mask in the context of a firewall rule. The web server would never be assigned this address: This statement is partially true; a web server intended for external access would not be assigned 127.0.0.1 as its IP address. However, the key reason for suspicion is that the loopback address is not reachable through the network and is intended for internal host communications only. This address is used to send a broadcast to all servers on the subnet: The address 127.0.0.1 is not used for broadcasting. Broadcasts to all hosts on a local network are typically done using the broadcast address specific to the subnet, not the loopback address. The loopback address is used solely for traffic internal to the host.


In her company, access controls are rule-based. Naomi is creating an access control list for files in the financial department's share. Naomi wants all managers to have read access to any file, but only employees in Accounts Receivable (AR) to be able to update the file and only employees in Accounts Payable (AP) to read the files; no-one else should be permitted access. Given these four rules, what is MOST likely the sequence to place them in the ACL? 1. Deny 2. Allow Managers read access 3. Allow AR update access 4. Allow AP read access
======================================================================
3, 4, 2, 1: In rule-based access control systems, the order in which rules are listed can affect how they are processed. Given the requirements stated by Naomi, the sequence should be: -Allow Accounts Receivable (AR) update access (most specific rule allowing the most permissions). -Allow Accounts Payable (AP) read access (specific to a department with read-only permissions). -Allow Managers read access (broader rule allowing read access to a larger group). -Deny all other access (the general rule that ensures anyone not matched by the previous rules is denied access). This sequence ensures that the most specific permissions are granted first before the more general permissions, and lastly, any access not explicitly allowed earlier is denied. The incorrect answers: 2, 3, 4, 1: This sequence incorrectly places the Managers read access rule before the more specific departmental rules for AR and AP, which could lead to unintended access permissions due to the processing order of ACLs. 4, 3, 2, 1: This sequence starts with AP read access, but following the principle of placing the most specific rules first, the AR update access should come before AP read access. 1, 2, 4, 3: Placing the deny rule first would prevent any of the subsequent allow rules from being processed, effectively blocking all access, which contradicts the intent.

A cyclic redundancy check (CRC) is a method used primarily in networks and storage devices to detect errors in data transmission. It creates a short, fixed-size block of data, known as a "checksum," that is appended to the message being transmitted. The sender computes the checksum based on the content of the message, and the receiver does the same upon receipt of the message. If the checksums match, the transmission is considered error-free. CRCs are preferred because they have high error-detection capabilities and are able to detect common errors like bit flips and burst errors, which are series of bits that have changed value due to noise or interference. The incorrect answers: While hash functions can be used to check the integrity of data, they are not typically used for detecting transmission errors. Hash functions are mainly used for storing and retrieving data in hash tables, digital signatures, message digest, and data integrity. They are generally more complex and compute-intensive compared to CRCs, and hence not as efficient for error detection in data transmission. Encryption is a method of protecting data from unauthorized access by transforming the original information into an unreadable format. Encryption can help ensure data confidentiality and integrity but it does not directly detect transmission errors. It is possible to have errors within encrypted data if the error occurred prior to encryption or during transmission. Parity checks are a type of error detection mechanism where an extra bit, known as a parity bit, is appended to the data to make the total number of 1's either even (even parity) or odd (odd parity). While parity checks can detect single-bit errors, they are not capable of detecting two-bit errors or errors that affect an even number of bits, making them less reliable than CRC for error detection in data transmission.

The transport layer is responsible for providing services to the application layer, such as data formatting and error checking. This layer ensures that the whole message arrives intact and in order, overseeing both error correction and flow control. This is the layer where you would ensure data integrity for applications. The incorrect answers: The physical layer (Layer 1) is responsible for transmitting and receiving raw bitstream data over a physical medium like a cable. It does not handle any data formatting or error checking, its concern is with physical characteristics of the data transmission. The network layer (Layer 3) is mainly responsible for routing and transferring data between networks. It manages network addressing, routing, and traffic control, but it does not perform error checking and data formatting for applications. The data link layer (Layer 2) is responsible for providing reliable transit of data across a physical network link. It handles error detection and correction to ensure a reliable link, but does not provide services directly to the application layer.

 The DNP3 (Distributed Network Protocol) is a set of communication protocols used between components in process automation systems. Its main use is in utilities such as electric and water companies. It's frequently used in SCADA systems and in protecting critical infrastructure, and also has application in access control systems where it helps maintain and manage entry into secure areas. Video surveillance, on the other hand, typically uses other protocols and technologies for transmission of video data over IP networks. The protocol that is often used is the Real Time Streaming Protocol (RTSP) rather than DNP3. Video surveillance is not a common use case for DNP3 in cyber security. The incorrect answers: SCADA systems: This is incorrect because DNP3 is commonly used in SCADA (Supervisory Control and Data Acquisition) systems. SCADA systems require a communication protocol to transmit data and DNP3 is one such protocol. SCADA systems are vital to many industries, including water treatment, oil and gas, and electricity, and DNP3 helps in securely transmitting data in these systems. Critical infrastructure protection: This is incorrect because DNP3 is often used in the protection of critical infrastructure. Critical infrastructure, such as power plants, water treatment facilities, and transport systems, often use SCADA systems for control and management, and DNP3 is a common protocol in these systems. It helps in securely transmitting data, thus aiding in the protection of these vital facilities. Access control systems: This is incorrect because DNP3 can be used in access control systems. In these systems, DNP3 can help manage and control access to secure areas. For instance, in a power plant, an access control system may use DNP3 to manage who can access certain areas, providing a layer of security to the facility.


Broadcasts/unicast/
The correct answer: A broadcast domain is a logical division of a computer network, in which all nodes can reach each other by broadcast at the data link layer. In simpler terms, a broadcast domain is a network segment where data is sent to every device in the network. This means that all devices within the broadcast domain will receive the same data, regardless of whether they were intended recipients or not. The incorrect answers: A network where only authorized users can access data refers to concepts of access control and authentication, rather than a broadcast domain. A network where data is only sent to specified devices would refer more accurately to a unicast communication, where data is sent from one sender to a single designated receiver. A network where data is sent to all devices in the network except for specified devices isn't a standard networking concept. It is possible to create rules to exclude devices from receiving the data but it does not describe a broadcast domain.

 The Security Content Automation Protocol (SCAP) is a suite of specifications that standardize the way security software communicates information about software flaws and security configurations. The most important benefit of implementing SCAP is an improved security posture. By using SCAP, organizations can automate processes for managing vulnerabilities and measuring security, which can significantly improve their overall security posture. This includes identifying system weaknesses, mitigating risks, ensuring that security settings are correctly configured, and responding more effectively to any security threats. 

Gateway Protocols (IGP) and Border Gateway Protocol (BGP) for internal and internet-wide routing, respectively.



D5
**********************************************************
Capability Table: If Thor is assigning read access to a group of files and can organize the users into five departments, using a capability table would be the most likely mechanism. A capability table is a system used to define access rights based on capabilities, which are tokens or keys that grant permissions to users or groups to access specific system objects such as files. Since Thor is dealing with several hundred employees, it would be more efficient to manage access by assigning capabilities to departments, rather than managing individual file permissions for each user. The incorrect answers: Access Control List: While ACLs are a common method for controlling access to files based on user identities and group memberships, they are not as suited to scenarios with a large number of users where permissions are uniform across common objects. ACLs could become complex and cumbersome to manage with several hundred individual entries. Content-dependent access controls: This form of access control relies on the content within the objects to determine access, rather than predefined capabilities or groups. This would not be the most efficient method for Thor, as he is looking to assign access based on departmental grouping, not content evaluation. Application access control: Although application access control could theoretically be used to govern access to certain files within an application, it is not the most likely choice for managing file access at an organizational or departmental level across multiple applications or systems. Application access controls are more specific to the functions within the application itself.

Authorization comes after authentication. Authentication is the process of verifying the identity of a user, system, or device. It often involves checking a username and password, or some other form of credentials. Authorization comes after a user or system is authenticated. It involves granting permissions to the authenticated user or system, determining what they can access and what actions they can perform. The incorrect answers: Authentication is granting permissions: This is incorrect. Authentication is about verifying identity, not granting permissions. Authorization is verifying the identity of a user: This is incorrect. Authorization is about granting access and permissions, not verifying identity. Authorization comes before authentication: This is incorrect. Authentication usually comes before authorization. You need to know who someone is (authentication) before you can decide what they can do (authorization).

type1/2/3 authenication ...


EER/FAR/CER
High FRR (False Rejection Rate): When deploying biometric access readers for areas labeled as critical security, setting the sensitivity to a high FRR is advisable. A high FRR means that the system is more stringent in its authentication process, leading to a lower chance of false acceptance (FAR), which is paramount in high-security areas. The trade-off with a high FRR is that legitimate users might occasionally be denied access and have to go through additional steps to authenticate, but this is acceptable in environments where security is the top priority. The incorrect answers: High FAR (False Accept Rate): A high FAR is not desirable in any security context, especially not in critical security areas, as it would mean a higher risk of unauthorized access being granted. Low CER (Crossover Error Rate): While a low CER indicates a good balance between FAR and FRR, in high-security areas, the priority is to minimize the chance of unauthorized access, which could mean accepting a higher FRR. Exactly at the CER (Crossover Error Rate): Setting the sensitivity exactly at the CER means equating FAR and FRR rates, which is not the most secure option for critical security areas. The focus should be on minimizing the risk of unauthorized access, typically by accepting a higher FRR.
The Equal Error Rate (EER) is the best indicator of the accuracy of a biometric system. It is the point where both the False Acceptance Rate (FAR) and False Rejection Rate (FRR) are equal. The lower the EER, the better the system is considered to be because it means the biometric system makes fewer mistakes in both falsely accepting an imposter and falsely rejecting an authorized user. The incorrect answers: While False Acceptance Rate (FAR) is an important metric for a biometric system, it only measures the likelihood of the system incorrectly accepting an access attempt by an unauthorized user. It does not take into account the system's ability to correctly identify authorized users, which is why it is not the best overall indicator of accuracy. Similar to FAR, the False Rejection Rate (FRR) only measures one aspect of a biometric system's performance: the likelihood of the system incorrectly rejecting an access attempt by an authorized user. While it's an important metric, it's not the best overall indicator of accuracy because it doesn't consider the system's performance in correctly rejecting unauthorized users. Crossing Error Rate (CER): This is a commonly mistaken term. The correct term is Crossover Error Rate (CER), which is essentially another term for Equal Error Rate.

Delegated Identity Management (DIM): Delegated Identity Management refers to a scenario where authentication is performed by one service but is trusted and accepted by another. In John's case, he is using his Facebook identity to log into the store online, which constitutes a delegation of the authentication process. The online store trusts Facebook to verify John's identity and does not require him to create a new account and password for their site. The incorrect answers: Single Sign-On (SSO): Single Sign-On is a user authentication process that allows a user to access multiple applications with one set of login credentials. However, SSO is generally used within the same organization or suite of services, rather than leveraging an external identity provider like Facebook. Federated Identity Management (FIM): Federated Identity Management is a system that links and manages the identity information across multiple autonomous systems or domains. While FIM encompasses the use of shared identities across different domains, the term "federated" usually implies a more mutual and cooperative arrangement, which might not be what is directly intended by the term "Delegated Identity Management." Active Directory Domain Services (AD DS): AD DS is a directory service for Windows domain networks and is not typically used for consumers logging into unrelated online stores. It is more focused on internal network authentication and directory services within an enterprise environment.

A cloud-based identity service: Given the requirements to minimize infrastructure, management overhead, and rapidly deploy the solution, a cloud-based identity service would be the best choice for Natalie. Cloud-based identity services can provide authentication for both on-premise and cloud-based services without the need for additional infrastructure. They are managed by the service provider, which reduces the overhead for Natalie's company, and can typically be deployed quickly. The incorrect answers: A federated identity solution: While federated identity solutions allow for single sign-on across different systems and can potentially work with cloud services, they may require more infrastructure and management overhead compared to a fully cloud-based identity service, especially if the federated solution is managed on-premise. An on-premise authentication solution: An on-premise solution would likely require significant infrastructure and ongoing management, which does not align with Natalie's goal to minimize infrastructure and overhead. A third-party identity service: This kind of service could potentially meet the requirements, but it is less specific than a cloud-based identity service. Third-party identity services could be either on-premise or cloud-based, so without more context, it's harder to assert that it would be the best solution compared to a clearly defined cloud-based identity service.

Object, subject. In the context of an access request, the "subject" is the entity that is making the request or performing an action, which is Francis in this scenario. The "object" is the entity that the action is performed on or that is being requested, which in this case is the "Tuna Notes" file. The incorrect answers: Subject, object: This is the inverse of the correct relationship. In this scenario, "Tuna Notes" is the object, not the subject, and Francis is the subject, not the object. Active entity, passive entity: While these terms could be used to describe a similar relationship, they are less commonly used in this context than "subject" and "object." Passive entity, active entity: This is the inverse of the active/passive relationship and is less commonly used than "subject" and "object."
The correct answer:

Multi-factor authentication, strong access controls, strong authorization controls, and encryption. For protecting Secret-level information, multiple layers of security are needed to ensure only authorized users can access the file and that the data remains protected even if credentials are compromised. Multi-factor authentication (MFA) verifies user identity through multiple factors, access and authorization controls ensure permissions align with role and clearance, and encryption protects the data at rest and in transit. Together, these controls provide defense in depth against unauthorized access.

Accountability: Accountability ensures that actions taken on a system can be attributed to an identified individual or entity, which provides assurance that only authorized users are accessing the system and using it properly. It involves the ability to track user activities and to hold users responsible for their actions, typically through logging and monitoring mechanisms. The incorrect answers: Authentication: While authentication is the process of verifying a user's identity before granting access to a system, by itself, it does not provide assurance that the system is being used properly after access has been granted. Authorization: Authorization determines what an authenticated user is allowed to do within a system, such as what resources they can access and what actions they can perform. It does not, however, track or record what the user actually does, which is necessary for accountability. Auditing: Auditing is the process of reviewing and analyzing records of system activities to ensure compliance with policies and procedures. While auditing is closely related to accountability and is often used to measure it, the term 'accountability' directly refers to the concept of users being answerable for their actions on the system.
Domain

Rule-based access control. Rule-based access control (RuBAC) does not primarily use the characteristics of the subject (the user or process requesting access) or the object (the resource being accessed) to grant or deny access. Instead, rule-based access control uses a set of pre-established security rules that govern access rights. The rules can include various conditions such as time of day, location, type of connection, etc. For example, a rule might specify that no one can access a particular system outside of business hours. The incorrect answers: Role-based access control: Role-based access control (RBAC) is based on the roles that individual users take on in a system. Under this model, users are granted access rights based on their role, which in turn is linked to the tasks they are expected to perform. This model is therefore inherently linked to the characteristics of the 'subject' (the user) in the authorization process. Attribute-based access control: Attribute-based access control (ABAC) is a model that grants access rights based on attributes associated with the user, the resource being accessed, and current environmental conditions. Both subject (user) and object (resource) attributes are central to this model of access control. Discretionary access control: In Discretionary Access Control (DAC), the data owner decides who can access specific resources. This access can be granted based on user identity or on the basis of rules that have been established by the user, so it can be argued that both subjects and objects play a role in this type of access control.



The incorrect answers:

Strong authorization controls, strong access controls, and detailed audit logs: Audit logs help detect unauthorized activity but don‚Äôt prevent it, making this option incomplete.

Strong authorization controls, enforcing the principle of need to know, and enforcing the principle of least privilege: These are sound administrative principles but lack technical enforcement mechanisms like encryption or MFA.

Strong authorization controls, strong access controls, and encryption: This is close but misses the added assurance provided by multi-factor authentication, which is critical for protecting sensitive or classified data.

Dual control, sometimes referred to as "two-person control" or "two-man rule," is a security principle that ensures that no single individual can access or use certain critical assets or perform high-risk tasks alone. This principle requires the presence or participation of two authorized individuals to execute certain actions, which drastically reduces the potential for malicious insider activity, fraud, or unintentional mistakes that can compromise security. By necessitating a second person, the chance of a single individual compromising a system or data is diminished. 

IAAA
vailability refers to the ability for a system or resource to be accessible and functioning properly. It is not a part of the IAAA model, which stands for Identity, Authentication, Authorization, and Accountability/Auditing. It is generally considered part of the CIA (Confidentiality, Integrity, and Availability) model of information security. The incorrect answers: Authentication is indeed one of the A's in the IAAA model. It refers to the process of verifying the identity of a user, device, or system. It usually involves a username and password, but can also involve other methods like biometrics or smart cards. Authorization is another one of the A's in the IAAA model. It refers to the process of granting or denying access to specific resources once a user, device, or system has been authenticated. Auditing, also known as Accountability, refers to the process of monitoring and recording the actions and activities of users within a system. It is a part of the IAAA model.

The correct answer: Mandatory Access Control (MAC) is designed for environments that require high security and strict control over data access. This model enforces access control based on security classifications and clearances, and is typically used in military or governmental agencies, making it suitable for a corporation operating in a highly regulated industry. The incorrect answers: Discretionary Access Control (DAC) can provide flexibility, but it can also lead to inconsistencies and potential security risks if not properly managed. Data owners may not always make decisions in line with corporate security policies or regulatory requirements, and this model does not offer the rigorous control needed in highly regulated industries. Role-Based Access Control (RBAC) is widely used and suitable for many organizations, but it may lack the granular control necessary in highly regulated industries where access needs to be tightly controlled, and compartmentalized based on specific clearances and categories of data. Attribute-Based Access Control (ABAC) can provide granular control based on multiple attributes (including the user's role, department, time of day, location, and even the type of transaction), but it can be complex to implement and manage. It may not offer the same level of stringent, compartmentalized control as the MAC model in highly regulated industries.

exmaples of something you have/know/are, type1/2/3

Federated access control relies on the concept of trust between different organizations or domains. In this model, a user's authentication process in one domain is trusted by another domain without the need to re-authenticate when accessing resources. A trusted third party (often an identity provider) vouches for or authenticates the user, and other entities (service providers) trust this authentication. A common example is Single Sign-On (SSO) solutions across different web services. The incorrect answers: Role-based access control (RBAC) is an approach where permissions are assigned to specific roles, and users are assigned roles. It's not based on the concept of a trusted third party for authentication. Instead, it centralizes permissions around roles within the system. In rule-based access control, access is granted or denied based on a set of rules, often defined by system policies. These rules can include aspects like time of day, the network source of a request, or other conditions. This model doesn't inherently rely on a trusted third party for authentication. Multi-factor authentication (MFA) is a method that requires a user to provide multiple types of credentials to authenticate their identity. This often includes something they know (like a password), something they have (like a smart card or a token), and something they are (like a fingerprint). MFA does not revolve around the concept of a trusted third party.

Verifying the identity of certificate holders: In a Public Key Infrastructure (PKI) system, RAs are responsible for verifying the identities of entities (e.g., individuals, organizations, or devices) before they are issued digital certificates by a Certificate Authority (CA). This process ensures that the digital certificates, which are used to establish secure communications and authenticate entities in a network, are issued to the correct and legitimate entities. The incorrect answers: Issuing digital certificates: This is primarily the role of a Certificate Authority (CA), not an RA. Although an RA assists in the process, the final issuance of a digital certificate is done by the CA. Providing support for certificate revocation: While an RA might assist in the certificate revocation process, it's the CA that holds the ultimate responsibility to revoke certificates when necessary. The RA can help identify instances where revocation may be needed, but the action of revocation itself is typically handled by the CA. Coordinating with other RAs: While it's true that RAs may need to coordinate with each other in large or complex systems, this is not their primary function. Their main role is to authenticate the identities of entities before a CA issues a certificate, ensuring the overall trustworthiness of the system.

D6
**********************************************************
Gray (Grey) box Pent test or attack (Partial Knowledge) Pen testing: The attacker has limited knowledge, a normal user, vendor or someone with limited environment knowledge.
Clear box
Black box

 Security audits are one of the most effective methods to identify potential vulnerabilities and threats in a system. A security audit is a systematic evaluation of the security of a company's information system by measuring how well it conforms to a set of established criteria. It involves a comprehensive examination of all system security measures, including hardware, software, procedures, networks, and even people to identify any areas where vulnerabilities and threats may exist. The incorrect answers: While statistical analysis can be used in the broader context of threat analysis to understand patterns, trends, and probabilities, it does not directly identify potential vulnerabilities or threats in a system. It's more about interpreting data rather than discovering vulnerabilities or threats. Attacker-centric threat modeling is a method of preemptively identifying, understanding, and addressing the threats that an attacker might exploit. It does provide insights into potential threats, but it doesn't directly help in identifying specific vulnerabilities in a system. A risk assessment is a method for identifying potential risks that could harm an organization. While it can help identify threats and vulnerabilities as part of the larger process, its main focus is to assess the potential impact of risks, their likelihood, and helps to prioritize risks based on these factors, rather than specifically identifying vulnerabilities and threats in a system.


Periodic staged intrusions: The best way for Nayeli to obtain valuable information about a network's vulnerabilities is through periodic staged intrusions, also known as penetration testing or ethical hacking. This proactive security measure involves simulating cyber attacks under controlled conditions to discover security weaknesses, potential entry points for actual attackers, and to understand the effectiveness of existing security measures. The incorrect answers: Periodic procedure updates: While updating procedures periodically is important for maintaining security, it does not directly provide information about network vulnerabilities. Procedures need to be actively tested against potential threats to reveal vulnerabilities. Periodic policy updates: Updating policies is a crucial part of maintaining a security posture, but policy updates alone do not expose or identify network vulnerabilities. Policies define the framework for security, but testing is required to find actual weaknesses in the network. Periodic drills: Drills often refer to exercises designed to prepare for emergency situations. While they can be valuable in training staff and testing incident response, they may not be as focused on identifying technical network vulnerabilities as a staged intrusion would be.

Logs: The best control that Natalie can use to compensate for the risk of role incompatibility under separation of roles is logging. By maintaining detailed logs of user activities, Natalie can create an audit trail that can help detect and review actions taken by individuals with conflicting roles. This provides a way to monitor and potentially correct any improper actions that result from the combination of roles. Logs can help ensure accountability and traceability, even when roles are not ideally separated. The incorrect answers: Access controls: While access controls are essential for limiting which resources a user can access, they do not directly compensate for the lack of separation of roles once access has been granted. Hashes: Hashes are used to verify the integrity of data and ensure that it has not been altered. They are not a control mechanism for handling separation of roles within an organization's operations. Processing totals: Processing totals is a method used to ensure that a batch of transactions has been processed correctly by comparing total values. It does not provide a means to compensate for the risks associated with individuals performing incompatible roles.

Ensure integrity of new data: Ross is creating a subroutine in the database front end to compare postal codes with street names to ensure the integrity of the new data being entered. This check helps verify that the data is accurate, consistent, and reliable, by confirming that the postal codes match the correct corresponding street names. The incorrect answers: Ensure availability of new data: Ensuring the availability of data typically involves maintaining access to data or systems, such as through redundancy or backup solutions. A subroutine that validates postal codes against street names is not directly related to data availability. Ensure confidentiality of the new data: Ensuring confidentiality involves protecting data from unauthorized access or disclosure. Comparing postal codes with street names does not pertain to the protection of data, but rather to its correctness and validity. Ensure the data entry staff are held accountable if they enter wrong data: While having checks in place can help trace the source of data entry errors and potentially hold individuals accountable, the primary reason for implementing such a subroutine is to prevent incorrect data from being entered into the system in the first place, thereby maintaining data integrity.

Security assessment and testing is the process of evaluating the security of an organization's systems, networks, and applications. This is to identify an organization's vulnerabilies, risks, and how effective the current security controls are. This process includes a variety of techniques, such as penetration testing, vulnerability assessment, security auditing, and risk assessment, among others. The incorrect answers: A risk assessment is the process of identifying and analyzing potential risks to an organization's assets. However, it doesn't encompass the entire range of activities involved in a security assessment. Penetration testing is a type of security testing that simulates a cyber attack to assess the organization's defenses against such attacks. It's a part of a broader security assessment but doesn't cover all aspects such as policy review, system configuration review, etc. Vulnerability assessment is the process of identifying and assessing vulnerabilities in an organization's systems, networks, and applications. This is also a part of the broader security assessment process, but it does not include penetration testing or risk assessment, which are also important aspects of a security assessment.

CWSS (Common Weakness Scoring System) is used to score and rank software weaknesses in terms of their severity, taking into consideration the attack surface, attack impact, and the environmental impact. Implementing the latest security patches and updates is an important part of maintaining a secure system, but it doesn't directly contribute to determining the CWSS score of a system. Rather, it is a reactive step taken to address vulnerabilities that have already been identified, and doesn't itself provide a measure of the overall security of a system or its specific weaknesses. The incorrect answers: Assessing the vulnerabilities of a system is an integral part of determining the CWSS score. The CWSS is specifically designed to score vulnerabilities based on their characteristics, and thus assessing the vulnerabilities of a system is directly tied to determining its CWSS score. Evaluating the effectiveness of security controls isn't part of the CWSS calculation itself, the effectiveness of security controls can affect the environmental score component of the CWSS, which takes into account how well a system is protected. Comparing the system's security score to industry standards is a valid method of using CWSS scores. By comparing a system's scores to industry standards or to scores of similar systems, one can gain insights into how the system's security measures up. While this won't directly affect the CWSS score, it can provide valuable context for interpreting the score and assessing the system's relative security.

Vulnerability assessments focus on identifying potential exploits, while penetration tests focus on actively exploiting them: A vulnerability assessment is a process that defines, identifies, and classifies the vulnerabilities in a computer, network, or communications infrastructure. It can also forecast the effectiveness of proposed countermeasures and predict their effectiveness when they are put into practice. Essentially, it's a systematic review of security weaknesses that an attacker could exploit. On the other hand, a penetration test (also known as a pen test) is a simulated cyber attack against a computer system, network, or web application to check for exploitable vulnerabilities. These are typically performed using manual or automated technologies to systematically compromise servers, endpoints, web applications, wireless networks, network devices, mobile devices, and other potential points of exposure. The focus here is not merely identifying vulnerabilities, but actively exploiting them to understand the level of access or other malicious activities a potential attacker could perform. The incorrect answers: Vulnerability assessments are more comprehensive: This is not necessarily true. The breadth and depth of either assessment largely depend on the scope defined before conducting the process. While vulnerability assessments may cover a wide range of systems, devices, and software to identify potential weaknesses, penetration tests can also be comprehensive, focusing on both the breadth of systems and the depth of attack vectors. Moreover, penetration tests often include vulnerability assessments as part of their process. Therefore, it's not accurate to categorize one as more comprehensive than the other. Penetration tests are more intrusive: The term 'intrusive' can be misleading here. While penetration testing does involve trying to exploit vulnerabilities (which could be seen as more 'intrusive'), it's done with the permission of the organization and with the aim of improving security. Both vulnerability assessments and penetration tests are invasive to some degree because they both involve analyzing and testing systems for weaknesses. However, the level of intrusiveness doesn't define the primary difference between these two. Vulnerability assessments are performed on a regular basis, while penetration tests are only performed when requested: The frequency of either test largely depends on the organization's security policies, available resources, and applicable regulatory requirements. Both vulnerability assessments and penetration tests can be performed regularly or on request. For instance, some organizations may perform vulnerability assessments regularly (daily, weekly, or monthly), while penetration tests might be conducted annually or bi-annually. Conversely, a company might request a penetration test after implementing a new system or post a security incident. The timing or frequency of these tests does not represent the primary difference between them.

Scheduling regular scans to identify new and existing vulnerabilities is the most crucial aspect when integrating vulnerability scanning into your security protocols. Given the dynamic nature of cybersecurity threats and the continuous updates in systems and software, regular scans are essential to identify new vulnerabilities as they emerge. Regular scans will help you keep up with any changes in your system configuration, detect outdated software, and track unpatched vulnerabilities. The incorrect answers: Selecting a scanning tool with the most comprehensive list of common vulnerabilities is important, but it doesn't ensure protection against new or emerging threats. Also, without regular scanning, even a comprehensive tool may fail to identify vulnerabilities that emerge after the initial scan. Defining the IP range for the scanning tool to focus on is a practical approach, but its utility is limited in an environment where the network configuration changes frequently. Defining the IP range also doesn't ensure the timely discovery of vulnerabilities unless combined with regular scanning. Ensuring the vulnerability scanning tool has the latest updates and definitions is a valuable practice as it keeps your scanning tool up-to-date with the latest known vulnerabilities, but without regular scans, even an up-to-date tool won't be effective in identifying new vulnerabilities in your systems.

The goals and objectives are clearly defined: Before having outside contractors conduct a penetration test, it is most important to ensure that the goals and objectives of the penetration test are clearly defined. Clear goals and objectives will guide the scope of the penetration test, the methodologies to be used, the systems to be tested, the rules of engagement, and the expected deliverables. This clarity is crucial for the penetration test to be effective, targeted, and for the results to be actionable. The incorrect answers: The penetration testers show us what the plan to do on a test system: While it's beneficial to understand the testers' approach and plan, having them demonstrate on a test system is not a prerequisite and is less critical than having clear goals and objectives for the actual penetration test. Our IT staff has been informed about the penetration test: Informing the IT staff about the penetration test is important to avoid confusion and ensure they do not mistake the test for a real attack. However, this should be done after establishing the goals and objectives of the penetration test. Everyone including senior management is unaware of the penetration test; to ensure the penetration test is as close to a real attack as possible: Keeping senior management and others unaware of the penetration test (a "blind" test) can provide insights into how an organization detects and responds to a real attack, but this is risky without clearly defined goals and objectives. Stakeholders must be aware and in agreement with the penetration test to provide authorization and to manage risks.

 Ensure accuracy and completeness: Claire is performing a one-for-one check of input documents most likely to ensure both the accuracy and completeness of the records. This type of check involves verifying that each item on a list or in a set of documents corresponds to an actual, specific counterpart in another set, ensuring that all items are accounted for and correctly recorded. The incorrect answers: Ensure completeness: While a one-for-one check does help to ensure completeness by making sure that all items are present, it also checks for accuracy by confirming that the items are correct. Therefore, the reason is broader than just completeness. Ensure correct sequence: A one-for-one check is not specifically about ensuring the correct sequence of items, although it may help identify sequence issues. Its primary purpose is to match items between sets to verify their existence and details. Ensure totals match the expected results: A one-for-one check typically focuses on the individual items rather than the aggregation of those items into a total. Ensuring totals match is more related to reconciliation tasks rather than the detailed verification of each item.
 
 Testing. Dynamic analysis involves executing the software under test and observing its behavior to detect possible errors. This approach is often used in testing, where a program is run with selected inputs and the outputs are checked for correctness. The incorrect answers: Inspections: These do not involve executing the software; instead, they involve manually reviewing the code or design documents. Code reading: This is a form of static analysis, not dynamic. It involves reading through the code to understand its purpose and functionality and to check for potential errors. Tracing: While tracing can be part of dynamic analysis, it refers specifically to following the flow of execution through a program, not the process of detecting errors as a whole.
 
  Employing the RTM as a dynamic, iterative tool that adjusts to changing requirements over time is the most significant aspect. This approach ensures that the RTM stays relevant and useful throughout the software development lifecycle, capturing changes in requirements, project tasks, and deliverable documents, as well as adapting to evolving security landscapes. This continuous adaptation and tracking of requirements are key to avoiding overlooked elements and mitigating potential vulnerabilities. The incorrect answers: Updating the RTM to reflect the requirements of every new software version is an important practice, but it's not the most significant aspect. This practice ensures that the new requirements are documented, but it doesn't necessarily ensure that the RTM evolves with changing project needs or that the implementation of these requirements is tracked effectively. Having the RTM encompass all software requirements, including project tasks and deliverable documents, is crucial for comprehensive project management. But the breadth of information included in the RTM is less significant than how this information is used and updated over time, particularly as the requirements and priorities of a project may change. Including specific security requirements in the RTM from the start is a best practice, as it embeds security into the software development process. Nevertheless, while this is important, it does not address the need for these requirements to be adaptable over time in response to changing threats and project objectives.
  
   regulatory investigation, primary purpose .....collect evidence and build a case against individuals or organizations: Regulatory investigations are typically initiated when there's a suspicion of violation of regulations or laws. The main goal is to gather facts, information, and evidence to determine if there has been a violation. If a violation is found, the evidence will be used to take enforcement action, which could include fines, penalties, or orders to change behavior. 
  
  enetration testing is a method used to evaluate the security of an application or system by simulating attacks from a malicious source. This includes abnormal user behavior or misuse. Penetration testing is designed to expose weaknesses in the system's defenses which attackers could exploit. The incorrect answers: Data Classification: Data classification is the process of organizing data by relevant categories so that it may be used and protected more efficiently. It involves categorizing data based on its level of sensitivity, value, and criticality to the organization. While this process can contribute to the overall security of data, it doesn't specifically address handling abnormal user behavior or misuse. Quality Assurance: Quality assurance (QA) refers to a way of preventing mistakes and defects in manufactured products and avoiding problems when delivering solutions or services to customers. However, QA mainly focuses on the functionality and performance of a software rather than its security aspects and its response to abnormal user behavior or misuse. Vulnerability Scanning: Vulnerability scanning is a comprehensive inspection of the potential points of exploit on a computer or network to identify security holes. It involves the use of automated software to scan a system against known vulnerability signatures. While vulnerability scanning is a critical part of maintaining system security, it primarily identifies known vulnerabilities, rather than specifically focusing on handling abnormal user behavior or misuse.
  

D7
**********************************************************
A high number of emergency change requests can be an indicator of insufficient or ineffective planning and control within the change management process. Emergency changes are typically unplanned and urgent, suggesting that the standard change request procedures may not be adequately addressing the needs of the organization or that changes are not being anticipated and managed proactively.

The MOST important aspect of configuration management is ensuring that all configuration changes are documented and approved. Configuration Management (CM) primarily focuses on establishing and maintaining consistency in a system's performance, functional, and physical attributes with its requirements, design, and operational information. Therefore, any changes to the system configuration need to be documented, reviewed, and approved. This practice helps avoid unnecessary or harmful changes, allows for the tracking of modifications, and aids in the ability to roll back changes if they cause problems. 


Communications team: Following the formal declaration of a disaster and the initial triage team assessment, the Communications team would most likely be called upon next. They play a crucial role in managing internal and external communications, informing stakeholders, coordinating with emergency services, issuing public statements, and keeping employees updated on the situation. Effective communication is critical in managing the aftermath of a disaster and in the recovery process. The incorrect answers: Facilities management: While Facilities management is important in addressing the physical impact of a disaster on the organization's property, the immediate next step after triage typically involves communication rather than facility repair or management. IT Services department: The IT Services department will be essential in restoring technological infrastructure and systems, but they are more likely to be called upon after the initial communication phase unless the disaster specifically impacted IT systems directly. Legal department: The Legal department will be important in navigating any legal implications of the disaster and for advising on regulatory reporting requirements. However, they are generally not the next team called upon immediately following the triage, unless there are immediate legal considerations that need to be addressed.

 Both internal and external users. To accurately measure the success of an incident handling process, it's helpful to gather feedback from both internal and external users. Internal users can provide insight into the efficiency of the process, while external users can offer perspective on how incidents were communicated and resolved from their end. The incorrect answers: All business units: While feedback from all business units can be valuable, it's only part of the picture. Including external users provides a more comprehensive understanding of the process's effectiveness. External investors, shareholders, and stakeholders: These parties are typically not directly involved in incident handling and are therefore less suitable for gauging the success of the process. Internal users only: While internal users can provide important feedback, it's also valuable to gain perspective from external users who interact with the organization's systems or services.

In our digital forensics, which of these should NEVER happen?
Modification of digital evidence: The integrity of digital evidence is of paramount importance in digital forensics. Modifying the digital evidence means altering the original data, which can potentially lead to wrongful conclusions. The accepted principle in handling digital evidence is to make a bit-by-bit copy of the original data (usually a hard disk or memory storage) and perform investigations on the copy. This ensures that the original data is preserved and untouched. Modifying digital evidence not only undermines the integrity of the investigation but could also have legal consequences. It should never happen in digital forensics. The incorrect answers: The destruction of physical evidence: While this might initially seem correct, there are scenarios where the destruction of physical evidence is necessary in digital forensics. For instance, once the relevant digital evidence has been extracted and properly documented, certain physical devices might need to be destroyed due to company policies, especially if they contain sensitive data. In some cases, authorities might also destroy physical evidence after a case is closed to prevent misuse of the data. While it's essential to handle physical evidence carefully, its destruction isn't always to be avoided. Taint of digital evidence: The phrase 'taint of digital evidence' might be interpreted as introducing or incorporating irrelevant or misleading data into the evidence. This is generally undesirable, as it could confuse the investigation or lead to false conclusions. However, unlike modification of digital evidence, this doesn't necessarily violate the integrity of the original evidence. It's more of an issue of poor practice or handling, which while should be avoided, doesn't hold the same level of prohibition as the modification of digital evidence. Collection of digital evidence: This is actually a crucial part of digital forensics. Investigators need to collect digital evidence in a forensically sound manner to help solve the case. This involves acquiring data from various digital sources while maintaining the integrity of the evidence. Without the collection of digital evidence, digital forensics would be impossible. Therefore, this is not only something that should happen, but it is a key aspect of the process.

 Gather information from different sources. Gathering information from different sources provides a more comprehensive view of the network, which can enhance the efficiency and effectiveness of Security Information and Event Management (SIEM) systems in correlating events and detecting attacks. This approach provides a holistic view of the network and enables the SIEM to identify patterns and connections that may not be visible when looking at data from individual sources. For example, gathering info from a router and honeypot devices may allow us to know exactly what an attacker is seeking, versus just router logs. The incorrect answers: Gather information only from web servers: This approach would limit the visibility of the SIEM to just the web servers, possibly missing attacks targeting other parts of the infrastructure. Gather information only from network devices such as routers and firewalls: While these devices are crucial for network security, they should not be the sole source of information for a SIEM. Attacks can also target servers, applications, and other parts of the network. Gather information only from application servers: This approach would limit the visibility of the SIEM to just application servers, missing attacks that may target other parts of the network.

Cold site: A cold site is the cheapest solution for an alternate site where specialized hardware is available in the event of a disaster. Cold sites typically provide only the basic physical environment for a data center, such as space, power, and connectivity, but not the actual hardware or data replication. The company would be responsible for installing and configuring their specialized hardware at the cold site in the event of a disaster, which allows for cost savings compared to more fully-equipped alternatives. The incorrect answers: Hot site: A hot site is a fully equipped and operational data center that includes not only the physical space but also the hardware, software, and live data replication necessary to assume operations quickly after a disaster. Hot sites are the most expensive alternate site solutions due to the high level of readiness and maintenance required. Warm site: A warm site is a compromise between a hot site and a cold site, providing some level of pre-installed equipment, but not the full real-time data replication of a hot site. Warm sites are less expensive than hot sites but more costly than cold sites because they include more infrastructure. Mobile site: A mobile site typically consists of a portable structure that can be shipped to a location and set up quickly in the event of a disaster. These sites are equipped with necessary hardware and connectivity solutions, making them more expensive than a cold site, which does not include such pre-installed equipment.

The data owner: Bob should notify the data owner first. In the context of incident response, the data owner is the individual responsible for the data that has been compromised and is in the best position to understand the significance of the breach. The data owner can assess the impact, help determine which customers are affected, and play a crucial role in coordinating the appropriate response, including customer notification and regulatory reporting. The incorrect answers: The Information Security steering committee: While the Information Security steering committee is an important body that should be informed about security incidents, notifying the data owner first is more critical for immediate response actions. The customers who were compromised: Notifying customers is a necessary step, but it typically comes after an initial assessment and containment of the breach. The data owner, along with the incident response team, will determine the timing and content of communication to customers. The regulatory agencies that govern our sector: Regulatory agencies are often required to be notified in the event of certain types of breaches; however, they are typically not the first point of contact. The immediate response involves assessment and containment, followed by notification to regulatory agencies as dictated by applicable laws and regulations, after the initial internal actions are taken.

Audit logs. Audit logs record events as they happen and can then be analyzed later to identify significant events or anomalies. They also meet the requirement of being passive, as they typically don't require active management until an event of interest is detected. The incorrect answers: Line monitoring: This typically involves active management and immediate response to issues, so it doesn't meet Nayeli's requirements for a passive, after-the-fact analysis strategy. Motion detectors: While motion detectors are indeed passive, they don't typically record detailed information that allows for in-depth analysis after the fact. Security guard and dog: This security measure is very active and immediate; it doesn't facilitate in-depth, after-the-fact analysis.

Identifying the critical files and directories to be monitored: Before deploying a File Integrity Monitoring (FIM) system, it's crucial to determine which files and directories are critical and should be monitored. This involves identifying sensitive data, configuration files, system files, and other assets that, if altered, could impact the security or functionality of a system. By prioritizing these critical assets, organizations can ensure that the FIM system is focused on the most relevant and potentially vulnerable parts of their environment. The incorrect answers: Installing the FIM software: While it's essential to install the FIM software to begin monitoring, it's more important first to determine what files and directories are critical. Installing the software without this determination could lead to inefficiencies or missed vulnerabilities. Configuring the FIM system: Configuring the system is a subsequent step after determining which assets are critical and after the software is installed. Without first identifying what to monitor, configurations may not be optimized. Conducting a risk assessment: While risk assessments are vital in understanding and prioritizing threats, the direct first step in implementing an FIM system specifically involves identifying the files and directories for monitoring. Once we know which files needs to be monitored we can conduct a risk assessment on those assets.

Non-Disclosure Agreement: The most likely document that would prevent Mark from sharing his inside knowledge with a friend who works for a competitor is a Non-Disclosure Agreement (NDA). An NDA is a legal contract between an employee and an employer that explicitly prohibits the sharing of confidential information with unauthorized parties. NDAs are designed to protect sensitive business information and trade secrets, and violating an NDA can result in legal consequences. The incorrect answers: Acceptable Use Policy: An Acceptable Use Policy (AUP) typically outlines the proper usage of company resources, such as computers, networks, and internet services. While it might include clauses about protecting company information, it is not as specifically targeted to prevent the sharing of inside knowledge as an NDA. Non-Compete Agreement: A Non-Compete Agreement (NCA) restricts an employee from working for competitors or starting a competing business for a specified period after leaving the company. While it's related to competition, it doesn't specifically prohibit the sharing of knowledge while employed. Third-party Access Policy: A Third-Party Access Policy governs how external entities can access the company‚Äôs resources and what security controls are in place for such access. It is more relevant to vendors or partners than to employees and does not directly address the issue of an employee sharing information with friends.

Calculate the risk: When faced with a system change that conflicts with security standards, the best initial course of action is to calculate the risk associated with the proposed change. By assessing the potential impact and likelihood of security issues that could result from the change, decision-makers can make an informed choice about whether to proceed with the change, modify it, or reject it. This risk assessment will help determine the most appropriate response, which may include enforcing security standards, adjusting the change, or adding mitigating controls. The incorrect answers: Enforce the security standard: While security standards are crucial and should be enforced, immediately enforcing them without assessing the specific risk may disregard the potential benefits of the system owner‚Äôs request or the uniqueness of the situation. Make changes to the proposed system change to match the security standard: Making changes to align with security standards is important, but without first understanding the risk, those changes might be unnecessary or ineffective. Add mitigating controls to the system: Adding mitigating controls may be part of the solution after a risk assessment. However, without calculating the risk first, it's not possible to know which controls would be effective or necessary.

UEBA (User and Entity Behavior Analytics)
Most frequently accessed data: UEBA (User and Entity Behavior Analytics) is a cybersecurity process that takes note of the normal conduct of users and then detects any anomalous behavior or instances when they deviate from these patterns. The "most frequently accessed data" is a key indicator used by UEBA. The reason is that changes in the data a user regularly accesses can indicate potentially harmful actions. For instance, if a user who typically accesses a particular set of data suddenly starts accessing a different, more sensitive set of data, it could signify a compromised account or insider threat. UEBA systems detect such sudden changes in behavior and alert cybersecurity teams accordingly. 

Criminal investigation: This is the most common type of investigation, as it involves the process of collecting evidence and information to solve crimes. Law enforcement agencies, such as the police, are primarily responsible for conducting criminal investigations. These investigations aim to identify, apprehend, and prosecute offenders. The incorrect answers: Internal investigation: This is not as common as criminal investigations. It refers to the process where organizations or companies conduct inquiries into alleged misconduct, violations of policies, or regulatory issues within the organization. Although important, these investigations are typically limited to specific organizations and not as widespread as criminal investigations. Forensic investigation: While forensic investigation plays a significant role in solving crimes, it is not as common as criminal investigations. It refers to the application of scientific methods and techniques to gather evidence and analyze it in the context of legal proceedings. Forensic investigations usually occur within the scope of criminal investigations, making it a subset of the broader category. Environmental investigation: This type of investigation is less common compared to criminal investigations. It focuses on examining environmental issues, such as pollution, contamination, or natural resource damage. While essential, these investigations are typically carried out by environmental agencies or specialized units, and their occurrence is not as frequent as criminal investigations.

Build a physical barrier around the data center to deter floodwater. Physical barriers, such as flood walls or levees, are preventive controls that stop or minimize floodwater from entering the facility in the first place. This directly mitigates the physical risk of flooding, protecting both equipment and operations. Since relocation isn‚Äôt feasible, the next best solution is a structural control that prevents the hazard from impacting critical systems.



The incorrect answers:

Implement CCTV systems to monitor potential flood events: Monitoring provides awareness but does not prevent or reduce the impact of flooding. It‚Äôs a detective control, not a preventive one.

Develop policies and procedures for flood response: Policies are important but administrative controls; they guide action after a flood occurs, not mitigation.

Install a water detection system with automated water removal pumps: This is a corrective control that activates once flooding has already started, helping to reduce damage but not to prevent the event.

Redundant/Reciprocal/Mobile /Subscription/community  / hot/cold/warm sites
 In a disaster recovery context, a subscription site is a service provided by a third-party company that has servers and other computing resources available upon request. These servers are typically kept offline and are brought online in the event of a disaster. This allows an organization to quickly switch operations to the subscription site, thereby maintaining business continuity during the disaster. The incorrect answers: Reciprocal refers to an agreement between two organizations to host each other's backup hardware and data in the event of a disaster. This doesn't match the scenario described. Redundancy involves having duplicate hardware and data ready to take over in the event of a failure. While the servers described are in some sense a form of redundancy, the term doesn't adequately describe the situation where servers are kept offline and brought online only during a disaster. A mobile site, in the context of disaster recovery, usually refers to a portable temporary setup, like a trailer equipped with necessary hardware and communication links, which can be dispatched to the disaster-struck site. This doesn't match the situation described.



D8
**********************************************************
: Scrum is a methodology that emphasizes flexibility and collaboration in the development of a project. It is a popular choice for agile software development, which focuses on delivering working software quickly and iteratively. It is designed for teams of three to nine members who break their work into goals that can be completed within timeboxed iterations called "sprints", typically two weeks, and track progress and re-plan in daily stand-up meetings called "scrums". The incorrect answers: Waterfall is a traditional, linear approach to project management that involves completing distinct phases of development in sequence. This model lacks the flexibility that Scrum offers because it typically doesn't proceed to the next phase until the current one is fully completed and it is difficult to return to a previous phase if changes are needed. 

Lean is a methodology that focuses on reducing waste and increasing efficiency in the development of a project. While it can bring some level of flexibility and efficiency, Lean does not emphasize on collaboration and rapid changes to the same extent as Scrum. DevOps is a culture and practice that aims to shorten the systems development life cycle and provide continuous delivery with high software quality. DevOps is focused more on the collaboration between development and operations teams and continuous delivery, not so much on project development methodology or the ability to pivot during the development process like Scrum.

 The BSD (Berkeley Software Distribution) license allows users to alter the original software and sell the altered software, as long as they give proper credit to the original developers and do not use their name in the altered software. This license is more permissive than other open source licenses, such as the GNU (General Public License) which requires that any modified software be released under the same license as the original software. The incorrect answers: The GNU General Public License (GPL) does allow the modification and sale of the software, but it requires that the same freedoms (including access to the source code) be preserved in derivative works. This means you can sell the modified software, but you can't prevent others from doing the same with your modifications. The Apache License does allow for modifications and sale of the software but, similar to the GPL, it includes clauses for attribution and preservation of the original license and disclaimers. It is a permissive license, but it also requires any modifications to be documented. CKR is not a recognized open source software license.

 Cleanroom methodology is a software development methodology that emphasizes the importance of creating software that is free from errors and defects. This is done by creating a clean and controlled environment where developers can work on the software without introducing any external factors that may compromise its integrity. It involves rigorous specifications, formal verifications, correctness by construction, and statistical usage testing. The cleanroom methodology places a strong emphasis on avoiding defects from the onset, rather than identifying and correcting them later. The incorrect answers: The waterfall model is a sequential (non-iterative) software development process, in which progress is seen as flowing steadily downwards (like a waterfall) through the phases of conception, initiation, analysis, design, construction, testing, production/implementation, and maintenance. While testing and correctness are part of the waterfall model, it doesn't focus specifically on the integrity of software like the cleanroom methodology does. Lean software development is a translation of lean manufacturing and lean IT principles and practices to the software development domain. It focuses on eliminating waste, amplifying learning, deciding as late as possible, delivering as fast as possible, empowering the team, and seeing the whole. While it can contribute to quality, it's not focused on software integrity in the same way the cleanroom methodology is. Agile software development emphasizes flexibility, customer collaboration, and delivering valuable software frequently. It encourages iterative progress, adaptability, and swift response to changes. However, unlike the cleanroom methodology, agile is not specifically centered around ensuring software integrity, even though quality is a key aspect of the agile approach.

 Lost or missing features after major code changes. 
 Regression testing is a type of software testing that ensures that previously developed and tested software still performs as expected after changes or additions. It is used to catch new bugs, or regressions, in existing functionality that may have occurred due to those changes.
 
  SQL injection is a code injection technique that attackers use to insert malicious SQL statements into input fields for execution by the underlying SQL database. This is usually done with the intent of manipulating the database to reveal information that it should not, such as user data. The best way to protect against a SQL injection attack is by implementing input validation and sanitization on all user-supplied data. This means that any data coming into the system from a user input is treated as untrusted and is carefully examined and cleaned. Non-alphanumeric characters that are key to SQL injection attacks, such as quotation marks and semicolons, are either escaped (treated as text rather than code) or removed. Additionally, using parameterized queries or prepared statements can also help protect against SQL injection.
  
Database transactions require atomicity, consistency, isolation, and durability, also referred to as the ACID model. 
  
**Atomicity** -  all or nothing' principle. If a transaction consists of multiple steps, atomicity guarantees that either all the steps are executed successfully and the transaction is committed, or if any step fails, the entire transaction is rolled back. No intermediate state is acceptable, 
**Consistency** - Consistency guarantees that a transaction brings the database from one valid state to another valid state. All database rules, constraints, ensure integrity conditions satisfied after the transaction completes.
**Isolation** - Ensuring that a transaction cannot be interrupted by other transactions  This relates more to 'Isolation' in the ACID model
**Durability** - Durability guarantees that once a transaction is committed, its changes are permanently saved. The data remains intact even in the event of system crashes or power failures.

**Principle of Flow. The Fellowship's practice of prioritizing the completion of their mission (destroying the Ring) over battling every enemy they encounter aligns with the Principle of Flow in the Scaled Agile Framework (SAFe). This principle highlights the importance of delivering value through solutions that smoothly flow through the entire system on an ongoing basis, as opposed to focusing on individual tasks or obstacles. 

**Value Streams: Value streams represent the series of steps an organization uses to build Solutions that provide a continuous flow of value to a customer. While the Fellowship is focused on delivering value (completing their mission), their practice doesn't directly relate to this concept. 

**Built-in Quality: This principle is about ensuring that each solution component meets appropriate quality standards throughout the development process. The Fellowship's strategy doesn't directly relate to this principle. **Lean-Agile Leadership: This principle emphasizes the role of leaders in embodying the core values and principles of Lean and Agile. The Fellowship's strategy of prioritizing the mission over individual battles isn't directly related to leadership.

 Address space layout randomization (ASLR)
 is a technique used to prevent code injection attacks by randomizing the memory locations of executables, making it difficult for attackers to predict where they will be stored and therefore makes code injection attacks significantly more challenging. The incorrect answers: Encrypting memory is used to protect sensitive information residing in a computer's memory from being read by unauthorized individuals or processes. However, it does not randomize the memory locations of executable code and, hence, doesn't directly prevent code injection attacks. Input validation is a method used to check and clean input from users to prevent things like SQL injection and cross-site scripting attacks. It is an important security practice but doesn't protect against memory-based attacks like code injection by randomizing memory locations. Disabling unnecessary kernel extensions can enhance system security, but it does not directly prevent code injection attacks, nor does it randomize memory locations of executable code.
 
 A synthetic transaction is a testing method that uses automated simulations of user interactions or transactions to monitor system performance. These transactions are called "synthetic" because they're not derived from actual user interactions, but are rather artificially created for testing purposes. They are designed to simulate an action or path that a user might take in an application. They are 'self-contained' because they are completely constructed and executed within a test environment and do not depend on external factors or real user data. 
 
 The Common Weakness Scoring System (CWSS) is a methodology used to score and rank software weaknesses. CWSS considers a variety of factors when scoring a weakness, such as the attack surface, the attack impact, and the environmental impact. However, the length of time the weakness has existed is not directly factored into the scoring system. While it's true that weaknesses that have existed for a longer time may have a higher chance of being exploited, the CWSS focuses more on the characteristics of the weakness itself and the impact it could have if exploited. 
 
  The key distinguishing factor between CMM Level 1 and Level 2 is the transition from ad-hoc, undocumented processes to repeatable and documented ones. At Level 2, the aim is to gain control over projects by defining processes that are repeatable, ensuring a consistent outcome. This makes standardizing and documenting security processes the most critical step in moving from Level 1 to Level 2. The incorrect answers: Training employees about IT Security protocols is vital for a secure IT environment, but it's not the defining aspect of the transition from CMM Level 1 to Level 2. Training can only be effective once processes are standardized and documented; otherwise, the training content may lack structure or consistency. This is why this option, although crucial in the larger context, is not the most critical in transitioning between these two specific levels. Investing in advanced security software can help bolster security, but it does not directly address the core issue at Level 1, which is the lack of repeatable and documented processes. Software can only be effectively utilized when integrated into standardized processes, making it a less relevant step for this specific transition. Conducting a company-wide audit can help identify security vulnerabilities, which is an essential part of establishing an IT security roadmap. But, identifying vulnerabilities is just one part of the overall process and does not directly lead to the establishment of repeatable and documented processes. Although this step is important in the overall security strategy, it's not the most crucial in transitioning from CMM Level 1 to Level 2.
 
 Agile methodology emphasizes iterative progress, team collaboration, and flexibility to changing requirements. It promotes adaptive planning and encourages rapid and flexible responses to change. The incorrect answers: Waterfall: This is a linear and sequential approach where each phase must be completed before the next phase begins. There's no iteration, and changes can be difficult to implement once a phase has been completed. Spiral: This model focuses on risk assessment and constant improvement in multiple iterations or 'spirals'. While it involves iteration, it centers on identifying and managing risks. V-Model: Also known as the Validation and Verification model, it is an extension of the waterfall model. Development and testing activities are concurrent, but it doesn't involve iterative development like Agile.
 
 Testing individual units or components of the software: Unit testing is a level of software testing where individual components or units of a software are tested. The purpose is to validate that each unit of the software performs as designed. A unit is the smallest testable part of any software. It usually has one or a few inputs and usually a single output. This kind of testing is done during the development (coding phase) of an application by the developers. Unit testing ensures that each part of the code functions correctly, and it is typically automated to save time and improve precision. The incorrect answers: Testing the functionality of the entire software system: While this statement seems accurate, it is more related to "system testing" than "unit testing." System testing involves evaluating the system as a whole to check if it meets the defined requirements. Unlike unit testing which tests individual components, system testing verifies the entire system's functionality. Hence, it is done after all the components have been integrated, not at the individual component level. Testing the security of the software system: This refers to "security testing", not unit testing. Security testing is a process intended to reveal flaws in the security mechanisms of an information system that protect data and maintain functionality as intended. It includes tests to uncover vulnerabilities like SQL Injection, Cross-Site Scripting, and data breaches, among others. While individual components might have security features that can be tested during unit testing, the holistic security of a software system is not the purpose of unit testing. Testing the compatibility of the software system with other systems: This is more in line with "compatibility testing", not unit testing. Compatibility testing is a type of software testing used to ensure compatibility of the system/application/website built with various other objects such as other web browsers, hardware platforms, users (in case if it's very specific type of software like a gaming application for non-technical people), operating systems etc. This type of testing helps find out how well a system performs in a particular environment that includes hardware, network, operating system and other software etc. It is not focused on individual units or components as in unit testing.
 
 Injection: Injection vulnerabilities arise from not properly validating input data. Unused code and components do not directly correlate with this vulnerability. Cross-Site Scripting: This type of vulnerability is specific to web applications, not source code complexity. Cross-Site Scripting (XSS) attacks involve injecting malicious scripts into webpages viewed by other users. Unused code and components do not directly introduce this type of vulnerability. Buffer Overflow: A Buffer Overflow vulnerability would arise from improper memory management, not unused code. Buffer overflow vulnerabilities occur when a program writes to a memory space that is not allocated for its use. Unused code and components do not directly introduce this type of vulnerability.
 
 
The software installs correctly on the customers hardware: This is more in line with installation or compatibility testing, not regression testing. 
Interfaces between components in the software: This would be more related to integration testing, which verifies that different components of the system work well together. 
Processes and security alerts when encountering errors: This would be more related to error or exception handling testing

 Source code: Source code should have the most limited access because it is the human-readable version of the software and contains the original design and structure of the application. Protecting access to the source code is crucial as it is the most informative for anyone looking to understand or modify the application, which could lead to security vulnerabilities or intellectual property theft if it falls into the wrong hands. The incorrect answers: Object code: Object code, which is the output of the compilation of source code, is an intermediate form between source code and executable code. It is less readable than source code and typically requires further linking to be run. While still sensitive, it is not as critical to protect as the source code. Executable code: Executable code, which can be run by a computer's operating system to perform the functions the software is designed to do, is important to protect. However, it does not usually contain the detailed information present in the source code, making it less sensitive. Machine code: Machine code is a low-level code for computers that is actually executed by the computer's CPU. It is the most difficult for humans to read and modify due to its binary form. While it is important to protect, the source code is usually of greater interest and should be more tightly controlled.

Compiled language like C+ or Pascal. Compiled languages are converted into machine code that can be directly executed by the computer's hardware. The resulting binary (such as an EXE file) cannot be easily inspected or reverse-engineered to view the original source code. The incorrect answers: Executable binary: This is not a programming language, but a format for program files that can be directly executed by a computer's hardware. Assembly language: While assembly language can be used to write executable code, it is much lower-level and more difficult to work with than a high-level compiled language like C+ or Pascal. Interpreted language like VBScript or Visual Basic for Applications (VBA): Code written in interpreted languages is generally distributed in a format that allows the source code to be viewed, which would not meet Marsha's need to prevent inspection of the source code.

System development phase: Building security controls and audit trails into a new application is best accomplished during the system development phase of the system development life cycle (SDLC). At this stage, the application is being designed and programmed, which allows for the integration of security features directly into the system architecture. Addressing security early in the development process ensures that it is not just an afterthought and typically results in stronger, more effective controls. The incorrect answers: System initiation phase: While the system initiation phase often involves defining high-level requirements, the actual building and integration of detailed security controls and audit trails occur later during the development phase. System implementation phase: The system implementation phase typically involves installing and deploying the system. While some aspects of security can be configured during this phase, integrating security controls into the application itself should have already taken place in the development phase. System operations phase: By the time the system reaches the operations phase, it is already in use. Adding or significantly altering security controls at this stage can be more difficult and disruptive, making it less ideal compared to integrating them during the development phase.

Software configuration management: The most likely tool Natalie can use to determine which versions of the software components constitute the current product is software configuration management. Software Configuration Management (SCM) involves tracking and controlling changes in the software, part of which includes maintaining the records of what versions of each software component are part of a specific build or release. SCM tools help manage different versions and ensure that the configuration of the product is known and reproducible. The incorrect answers: Bug tracking: Bug tracking tools are used to record, report, and manage bugs within the software development process. While they are an essential part of the development workflow, they do not typically maintain information about which versions of software components are used in the product. Source code repository: A source code repository is where the actual code is stored, and it can be part of an SCM system. However, by itself, it does not necessarily provide the overarching view of which versions of each component are combined to make the current product; that's the role of SCM. Versioning control: Version control is a component of SCM and refers specifically to the management of changes to documents, computer programs, large websites, and other collections of information. While version control systems are integral to SCM, the term SCM is more encompassing of the task Natalie needs to perform.

The interactions between different components of the software system: Interface testing is a type of software testing that verifies whether the communication between different software components is functioning correctly. In software development, an interface is a point where two components meet and interact. This could be software modules, different systems, or hardware and software. The main purpose of interface testing is to ensure that all interactions across these interfaces are successful and data is communicated correctly between them. The incorrect answers: The compatibility of the software with different operating systems: This type of testing is actually referred to as compatibility testing. It is a type of non-functional testing carried out to check whether your software can run on different hardware, operating systems, applications, network environments or Mobile devices. While it's important to make sure the software works across different environments, this is not the focus of interface testing. The user experience of the software: This refers to usability testing, not interface testing. Usability testing is a method of testing the functionality of the software from an end-user's perspective. It is used to assess the software‚Äôs ease of use and whether the user interface is intuitive and easy to understand. While interface testing could impact user experience (for example, if poor interface integration leads to slow response times), it does not directly assess the quality of the user experience. The security of the software: This is known as security testing, which is a testing approach to ensure software systems and applications are free from any vulnerabilities, threats, or risks that may cause a big loss. Security testing's main objective is to identify any vulnerabilities or weaknesses in the system that could result in a loss of information, revenue, or reputation. While security could be affected if interfaces aren't properly secured, interface testing in and of itself does not typically encompass the full breadth of security testing.  

 Agile methodology is considered the most effective for managing software development in a complex environment. It allows for rapid adaptation to changing requirements and environmental factors. Agile development emphasizes iterative progress, flexibility, and collaboration with stakeholders. This approach to development tends to work well in complex environments where requirements are often changing and difficult to fully define at the outset of the project. The incorrect answers: Scrum is a specific type of Agile methodology that focuses on dividing work into small manageable 'sprints'. While Scrum can be an effective methodology in a complex environment, it is a subset of Agile, and not all Agile methodologies are Scrum. This answer is less comprehensive than Agile. The Waterfall model is a sequential design process, often used in software development, where progress is seen as flowing steadily downwards (like a waterfall) through the phases of conception, initiation, analysis, design, construction, testing, production/implementation, and maintenance. In a complex environment, the Waterfall methodology is less adaptive to changes and has a higher risk of project failure if upfront requirements and scope are not clearly defined and accurately estimated. Lean software development is a translation of lean manufacturing and lean IT principles and practices to the software development domain. It focuses on eliminating waste, amplifying learning, and delivering as fast as possible, among others. While it can be effective, the choice of Lean or Agile would depend on the specific characteristics of the project and organization. In general, Agile is often seen as more universally applicable in complex software development scenarios.



What type of analysis can help one quickly observe anomalous behavior in an application without needing access to source code?

A. Static binary analysis

B. Static source analysis

C. Dynamic source analysis

D. Dynamic binary analysis









Domain
6. Security Assessment and Testing
The primary reason for using an industry-standard report format is to ensure that the test scope, objectives, and results are presented in a way that is relevant for senior management to understand and make informed decisions. Consistency and technical details are secondary, as the primary focus is on conveying actionable information in alignment with management priorities.

Domain
4. Communication and Network Security
WPA3 is the most secure wireless encryption standard, offering stronger encryption, protection against brute-force attacks, and enhanced security for public Wi-Fi networks through features like Simultaneous Authentication of Equals (SAE). WEP is outdated, easily cracked within minutes, and highly vulnerable to replay and IV-based attacks. WPA and WPA2 improved encryption over WEP but remain susceptible to offline dictionary attacks and vulnerabilities like KRACK (Key Reinstallation Attack), making WPA3 the preferred choice for securing modern wireless networks

Domain
1. Security and Risk Management
The Wassenaar Arrangement is an international export control agreement that regulates the transfer of dual-use technologies, including strong encryption, to prevent misuse by unauthorized entities. While GDPR focuses on data privacy and CFAA addresses cybercrime, they do not govern international technology exports. FTAs do include cybersecurity provisions, but they do not specifically govern encryption exports. Understanding compliance with the Wassenaar Arrangement is critical when dealing with global cybersecurity trade laws. 

Domain
3. Security Architecture and Engineering
Confusion refers to the cryptographic technique of making the relationship between the plaintext and ciphertext as complex as possible to thwart attacks. 
Diffusion refers to spreading the plaintext across the ciphertext, and non-repudiation ensures actions cannot be denied. 
A secret key is simply a key used in encryption, but confusion is the correct term for this cryptographic concept. 


Domain
7. Security Operations
 The security manager is responsible for deciding how to handle physical security incidents, such as response strategies and managing the overall incident. Security officers are typically tasked with day-to-day duties like monitoring, deterrence, and incident detection, but do not make high-level decisions on incident response.
 
 
 Domain
6. Security Assessment and Testing

 A security regression test ensures that updates or changes to a software application do not unintentionally alter or break existing security controls. 
 This type of testing is crucial when updating security features to ensure compliance with new regulations while maintaining previous functionality. 
 Unit tests and integration tests focus more on functionality rather than security configurations.
 
 
 Domain
7. Security Operations
 Establishing a continuous monitoring program provides ongoing assessment of security controls, ensuring risks are detected and managed in real-time. While automating scans and audits can help, they are periodic. Continuous monitoring offers more comprehensive oversight, allowing for immediate response to emerging threats or changes in the environment. 
 
 Domain
1. Security and Risk Management
 The primary goal of a privacy control assessment is to evaluate how effectively the security controls protect sensitive information and ensure they comply with regulatory requirements. While reviewing physical security and employee training are important aspects, the assessment focuses on the broader protection of data privacy as well as ensuring adhere to the new regulations.
 
 Domain
2. Asset Security
 Retaining assets beyond their required retention period increases the risk of unauthorized access and data breaches. Older data may not be subject to the latest security controls, making it a target for attackers. While storage costs and inefficiencies are concerns, they are secondary to security risks. Data migration and integrity issues are possible but do not directly address the main security concern of prolonged retention.
 
 
 Domain
1. Security and Risk Management
 The Process for Attack Simulation and Threat Analysis (PASTA) is a risk-based threat modeling methodology that aligns security threats with business objectives. It emphasizes risk management, business impact analysis, and attack simulation to prioritize security efforts. 
 STRIDE classifies threats without a direct business impact focus, 
 VAST is a scalable enterprise threat modeling methodology, 
 and DREAD is a risk assessment framework that scores threats rather than aligning them with business goals.
  
 Domain
7. Security Operations
 Relocating the server to a secure data center provides the most comprehensive solution by addressing the root cause of the vulnerability‚Äîits physical location. Other measures (access controls, intrusion detection, biometrics) add security layers but don‚Äôt fully mitigate the risk of physical theft. 
 
 Domain
1. Security and Risk Management

 The recovery time objective (RTO) is the maximum time allowed to restore a system after a disruption, ensuring that business operations can continue within an acceptable time frame. In this case, the 10-minute restoration time refers to how quickly the server can be restored, which must be completed within the 30-minute maximum tolerable downtime.
 
 Domain
1. Security and Risk Management

  Integrity ensures that data remains accurate, complete, and unaltered. A man-in-the-middle (MITM) attack that modifies a financial transaction compromises data integrity because it results in unauthorized changes to the information being transmitted. 
  Least privilege violations impact access control, not integrity. 
  DDoS attacks affect availability, not integrity, 
  and data exfiltration compromises confidentiality, rather than integrity. 
  Maintaining cryptographic protections like message authentication codes (MACs) and digital signatures helps preserve integrity. 
  
  
  Domain
5. Identity and Access Management (IAM)
  Role-based access control (RBAC) is most likely to ensure effective management of user permissions because it directly ties access to specific job functions, reducing the risk of users having excessive or unnecessary permissions. MAC enforces more rigid controls and is typically used in high-security environments. Rule-based systems can be complex to manage, and allowing employees to determine their own access introduces significant security risks.
  
  Domain
5. Identity and Access Management (IAM)
  Just-In-Time (JIT) privileged access creates temporary accounts only when needed and removes them after use, reducing the attack surface. This approach limits the window of exposure for privileged credentials. Role-based access (RBAC) assigns static roles, while dynamic inheritance and manual approvals are unrelated to JIT. JIT is widely used in zero-trust architectures and cloud security to mitigate privileged account abuse. For support or reporting issues
  
  
  Domain
1. Security and Risk Management
   Qualitative risk analysis is more efficient because it relies on subjective assessments, often from experienced personnel, rather than time-consuming numerical calculations. This makes it quicker to identify which risks should be prioritized based on their impact and likelihood, especially when time or data for a full quantitative analysis is not available. Quantitative analysis provides more precise data but often requires significant resources to compute monetary values, which may delay decision-making. 
   
   
   Domain
7. Security Operations
   A parallel test involves running backup systems alongside production systems to verify recovery capabilities without disrupting operations. A full interruption test involves shutting down production systems, which is too risky for a global enterprise. A walkthrough test is useful for reviewing procedures but does not validate system functionality. A cutover test involves switching to the recovery site completely, which can disrupt business operations. A parallel test provides a balance between verification and operational continuity
   
   
   Domain
4. Communication and Network Security
   
   The Application layer (OSI Layer 7) manages the interactions between applications and the network, ensuring that updates and patches are correctly sent and received. It handles data formatting and communication services for application processes, while lower layers manage the actual data transmission and routing. 
   
   Domain
3. Security Architecture and Engineering
   Mandatory Access Control (MAC) provides the highest level of security by strictly enforcing access policies based on security classifications and predefined rules, reducing the risk of unauthorized data access. 
   Transparent Data Encryption (TDE) protects data at rest but does not prevent unauthorized access. 
   DLP helps prevent data leaks but does not enforce access controls. 
   Database Activity Monitoring (DAM) detects suspicious activity but does not actively prevent unauthorized access. 
   
   Domain
2. Asset Security
    - Different countries have unique data retention laws, requiring organizations to tailor retention policies to local regulations. Encryption does not exempt organizations from compliance, and indefinite storage increases risks of data breaches and regulatory violations. Applying a single retention period may violate local laws. Implementing region-specific retention policies ensures that legal obligations are met while minimizing security risks. 
	
	Domain
2. Asset Security
	Defining asset handling requirements involves considering factors such as data classification, legal regulations, and physical security. The software version is generally not a primary concern in determining asset handling but may be relevant for patch management and security updates.
	
	
	Domain
2. Asset Security
	we should focus on selecting security controls that align with both organizational goals and regulatory requirements to ensure compliance and successful accreditation. Although using controls from other frameworks or previous projects may help, the most effective approach is to tailor controls specifically to the needs and regulations governing the current project. 
	
	Domain
1. Security and Risk Management
	Security controls are the mechanisms (including policies, procedures, and technical safeguards) used to enforce security requirements. They reduce risks and ensure compliance with security policies. Evaluation criteria are standards for assessing controls, while a security plan and requirements outline what needs to be protected but do not directly enforce protection.
	
	Domain
6. Security Assessment and Testing

	Manually inspecting the system component ensures the most accurate verification of configuration, as it allows direct review of settings without the potential false positives or incomplete analysis that automated tools may produce. White-box testing focuses on internal workings but doesn‚Äôt guarantee full configuration correctness, and vulnerability assessments seek security gaps rather than correct configurations.
	
	Domain
7. Security Operations
	A warm site has pre-installed hardware and some pre-configured systems, but staff and full data synchronization occur only when activated. A hot site is fully operational with live data, making it more expensive. A cold site lacks equipment, delaying recovery. Mobile sites are temporary solutions, usually for emergency response. A warm site balances cost and readiness, making it ideal for organizations needing quick recovery without the expense of a fully functional hot site
	
	
	Domain
6. Security Assessment and Testing
	Conducting an immediate user access review and implementing a structured review process helps mitigate risks associated with excessive or outdated privileges. Revoking all access could disrupt operations unnecessarily. While a forensic investigation may be warranted if there are signs of misuse, it is not the primary corrective action. Reporting to management is important but does not resolve the issue. Regular access reviews enhance security by ensuring least privilege principles are enforced.
	
	Domain
8. Software Development Security
	Security as Code (SaC) refers to integrating security controls and checks directly into the development pipeline and processes early in the software lifecycle, ensuring security is not an afterthought. It contrasts with configurations (CaC) and policies (PaC), which focus on infrastructure or compliance rather than embedding security practices in development workflows. 
	
	Domain
7. Security Operations
	Security Orchestration, Automation, and Response (SOAR) platforms use playbooks to automate incident detection, response, and handling. SOAR integrates with other security tools to automate processes based on predefined workflows. While SIEM and EDR systems provide valuable security information and incident detection, they do not typically manage automated responses as SOAR does. CIRTs are teams that handle incident management.
	
	Domain
1. Security and Risk Management
	 Civil investigations address legal disputes between parties, typically leading to lawsuits where financial compensation or other remedies are sought. Since this case involves a breach of contract and financial damages, it falls under civil law. Criminal investigations involve law enforcement, regulatory investigations enforce compliance with industry regulations, and administrative investigations handle internal organizational issues. Because the company is pursuing financial damages in a lawsuit without regulatory or law enforcement involvement, this is classified as a civil investigation
	 
	 Domain
1. Security and Risk Management
	 hen an employee is terminated, the highest priority is preventing unauthorized access by immediately revoking all accounts, passwords, and physical access. Delaying this step could allow a disgruntled employee to steal data, disrupt operations, or commit insider threats. While documentation, asset retrieval, and password changes are important, they do not address the immediate risk of continued access, which is the first security priority in offboarding.
	 
	 Domain
2. Asset Security
	 The best approach is to apply the strictest data handling standard across all jurisdictions, ensuring compliance with the most restrictive privacy laws. This reduces legal risks and simplifies compliance management. Developing separate policies for each country can be complex and inconsistent. Allowing local offices to define their own handling procedures increases the risk of non-compliance. A policy focused only on headquarters neglects international operations.
	 
	 Domain
3. Security Architecture and Engineering
	 A Public Cloud is the most appropriate choice for organizations seeking cost-effectiveness, scalability, and minimal maintenance. Public cloud providers manage infrastructure, allowing organizations to focus on business operations without extensive IT overhead. Private clouds require more internal management, hybrid clouds mix private and public elements but add complexity, and community clouds are shared among specific organizations with common concerns. 
	 
	 
	 Domain
4. Communication and Network Security
	  RF modulation refers to the process of converting digital data into analog signals that can be transmitted wirelessly over radio waves. This technique allows data to be encoded onto carrier frequencies and transmitted between wireless devices. While modulation supports reliable wireless communication, it does not replace encryption, nor does it directly enhance security. Its main function is ensuring the signal can be transmitted over a wireless medium effectively. The other choices misunderstand modulation‚Äôs role in physical security, signal range, and encryption.
	  
	  
	  Domain
1. Security and Risk Management
	  Audit logs and security control monitoring reports provide the most comprehensive and ongoing evidence of an organization‚Äôs adherence to security policies and controls. These artifacts show real-time compliance activities and can highlight deviations or trends. Risk assessments and incident reports are useful but offer more snapshot-like insights, while training records demonstrate only a specific part of the security program
	  
	  Domain
6. Security Assessment and Testing
	  Reviewing past incidents and response documentation provides insights into the effectiveness of the incident response program. It helps identify gaps, assess response times, and evaluate the organization's ability to contain and mitigate threats. Firewall configurations and patch management are important but do not directly measure incident response effectiveness. Employee interviews help gauge awareness but do not provide tangible evidence of response capabilities. 
	  
	  Domain
8. Software Development Security
	  Policy as Code (PaC) automates security policies by defining them in code, allowing security controls to be consistently applied across systems and environments. This approach ensures that security policies are integrated into the deployment process and enforced uniformly. While DevSecOps promotes security integration, PaC specifically addresses policy automation.
	  
	  Domain
2. Asset Security
	  The most important reason to maintain an asset inventory is to understand the security risks each asset poses to the organization. By having a clear view of all assets, security teams can identify vulnerabilities, ensure proper configuration, and apply appropriate security controls. While operational concerns like integration and end-of-life management are important, they are secondary to understanding the security implications of each asset.
	  
	  Domain
5. Identity and Access Management (IAM)
	  While ease and efficiency of identity management are benefits, they are not the most critical security considerations. More important are factors such as having reliable identification infrastructure and ensuring business continuity. Ease of access should not be prioritized over maintaining secure, consistent authentication processes and disaster recovery capabilities. 
	  
	  Domain
1. Security and Risk Management
	   The Process for Attack Simulation and Threat Analysis (PASTA) is a risk-based threat modeling framework that emphasizes analyzing security threats from an attacker's perspective. It incorporates business impact analysis and attack simulation to identify vulnerabilities. STRIDE is a Microsoft-developed methodology for classifying threats, DREAD is used to assess and rank threats based on risk factors, and OCTAVE is a risk management framework that focuses on organizational risk rather than attack simulation
	   
	   Domain
4. Communication and Network Security
	   Multi-mode fiber optic cables are ideal for short-distance, high-speed connections like those between devices in the same rack. They provide excellent bandwidth and lower latency than other options like coaxial and CAT 6 cables. Single-mode fiber is more suitable for longer distances, which is unnecessary in this case. 
	   
	   Domain
8. Software Development Security
	   The input validation procedures are the main concern because proper validation would prevent the malicious SQL queries (SQL Injection) from executing. This type of attack exploits a lack of validation to manipulate the database through the user interface. 
	   
	   Domain
3. Security Architecture and Engineering
	   SCADA systems require strong authentication to prevent unauthorized access. Default passwords are a major security risk and should always be changed. Network segmentation is important, but authentication controls remain the best frontline defense. While penetration testing helps assess vulnerabilities, implementing strong authentication immediately reduces the risk of intrusion and unauthorized control of industrial systems.
	   
	   Domain
6. Security Assessment and Testing
	   Digital signatures provide validation that both the auditor and the system owner have authenticated the contents of the audit report. This ensures that they agree on the accuracy of the report and that it has not been tampered with. While integrity is ensured by the signature, the primary function of the signature is validation, not confidentiality. 
	   
	   Domain
5. Identity and Access Management (IAM)
	    Transitive trust allows authentication from one domain to be implicitly extended to another trusted domain without requiring additional authentication. This can simplify access management but introduces risks if not properly controlled. Federated Identity Management (FIM) involves sharing authentication across organizations, while Single Sign-On (SSO) enables seamless access within the same environment. Identity Federation is similar to FIM but applies specifically to cross-organizational authentication. 
		
		Domain
1. Security and Risk Management
		OBJ. 1.12 - Regular communication, such as security tips and reminders, keeps security awareness active throughout the year. One-time annual training sessions are less effective because employees may forget critical information. Onboarding training is important but must be supplemented with ongoing education. Focusing only on privileged users ignores the risks posed by all employees, including those susceptible to phishing and social engineering attacks
		
		Domain
5. Identity and Access Management (IAM)
		OBJ. 5.3 - Security Assertion Markup Language (SAML) is a widely adopted standard for federating identity between different services and applications, enabling Single Sign-On (SSO) across disparate systems. SAML facilitates secure exchange of authentication and authorization data between identity providers and service providers. LDAP and X.500 are directory services for managing user data, while XACML focuses on authorization decisions, not identity federation.
		
		Domain
7. Security Operations
		OBJ. 7.3 - A well-defined configuration management strategy ensures that all components and technologies within the SaaS application are thoroughly documented and understood. This is essential for maintaining compliance with regulatory standards and for the proper maintenance of security controls over time. It also helps streamline updates and ensures future changes are managed securely without disrupting sensitive data. 
		
		Domain
7. Security Operations
		OBJ. 7.13 - The primary goal of BC exercises is to identify weaknesses in the business continuity plan (BCP) and improve response effectiveness before an actual disaster occurs. These exercises ensure that personnel understand their roles and that recovery procedures function as intended. Cybersecurity awareness training is important but not the main purpose of BC testing. While BC planning helps mitigate financial risks, testing is focused on operational readiness, not liability reduction.
		
		Domain
5. Identity and Access Management (IAM)
		OBJ. 5.4 - Role-Based Access Control (RBAC) assigns access rights based on user roles within the organization, which simplifies managing permissions and prevents privilege creep. By assigning access at the role level, it becomes easier to enforce least privilege and need-to-know. Rule-based and attribute-based models focus on different criteria for access, while mandatory access control is much stricter and often more complex. 
		
		Domain
6. Security Assessment and Testing
		OBJ. 6.2 - The key focus of API security is understanding how data is exchanged between systems, ensuring proper access controls, and mitigating data exposure risks. While misuse testing and compliance are important, they are secondary to ensuring secure data handling in this context. An SLA is focused on availability, not the security of data transfer. 
		
		Domain
5. Identity and Access Management (IAM)
		OBJ. 5.1 - Attribute-Based Access Control (ABAC) grants or denies access based on multiple attributes such as user role, device type, location, and time of access. This provides granular and dynamic control. DAC allows resource owners to set permissions, MAC enforces access based on classification levels, and RBAC assigns permissions based on predefined job roles. ABAC is particularly useful in dynamic cloud and enterprise environments where context-based access is required.
		
		Domain
8. Software Development Security
		OBJ. 8.3 - Peer code reviews help enforce secure coding standards by allowing multiple developers to analyze and validate code security. Coding style guidelines improve consistency but do not guarantee security. Security training is beneficial but does not assess adherence to secure coding in practice. Stress testing evaluates performance but not security. Regular peer reviews with a security focus provide continuous assessment of secure coding practices.
		
		Domain
3. Security Architecture and Engineering
		OBJ. 3.5 - Sandboxing isolates applications in a controlled environment, allowing testing without risking production systems. This is commonly used for malware analysis and secure software testing. Virtualization enables multiple operating systems to run on shared hardware but does not necessarily isolate risky applications. Network segmentation restricts traffic flow but does not provide application isolation, and compartmentalization applies access restrictions rather than execution isolation.
		
		Domain
3. Security Architecture and Engineering
		OBJ. 3.9 - A redundant power system with backup generators is the MOST critical control for business continuity, ensuring that operations can continue during power failures. Armed security guards deter intrusions but do not ensure uptime. Background checks improve personnel security but do not maintain system functionality. Visitor escort policies prevent unauthorized access but do not keep systems operational. Power redundancy is essential for ensuring continuous operation at a disaster recovery site.
		
		Domain
4. Communication and Network Security
		OBJ. 4.1 - VLANs are primarily used for network traffic segmentation, simplifying traffic management, and reducing broadcast domains. They do not reduce the need for web application firewalls, which are responsible for filtering and monitoring HTTP traffic between the web application and the Internet
		
		Domain
3. Security Architecture and Engineering
		OBJ. 3.10 - The Implementation/Assessment phase involves ensuring that security controls are in place and that the system meets all security requirements before being authorized for production use. This phase also addresses any long-term remediation necessary for maintaining ongoing compliance with security standards. 
		
		Domain
7. Security Operations
		OBJ. 7.7 - A Next-Generation Firewall (NGFW) offers deep packet inspection and advanced features such as intrusion prevention, application awareness, and the ability to inspect client-server communications. While circuit-level and stateful inspection firewalls examine connections and session states, NGFWs provide the highest level of security by inspecting data at multiple layers. Application-level firewalls focus on specific protocols but lack the advanced capabilities of NGFWs.
		
		Domain
7. Security Operations
		OBJ. 7.7 - The scenario described is characteristic of ransomware, which encrypts files and demands payment to restore access. A logic bomb triggers malicious actions under certain conditions, while a polymorphic virus mutates to avoid detection. Encrypted viruses also encrypt data, but ransomware specifically involves a ransom demand, which is the key distinction in this case.
		
		Domain
3. Security Architecture and Engineering
		OBJ. 3.1 - The Default Deny principle ensures that all actions or connections are explicitly denied unless specifically authorized, reducing unauthorized access risks. This aligns with fail-safe security, ensuring that access is restricted by default. Fail Secure means that if a system fails, it maintains security controls. Fail Open does the opposite, allowing access in case of failure. Implicit Allow means that anything not explicitly denied is permitted, which is a less secure approach.
		
		Domain
6. Security Assessment and Testing
		OBJ. 6.3 - Marketing software inventories are least likely to provide useful security process data because they are not directly related to the organization's security controls. Relevant sources of security process data include firewall logs, vulnerability reports, and SIEM alerts, as they help assess how well security measures are protecting critical systems and networks
		
		Domain
2. Asset Security
		OBJ. 2.3 - Implementing role-based access control (RBAC) and enforcing least privilege ensures that users have only the permissions necessary for their roles. Assigning default passwords creates a security risk, and granting administrative privileges to all users violates access control best practices. Using a secure self-service portal for password resets is preferable to manual IT requests, which may introduce inefficiencies and potential security risks
		
		Domain
4. Communication and Network Security
		OBJ. 4.1 - IPsec is the best solution for encrypting VPN traffic, providing end-to-end security at the network layer. It supports secure communications across public networks by authenticating and encrypting each packet. GRE and L2TP do not inherently provide encryption, and "VPN security" is too vague to specify the actual encryption method used
		
		Domain
5. Identity and Access Management (IAM)
		OBJ. 5.6 - The fingerprint represents something you are (biometric authentication), while the hardware token OTP represents something you have (a physical device for authentication). These two factors provide strong multifactor authentication (MFA). The other answers incorrectly identify "something you know," which is not present in this scenario. Using different authentication factors improves security by making credential compromise more difficult. 
		
		Domain
5. Identity and Access Management (IAM)
		OBJ. 5.6 - Multifactor authentication (MFA) requires authentication using two or more factors from different categories (e.g., something you know, something you have, or something you are). A password (knowledge) and a security token (possession) meet this requirement. Single-factor authentication (SFA) uses only one category. Biometrics alone do not constitute MFA. Challenge-response authentication verifies identity dynamically but is not necessarily MFA. 
		
		Domain
2. Asset Security
		OBJ. 2.3 - The first step in provisioning laptops securely is to implement full-disk encryption and endpoint security software to protect against data breaches and malware. Installing personal applications can introduce security risks. Disabling USB ports may be beneficial but should be assessed based on business needs. Allowing employees to set up their own administrative accounts weakens security controls by granting excessive privileges. 
		
		Domain
4. Communication and Network Security
		OBJ. 4.1 - Ad-hoc mode allows devices to communicate directly with each other without requiring a central access point, which is likely used in this BYOD scenario. Infrastructure mode requires an access point, bridge mode connects different networks, and stand-alone mode isn't a typical term for wireless networking
		
		Domain
8. Software Development Security
		OBJ. 8.2 - Restricting access so that developers can only interact with the code within the organization-owned repository prevents unauthorized distribution or interaction with proprietary code. Encryption and DLP provide security but do not control where the code can be accessed or modified. Digital signatures ensure integrity but do not protect confidentiality. 
		
		
		OBJ. 1.4 - Under HIPAA, Protected Health Information (PHI) includes all individually identifiable health information held or transmitted by a covered entity or its business associates in any form or medium, whether electronic, oral, or written. This comprehensive definition ensures that all forms of health data, if linked to a patient, are protected, not just those stored electronically or used in specific circumstances like billing. For support or reporting issues, include Question ID: 67d89e7a5c620278e445f676 in your ticket. Thank you.
Domain
1. Security and Risk Management

OBJ. 8.4 - Compliance with government security standards, such as FedRAMP, ensures that the cloud-based system meets stringent security controls. Stress testing evaluates performance but does not assess security. A user-friendly interface is beneficial but not a security measure. Technical support ensures availability but does not address potential security risks. Verifying compliance ensures the software has undergone security assessments and meets regulatory security requirements. For support or reporting issues, include Question ID: 67d969efea8a29ab42ad9794 in your ticket. Thank you.
Domain
8. Software Development Security

OBJ. 8.1 - Creating an integrated team of cross-functional members is a proven strategy to improve the efficiency of product development. This approach ensures collaboration between diverse roles such as developers, security experts, and business stakeholders, leading to better planning and reduced operational and security risks. Although other choices are useful, an integrated team approach offers the most holistic improvements to the development process. 
Domain
8. Software Development Security

OBJ. 8.1 - By integrating security assessments into the continuous development process, security becomes an inherent part of the development lifecycle, allowing for rapid and secure software delivery. This policy ensures that security is not bypassed due to time constraints. Automated vulnerability scanning is beneficial but may not cover all aspects of a comprehensive security assessment. For support or reporting issues, include Question ID: 67d969d2531aa8d1859b24ea in your ticket. Thank you.
Domain
8. Software Development Security

OBJ. 3.9 - Biometric access control prevents unauthorized entry by verifying identity, while redundant power supplies ensure continuous operation in case of power failure. Keypad locks are weaker than biometrics. Smoke detectors help with fire detection but do not actively suppress fires or prevent unauthorized access. Security guards and emergency lighting improve safety but do not provide comprehensive protection. Water-based fire suppression can damage electronic equipment. Biometrics and power redundancy provide the BEST balance of security and reliability. For support or reporting issues, include Question ID: 67d967ec1a98725b29776a91 in your ticket. Thank you.
Domain
3. Security Architecture and Engineering

OBJ. 4.3 - TLS encrypts communication between a web server and a client, protecting against MITM attacks, data tampering, and eavesdropping. TLS ensures both confidentiality and integrity by encrypting transmitted data and verifying the authenticity of the server through digital certificates. SMTP and FTP do not provide encryption by default, making emails and file transfers susceptible to interception. HTTP transmits data in plaintext, allowing attackers to capture and manipulate sensitive information, making it an insecure choice for web communication. For support or reporting issues, include Question ID: 67d9681eda28fa705a58216c in your ticket. Thank you.
Domain
4. Communication and Network Security

OBJ. 1.6 - A security policy is the most effective way to establish clear, enforceable rules prohibiting the use of personal cloud storage for sensitive data. Unlike guidelines, which only provide recommendations, policies define mandatory requirements. While standards and procedures are useful for technical implementation, they do not establish fundamental security requirements. A well-defined policy ensures employees understand the risks and compliance expectations regarding personal cloud storage use. For support or reporting issues, include Question ID: 67d89eb13285ff8c3c282061 in your ticket. Thank you.
Domain
1. Security and Risk Management


OBJ. 1.4 - The Wassenaar Arrangement regulates the export of dual-use technologies, including strong cryptographic software (e.g., AES-256, RSA-4096). Governments restrict exports to prevent adversaries from obtaining advanced encryption tools. Symmetric encryption under 128 bits is typically not restricted due to its weaker security. Open-source cryptographic tools and hashing algorithms are generally not subject to Wassenaar controls, as they do not provide encryption in transit or storage. For support or reporting issues, include Question ID: 67d89e7a70492f270af47f94 in your ticket. Thank you.
Domain
1. Security and Risk Management

OBJ. 3.5 - In this scenario, the production environment is hosted on a public cloud, as it is made available over the internet, while the shared development environment reflects a community cloud model, where resources are shared between multiple organizations with common concerns (Alpha and Beta). This provides flexibility while maintaining cost efficiency. For support or reporting issues, include Question ID: 67d967bde48d64ad351067a6 in your ticket. Thank you.
Domain
3. Security Architecture and Engineering

OBJ. 4.2 - A Virtual Private Network (VPN) provides secure, encrypted communication over an untrusted network, such as the Internet, allowing remote users to securely access corporate resources. SFTP secures file transfers but does not provide full remote access. NAC enforces security policies for network access but does not encrypt connections. TLS is a cryptographic protocol used for secure communication but does not inherently provide remote access like a VPN does. For support or reporting issues, include Question ID: 67d96813925da16140f2c7b7 in your ticket. Thank you.
Domain
4. Communication and Network Security


OBJ. 2.4 - Business managers are typically responsible for data categorization because they best understand the business context and value of the data. They determine the classification level based on the sensitivity and impact of the data on business operations. Security and IT managers may assist, but business managers have primary responsibility for the data's significance and classification. For support or reporting issues, include Question ID: 67d962b3cb7f6258bfd9efff in your ticket. Thank you.
Domain
2. Asset Security

OBJ. 4.3 - The RADIUS server is responsible for managing authentication, authorization, and accounting for VPN users. When testing VPN security, user authentication data, including credentials and session records, are stored and managed by the RADIUS server. The Encapsulated Security Payload and Authentication Header are part of the VPN's encryption protocols, while the Certificate Authority issues certificates but does not handle user account authentication directly. For support or reporting issues, include Question ID: 67d9681ef7cadb94775b41cc in your ticket. Thank you.
Domain
4. Communication and Network Security

BJ. 7.12 - The primary goal of DR testing is to validate recovery procedures and identify weaknesses in the plan. Testing helps organizations refine processes, ensure compliance, and improve response times. While emergency evacuation procedures, financial impact assessments, and physical security improvements are important, they are not the main focus of DR testing. A structured test ensures that critical systems and operations can be restored effectively in the event of a disaster. For support or reporting issues, include Question ID: 67d969a898241465d0be20cf in your ticket. Thank you.
Domain
7. Security Operations

OBJ. 4.3 - IPSec ensures confidentiality, authentication, and integrity for site-to-site VPNs by encrypting entire IP packets, protecting data from eavesdropping and tampering. It operates in tunnel or transport mode, securing communication between networks over untrusted links. TLS and SSL encrypt web sessions but are not designed for full network tunneling. HTTPS secures web traffic at the application layer but does not encrypt all network communication like IPSec, making it unsuitable for VPNs requiring end-to-end encryption across multiple protocols. For support or reporting issues, include Question ID: 67d9681e3d65814ff2a11d56 in your ticket. Thank you.
Domain
4. Communication and Network Security

OBJ. 5.4 - Authorization is the process of granting specific permissions to users, allowing them to access resources like personally identifiable information (PII). In this scenario, after authentication verifies the manager‚Äôs identity, authorization determines what data the manager can access. Least privilege is a security principle that restricts access, while access controls are broader mechanisms governing access, and authentication verifies identity but does not grant access to specific data. For support or reporting issues, include Question ID: 67d9685f3d65814ff2a11d65 in your ticket. Thank you.
Domain
5. Identity and Access Management (IAM)

OBJ. 6.2 - Statement coverage ensures that every line of code is executed at least once during testing, making it a comprehensive metric for security testing. This helps identify logic errors, missing statements, or potential security flaws that might go unnoticed. Function coverage ensures each function is called, but it does not guarantee full statement execution. Condition and path coverage focus on decision points, making them useful but not as exhaustive for full statement execution. For support or reporting issues, include Question ID: 67d968f97b2bc17cb0ad4679 in your ticket. Thank you.
Domain
6. Security Assessment and Testing

OBJ. 5.1 - Access control lists (ACLs) on routers are designed to restrict logical network access, such as controlling traffic flow and permissions between systems, but they do not address physical security. Smart cards, CCTV, and biometric scanners are widely used physical security controls to limit access to secure areas like server rooms. For support or reporting issues, include Question ID: 67d9683e987330b81ed2562f in your ticket. Thank you.
Domain
5. Identity and Access Management (IAM)

OBJ. 8.3 - Adversarial machine learning testing helps assess an AI model's resilience to attacks that attempt to manipulate its decision-making process. Validating accuracy is important but does not assess security effectiveness. Comparing AI with rule-based systems evaluates performance rather than security. Stress testing measures performance but not security vulnerabilities. Ensuring resistance to adversarial attacks is the best way to evaluate the security of an AI-based fraud detection system. For support or reporting issues, include Question ID: 67d969e67b2bc17cb0ad4688 in your ticket. Thank you.
Domain
8. Software Development Security

OBJ. 7.8 - Effective patch management is a critical component of any vulnerability management program, as it directly addresses known vulnerabilities by applying fixes in a timely manner. Without proper patch management, vulnerabilities remain exploitable despite other security measures. While defense-in-depth strategies, continuous monitoring, and advanced threat intelligence are essential, the primary focus should be on ensuring that known vulnerabilities are patched quickly to minimize risk. This foundational process helps prevent exploitation from already identified threats. For support or reporting issues, include Question ID: 67d969816958032c658b314c in your ticket. Thank you.
Domain
7. Security Operations

OBJ. 3.6 - Advanced Encryption Standard (AES) is a robust encryption method well-suited for protecting data at rest, while Transport Layer Security (TLS) is designed for securing data in transit. This combination ensures that trade secret information is encrypted and protected against unauthorized access, whether stored or transmitted over the network. For support or reporting issues, include Question ID: 67d967cc1a98725b29776a82 in your ticket. Thank you.
Domain
3. Security Architecture and Engineering

OBJ. 6.5 - Implementing a log retention policy that meets regulatory requirements ensures compliance and supports forensic investigations. Deleting old logs may violate compliance regulations. Increasing storage capacity without addressing retention policies does not solve the issue. Ignoring the finding could result in legal penalties and hinder future investigations. A properly defined retention policy ensures security logs are available for audits and incident response. For support or reporting issues, include Question ID: 67d9692c16b973cef867fcfc in your ticket. Thank you.
Domain
6. Security Assessment and Testing

OBJ. 3.6 - Collision resistance ensures that no two different inputs generate the same hash value, preventing data tampering. Hash functions like SHA-256 are designed to withstand collision attacks, making them suitable for digital signatures and data integrity checks. Low computational cost is a factor but not the primary concern. Hash functions are one-way operations and do not support decryption, so speed is irrelevant to decryption. For support or reporting issues, include Question ID: 67d967cc987330b81ed25611 in your ticket. Thank you.
Domain
3. Security Architecture and Engineering

OBJ. 5.3 - SAML is best suited for enterprise Single Sign-On (SSO), enabling users to log in once and access multiple applications using corporate identity providers (e.g., Active Directory Federation Services). OpenID Connect (A) is optimized for consumer-facing web apps rather than enterprise environments. OAuth 2.0 (B) is an authorization framework, not an authentication protocol. Kerberos (D) is used for authentication within internal enterprise networks, not for cloud-based SSO. For support or reporting issues, include Question ID: 67d96855c59c26a80470d58a in your ticket. Thank you.
Domain
5. Identity and Access Management (IAM)

OBJ. 6.3 - Misuse case testing involves simulating malicious or unexpected actions to determine how an API or system responds. By attempting to bypass the API's authentication mechanisms, Karmen is testing a potential misuse case scenario to identify vulnerabilities in the system‚Äôs security. Fuzz testing focuses on random inputs, and sanity tests are for basic functionality checks, making misuse case testing the most appropriate method. For support or reporting issues, include Question ID: 67d9691357ec55fc36ab693d in your ticket. Thank you.
Domain
6. Security Assessment and Testing

OBJ. 5.3 - Federated Identity Management (FIM) enables Single Sign-On (SSO) across multiple systems, reducing password fatigue and simplifying user authentication. FIM helps improve security but does not enforce password complexity rules or directly prevent insider threats. While it limits unauthorized access, its primary advantage is minimizing the number of passwords users must remember, thereby reducing risks associated with password reuse. For support or reporting issues, include Question ID: 67d96855a2376fbaf1927708 in your ticket. Thank you.
Domain
5. Identity and Access Management (IAM)

OBJ. 3.5 - SCADA systems are designed for supervisory control and data acquisition, typically used in industries like shipping for managing large, distributed equipment. The system mentioned here likely refers to SCADA, as it manages both terminal units and machine interfaces across a network. Distributed control systems (DCS) and distributed systems may involve control but are not specifically designed for supervisory control and remote monitoring. An open system refers to interoperability with other systems, which doesn't fit the context. For support or reporting issues, include Question ID: 67d967b998241465d0be2098 in your ticket. Thank you.
Domain
3. Security Architecture and Engineering

OBJ. 7.10 - While perimeter security is important, in an emergency scenario, other measures like team training and clear contact information take precedence. Ensuring team readiness and communication pathways are more critical for an effective disaster response than perimeter control during an emergency, as entry and exit might need to be flexible. For support or reporting issues, include Question ID: 67d9699516b973cef867fd24 in your ticket. Thank you.
Domain
7. Security Operations

OBJ. 8.1 - Integrated Product Teams (IPTs) foster collaboration by bringing together cross-functional members, which leads to improved problem-solving and decision-making during the software development process. This approach enhances efficiency and communication, making it easier to identify and resolve issues early. The other options represent negative impacts that are not typical of an IPT. For support or reporting issues, include Question ID: 67d969d27e175d348d2cc8c6 in your ticket. Thank you.
Domain
8. Software Development Security



COMPLEX CISSP practice questions #1: All CISSP domains 125Q
The Correct Answer: Upgrade the web application firewall to mitigate the identified vulnerability, ensuring that no unauthorized access occurs: Addressing the critical vulnerability in the WAF is essential to safeguarding sensitive customer data against immediate threats. 

As the risk of unauthorized access poses a direct impact on compliance and data protection, this action not only enhances security but also aligns with regulatory requirements for protecting customer data. 

The Incorrect Answers: Implement an organization-wide security awareness training program focused on the latest phishing techniques for all employees: While security awareness training is crucial for enhancing overall organizational security culture, it does not address the immediate technical vulnerability in the WAF. Training cannot prevent potential breaches stemming from system flaws that need urgent attention. Conduct a thorough risk assessment to identify all other potential vulnerabilities beyond the web application firewall before making any changes: Although a risk assessment is a fundamental aspect of security management, delaying remediation of the WAF vulnerability while performing a broad assessment could expose the organization to significant risk. Immediate action is warranted here. Revise the encryption protocols across all data at rest and in transit to meet the latest industry standards before addressing other vulnerabilities: Updating encryption protocols is an important security measure; however, it does not resolve the current and critical risk posed by the WAF vulnerability. This action might consume valuable time that could be better spent patching the most pressing security issue.
Domain
Domain 3: Security Architecture and Engineering


Overall explanation
The Correct Answer: Conduct a risk assessment of the vendor‚Äôs previous incidents and present the findings to senior management to reassess the procurement decision: 
By conducting a thorough risk assessment of the vendor‚Äôs past incidents, the CRO can provide senior management with evidence-based insights, allowing them to make an informed decision regarding procurement. 
This approach addresses both the concerns of management and the agency's need for accountable risk management. 

The agency risks falling behind on its procurement timeline without assessing the vendor‚Äôs past incidents. Pause the procurement process until more detailed information is gathered regarding the vendor's security incidents and risk management policies: Temporarily halting the procurement process may hinder operational progress and could lead to missed deadlines. Instead, the CRO should conduct a proactive risk assessment to inform the decision-making process without unnecessary delays.

The Incorrect Answers: Engage an external auditing firm to independently evaluate the vendor‚Äôs security posture and provide recommendations to management: While obtaining an external evaluation could be useful, it may unnecessarily delay the procurement process. The CRO should prioritize internal assessment of the vendor's incidents before seeking external validation, ensuring timely and relevant decision-making. Arrange a series of internal meetings to clarify internal policy documentation before making any decisions on the vendor‚Äôs procurement: Focusing on internal clarity regarding policies does not directly address the immediate concern surrounding the vendor's security history. 

Domain
Domain 1: Security and Risk Management

Overall explanation
The Correct Answer: Conduct a comprehensive risk assessment of all third-party vendors to identify and prioritize those posing the highest risk to PHI: This approach aligns with the emphasis on risk management and assessment. By conducting a thorough risk assessment, the organization gains a detailed understanding of each vendor's risk level regarding PHI, enabling strategic prioritization of actions to address the most significant threats. This step ensures compliance with HIPAA by identifying inadequacies in vendor practices and is foundational in establishing robust third-party risk management. The Incorrect Answers: Immediately terminate contracts with vendors who have a history of security incidents, regardless of their current performance: Though revoking contracts might seem like a decisive action, it is overly reactive and does not account for vendors' current security posture or efforts toward improvement. Termination without comprehensive evaluation could lead to business disruption and might not be compliant with contractual obligations. Implement a continuous monitoring system to audit vendor security practices and compliance with HIPAA on an ongoing basis: While proactive monitoring is important, initiating it without first understanding the risk landscape through an assessment might be premature. This option should follow the risk assessment to ensure monitoring resources are allocated effectively towards high-risk vendors. Revise contracts with all vendors to explicitly prohibit unauthorized access to applications and impose strict penalties for non-compliance: Revising contracts is a necessary step for compliance and setting clear expectations, but it doesn't address the initial need to understand and evaluate current risks. Without a risk assessment, contract revisions might not adequately address the most critical areas of concern or prioritize the right vendors for compliance enhancements.
Domain
Domain 7: Security Operations


Overall explanation
The Correct Answer: Conduct a cost-benefit analysis to determine the potential risks of maintaining legacy systems against the need for disruption-free patient care, using this data to drive decisions: This option allows for a strategic approach that balances the need for security with operational continuity. By conducting a cost-benefit analysis, the CISO can assess risks associated with legacy systems and weigh them against the critical requirement of uninterrupted patient care. This method aligns with principles of risk management and governance, helping form a data-driven decision-making process that aligns with the organization's risk tolerance and business objectives. It also considers compliance and the practical constraints of the healthcare environment. The Incorrect Answers: Create a phased retirement plan for legacy systems, allowing for gradual migration to updated systems while instituting temporary compensating controls to maintain security: While this approach offers a structured transition, it lacks immediate action on assessing risks and benefits objectively. Without first understanding the impact through a cost-benefit analysis, the phased approach might overlook critical insights regarding patient care disruption or compliance challenges. Mandate immediate replacement of all legacy systems with new, zero-trust-compliant technologies to ensure full compliance and security alignment: This aggressive strategy prioritizes compliance over operational effectiveness, posing significant risks to patient care continuity. It fails to evaluate the organization's capacity to absorb such changes without undue disruption and lacks consideration of potential resource limitations or unforeseen operational impacts. Engage with your healthcare regulatory body to seek exceptions regarding the use of legacy systems until the complete implementation of zero trust is feasible without disrupting patient services: While engaging regulators can be part of the overall strategy, this option doesn't address how to balance security and operational needs internally. Seeking exceptions may not offer a long-term solution and could potentially delay necessary security enhancements without focusing on assessing and mitigating risks.
Domain
Domain 3: Security Architecture and Engineering


Overall explanation
The Correct Answer: Prioritize the implementation of a secure data repository system that integrates with existing workflows while phasing out unauthorized applications: Implementing a secure data repository ensures data integrity and compliance with NIH regulations while addressing the immediate security lapses related to unauthorized application usage. 

It provides a structured solution that maintains organizational reputation and meets stakeholder demands without delay. 

The Incorrect Answers: Implement a policy that restricts the use of unauthorized applications, ensuring a formal training program on compliant data handling for all employees: Establishing a policy is essential, but it does not address the immediate risk posed by unauthorized application usage nor ensure compliance quickly enough given the aggressive timeline for reporting to stakeholders. 

Conduct a comprehensive investigation into current data access practices, identifying potential data leaks, and develop an immediate remediation plan: While understanding data access practices is critical, this option might delay decisive action. Immediate implementation of a secure repository provides a faster resolution to ensure compliance and data integrity. Engage with stakeholders to communicate the findings, including existing gaps in security and immediate steps being taken to safeguard data integrity: 

Open communication with stakeholders is important, but without concrete actions being taken to remediate the issue, this option would likely undermine their confidence in the firm‚Äôs capabilities to safeguard sensitive research data.

Domain
Domain 3: Security Architecture and Engineering


Overall explanation
The Correct Answer: Seek interim measures such as enhanced monitoring and access controls for legacy systems while gradually planning for their upgrade to meet HITRUST standards: This approach allows the organization to maintain compliance and secure sensitive data without causing immediate disruptions to services. By enhancing controls on legacy systems while planning for upgrades, the institution can balance risk management with operational integrity. The Incorrect Answers: Disconnect legacy systems from the network entirely until they are upgraded to ensure no sensitive information can be compromised, even temporarily: While disconnection may seem secure, it can cripple operational capabilities and disrupt services, potentially leading to greater adverse impacts. It is crucial to manage risk while keeping essential services operational. Implement HITRUST controls strictly on newer systems only, arguing that they will serve the majority of the organization‚Äôs compliance needs: This strategy overlooks the necessity of securing legacy systems, which still contain sensitive data and represent a considerable risk to compliance. Focusing solely on newer systems fails to resolve the compliance gaps related to legacy data. Communicate findings to upper management, suggesting that HITRUST certification may need to be postponed until the legacy systems can be fully remediated: This option lacks proactivity and does not propose any interim measures to mitigate risks. Postponing certification could damage the organization's reputation and operational capabilities in the interim.
Domain
Domain 1: Security and Risk Management

Overall explanation
The Correct Answer: Develop a detailed post-incident review process that includes input from all stakeholders to refine incident management practices: This option addresses both the immediate need for improvement in the incident response strategy and ensures that the organization is learning from past incidents, which is a critical aspect of both effective incident management and regulatory compliance. A post-incident review fosters continuous improvement and identifies areas for enhancement, aligning with compliance requirements to demonstrate due diligence. The Incorrect Answers: Revise the communication protocol to clarify notification processes for internal and external stakeholders during an incident: While improving the communication protocol is important, solely focusing on communication does not address the need for a comprehensive review and refinement of the entire incident response strategy. It risks being a superficial fix without ensuring overall effectiveness and improvement. Conduct a thorough training program for all staff involved in the incident response to ensure they understand their roles and responsibilities: Training is essential; however, without first refining and updating the incident response plan itself, the training may be ineffective or based on outdated procedures. It is more efficient to first ensure that the plan incorporates current best practices. Create a comprehensive audit trail of all incidents to satisfy compliance requirements, even if it means delaying other enhancements to the incident response plan: While maintaining an audit trail is important for compliance, prioritizing this over holistic improvements to the incident response strategy could lead to continued vulnerabilities. Compliance should not be achieved at the cost of operational effectiveness, as the overarching strategy should be to prevent incidents and respond effectively when they occur.
Domain
Domain 7: Security Operations

Overall explanation
The Correct Answer: Implement a dedicated incident communication team to manage all internal and external communications during a breach, ensuring consistency and clarity: Establishing a dedicated incident communication team directly addresses the issue of fragmented and unclear communication. This team would specialize in coordinated and clear communication, maintaining consistency in messaging across all stakeholders, including customers and regulatory bodies. This prioritizes stakeholder engagement and compliance, aligning with strategic decision-making essential for a CISO's role. The Incorrect Answers: Develop a streamlined communication protocol that specifies key messaging points for different stakeholders, including both internal and external parties: While important, merely developing a protocol does not ensure the capability to execute it effectively during an incident. A dedicated communication team would better manage real-time communications. Conduct regular tabletop exercises focused on incident response scenarios to evaluate the effectiveness of existing protocols and improve team coordination: Tabletop exercises are beneficial for testing protocols, but they do not directly address the immediate need for enhanced communication strategies, which was identified as a significant issue in the post-incident review. Review and update the incident response plan to incorporate lessons learned from the data breach and explicitly address communication strategies: Updating the incident response plan is necessary for continual improvement, but without a concrete mechanism like a dedicated team to ensure effective communication, similar issues could still arise. A dedicated team deals directly with the recurring communication problem more effectively.
Domain
Domain 7: Security Operations

$fp?)(kw]4U@8[QG9:
$fp?(kw]4U@dR8[Q

https://upbillpayportal.aciondemand.com/Login.aspx?BrandingID=582580&kp_shortcut_referrer=kp.org/payonline

ReM69pkFQSLZ

a2026ReM@69pkFZ=

2026ReM69pkFQSLZ+
a2026ReM69pkFZ+
Gxr^(4sR83R@+
4cGr^4sR83R@txQ78


Overall explanation
The Correct Answer: Conduct a gap analysis to assess current policies against local cybersecurity regulations and develop a tailored compliance framework to ensure adherence: This is the best action to prioritize as it aligns with the need for compliance with varying local regulations, which is critical given the restrictions on data transfer and storage. Conducting a gap analysis will help identify discrepancies between current practices and local requirements. This proactive approach addresses the root cause of potential regulatory non-compliance, supporting the organization's risk management objectives and maintaining a robust security posture essential for the new office setup. Additionally, this is the only option that covers compliance, log consolidation and enhanced threat detection. The Incorrect Answers: Implement the SIEM solution immediately to begin monitoring and correlating logs from existing systems, delaying compliance measures until the security posture is improved: This option is less suitable because it prioritizes technical improvement over critical compliance work. While SIEM implementation can enhance threat detection, ignoring compliance issues could lead to significant legal and financial repercussions, especially given the local restrictions on data handling. 

Contact legal counsel to draft an updated Memorandum of Agreement (MOA) with local authorities, ensuring clear communication about data handling practices and compliance with regulations: Although involving legal counsel and negotiating with local authorities is important, this action alone does not directly address the critical task of analyzing current policies and ensuring comprehensive compliance with local regulations. 

It may be part of a broader compliance strategy but is not the first priority. Initiate a comprehensive employee training program focusing on data protection, security incident response, and regulatory awareness to address the recent breach and improve the overall security culture: While training is beneficial and necessary for improving security culture, it does not address immediate compliance gaps or the specific regulatory challenges posed by the new office location. This should be part of an ongoing strategy following the establishment of a compliance framework.
Domain
Domain 7: Security Operations

Overall explanation
The Correct Answer: Form a cross-functional team to analyze the current integration plan, balancing compliance needs with timelines to discover a way to expedite the secure deployment of services: This option effectively addresses the dual requirement of mitigating regulatory risks and facilitating business objectives. Involving a cross-functional team ensures that diverse perspectives are considered, including compliance, risk management, technical, and operational insights, allowing for a balanced and strategic approach. By analyzing the integration plan with a keen focus on compliance and timeline alignment, the organization can prioritize necessary security enhancements while facilitating a smoother transition. This exemplifies an approach that aligns with governance, stakeholder engagement, and strategic risk management. The Incorrect Answers: Conduct a third-party security assessment of the fintech startup's technology to understand vulnerabilities before any integration takes place: While conducting a security assessment is essential for identifying vulnerabilities, it may not sufficiently address the immediate need to balance compliance and business timelines. This action, while valuable, focuses more on discovery rather than proactive governance and does not directly facilitate integration and compliance alignment. Implement an immediate overhaul of the startup‚Äôs authentication systems, ensuring industry-standard protocols are in place before the integrated technology goes live: Although this option prioritizes security upgrades, it may delay the integration process due to the time needed for implementation. It focuses heavily on technical remediation without addressing broader compliance and operational goals, which are crucial given the regulatory implications and integration urgency. Proceed with the integration as planned, while documenting identified risks and planning to remediate them after deployment to avoid operational delays: This approach underestimates the significance of addressing compliance issues upfront. Waiting to remediate risks after deployment can exacerbate regulatory vulnerabilities and legal consequences, which is particularly risky in light of the organization's prior penalties and strict security alignment policy. Prioritizing post-integration fixes contradicts a preventative strategy and governance-focused decision-making.
Domain
Domain 5: Identity and Access Management (IAM)


Overall explanation
The Correct Answer: Postpone integration until comprehensive vendor documentation is obtained, and all devices have received necessary security updates and usability evaluations: This option ensures compliance with healthcare regulations and prioritizes the security of sensitive patient data. By delaying integration, the CISO can guarantee that all devices meet necessary security standards and that stakeholders have confidence in their usability before use. This thorough approach mitigates risk effectively. The Incorrect Answers: Conduct a vendor assessment to compare the security features of the medical devices and implement the required firmware updates before integrating them into the network: While evaluating vendor security features is important, this answer does not address the immediate risk of deploying equipment that potentially lacks adequate documentation or updates. It risks deploying devices without sufficient background checks for compliance. Mandate that all medical device users go through training to become familiar with the usability of devices, assuming that usability issues will diminish with training: This approach neglects the core issue of device security and compliance, creating potential vulnerabilities in patient data security. Training on usability alone does not resolve underlying systemic risks associated with integrating unverified devices. Deploy the integration of devices that come from reputable vendors, while prioritizing usability for medical staff over stringent security documentation from less-known suppliers: This option compromises patient data protection by implying that user experience is more important than security compliance. Even reputable vendors may have overlooked significant security issues, and the CISO must ensure adequate documentation and updates prior to deployment.
Domain
Domain 3: Security Architecture and Engineering


Overall explanation
The Correct Answer: Create and enforce a robust access control policy across the organization to regulate who can access, modify, and share patient data with external applications: This option focuses on governance and policy, which is crucial for ensuring compliance with HIPAA regulations. Access control is fundamental in safeguarding patient data, ensuring that only authorized personnel can view or manipulate sensitive information. 

By implementing a strong access control policy, the organization addresses both privacy and data protection requirements, aligning with HIPAA's emphasis on protecting patient information while maintaining operational readiness. The Incorrect Answers: Implement strict encryption for all data transmitted between the EHR system and external applications to secure patient information and maintain HIPAA compliance: 

While encryption is essential for protecting data in transit, it does not address all aspects of HIPAA compliance, such as access control and auditing. Relying solely on encryption might overlook internal threats or misuse of access, which are covered more effectively by robust access control measures. Establish a comprehensive data loss prevention (DLP) program to monitor, classify, and restrict the movement of patient data across systems and applications: A DLP program can help manage data movement and prevent leaks. However, it's a more reactive measure compared to access control. 

It focuses on data prevention rather than controlling who accesses or modifies the data, which is more directly related to HIPAA compliance. Conduct regular security assessments and audits to identify vulnerabilities in the EHR system and develop remediation strategies as the transition progresses: Regular audits are indeed important for maintaining system security and compliance. However, without strong operational access controls in place, audits alone may not prevent unauthorized data access or sharing. Regular assessments should complement an access control strategy rather than replace it, ensuring vulnerabilities are identified and addressed within a controlled access framework.
Domain
Domain 3: Security Architecture and Engineering


Overall explanation
The Correct Answer: Engage a forensic investigation team to conduct a thorough analysis of the breach and identify the root cause, while temporary measures to prevent further incidents are implemented: This approach prioritizes immediate risk mitigation by seeking to understand how the breach occurred, which is essential for compliance with PCI DSS and for formulating a long-term remediation strategy. 
Understanding the breach is crucial to ensure that any implemented fixes will be effective and that compliance requirements are thoroughly addressed. 

The Incorrect Answers: 

Notify affected customers to disclose the breach details, offering credit monitoring services to mitigate reputational damage and meet potential legal obligations: While customer notification is important, addressing the breach's root cause and preventing further incidents should be prioritized first in order to ensure a full understanding of the situation before communicating with affected customers. Premature notification without proper context could lead to further reputational harm. 

Implement a temporary ban on all online transactions until the root causes of the breach are identified and resolved, ensuring that no further data loss occurs: While this may prevent additional breaches, it could severely impact the company‚Äôs revenue during a critical shopping time. It may not be the most effective initial step, given that identifying and addressing the root cause can lead to more targeted and less disruptive responses. 

Revise the current security policies to prohibit third-party vendors from accessing customer payment information until all vulnerabilities are remedied and the environment is thoroughly assessed: This option reacts to the breach but does not focus on immediate containment or investigation. Denying vendor access could limit business operations and not immediately alleviate the risks posed by the breach itself. A better strategy would involve first understanding the breach rather than cutting off potentially essential vendor relationships.
Domain
Domain 7: Security Operations

Overall explanation
The Correct Answer: Conduct a comprehensive risk assessment to identify the extent of the non-compliance before determining the next steps: Performing a risk assessment is crucial to understand the impact and scope of the non-compliance issue. This analysis guides the appropriate remediation strategy, ensuring that business continuity is upheld while addressing legal obligations effectively. The Incorrect Answers: Initiate an immediate data encryption project to secure all sensitive health information stored by the start-up, regardless of the associated costs and potential delays: While encryption is vital for protecting sensitive data, rushing this project without understanding the full scope of the non-compliance may lead to oversight of other critical factors, including potential impacts on business operations and relationships. Engage legal counsel to explore the possibility of salvaging customer relationships while notifying affected clients of the compliance breaches: This option focuses too heavily on public relations rather than immediately addressing the compliance issue. The CISO must first understand the risks before effectively communicating with stakeholders, as premature notification could escalate reputational damage. Terminate the start-up's data analytics operations until the entire infrastructure complies with legal and regulatory standards: Halting operations could lead to significant business disruptions and loss of client trust. It is essential to evaluate the current state of compliance and address specific shortcomings without entirely abandoning operational capabilities.
Domain
Domain 1: Security and Risk Management

Overall explanation
The Correct Answer: Implement a mandatory training session focused on the importance of the clean desk policy and data protection practices: This option is the best initial action because it directly addresses the root cause of policy non-compliance: a lack of understanding or awareness among employees. Through training, employees will better understand the importance of the policy and the associated regulatory requirements, fostering a culture of compliance and responsibility. This proactive measure not only mitigates immediate risks but also supports long-term adherence to security practices, linking governance with strategic security awareness. The Incorrect Answers: Install comprehensive surveillance cameras in all workspaces to observe compliance with the clean desk policy and secure device usage: While surveillance could assist in policy enforcement, it is reactive and may not address the underlying issue of employee understanding and engagement. This approach can also cause privacy concerns and degrade trust within the organization without ensuring regulatory compliance or addressing the root cause of the problem. Conduct a thorough risk assessment to identify specific vulnerabilities related to the clean desk policy and devise tailored solutions: Although conducting a risk assessment is an important step in understanding vulnerabilities, it doesn't provide immediate alignment with compliance and regulatory requirements. Additionally, it may delay the necessary actions to enhance awareness and correct current non-compliant behavior, making it less effective as an initial step. Formulate and communicate strict disciplinary procedures for violations of the clean desk policy, including immediate termination for the most serious breaches: Strict disciplinary actions could lead to fear-based compliance rather than genuine understanding and commitment. It does not proactively encourage correct behavior or ensure employees understand why policies are important, failing to address the necessity for training and awareness to achieve compliance and risk mitigation.
Domain
Domain 1: Security and Risk Management

Overall explanation
The Correct Answer: Hold a workshop with employees from various roles to collaboratively develop revised AUP guidelines that address essential security practices while considering employee feedback: This approach fosters collaboration, inclusivity, and shared ownership of the policy. By actively involving employees in the development process, the CISO can ensure that the resulting AUP reflects real-world use cases, addresses employee concerns, and enhances compliance without fostering resentment. The Incorrect Answers: Conduct a review of existing usage patterns using logs and reports to identify the most frequently accessed unauthorized applications and then create bans: This action may identify common issues, but solely relying on bans without employee involvement can lead to resistance and frustration. It risks creating a culture of punitive measures rather than encouraging constructive dialogue about acceptable use. Increase the penalties for violations of the existing AUP to demonstrate the seriousness of compliance with organizational policies: While establishing penalties is important, emphasizing punishment can breed distrust and resentment among employees. It does not address the underlying issue of unclear guidelines nor encourages responsible behavior, which is vital for effective security. Develop a simplified, easily digestible version of the AUP highlighting key security practices without detailing unacceptable behaviors: Although simplification can aid understanding, omitting details about unacceptable behaviors could lead to further ambiguity and misuse. Ensuring clarity on what is prohibited is essential for effective compliance and security enhancement, necessitating a more comprehensive reform rather than partial simplification.
Domain
Domain 1: Security and Risk Management


Overall explanation
The Correct Answer: Engage stakeholders from both IT and clinical teams to collaboratively develop a timeline that accommodates both the EHR launch and annual regulatory audits, ensuring transparency in all processes: This option is the best choice because it addresses both the collision of the implementation timeline with regulatory audits and the interoperability issues. By involving stakeholders from both IT and clinical teams, the CIO fosters collaboration and ensures that all perspectives are considered (Guideline 4: Stakeholder Engagement). 

Developing a mutually agreeable timeline enhances transparency and aligns the project with compliance requirements (Guideline 3: Compliance and Legal Obligations). This approach balances operational efficiency with security and compliance needs (Guideline 9: Balanced Approach), and proactively manages risks associated with the migration and audit schedule. 

The Incorrect Answers: 
Conduct a comprehensive data integrity assessment that maps data flow between legacy systems and the new EHR, ensuring that critical data remains accessible during migration: 
While important for ensuring data integrity, this action focuses solely on technical aspects without addressing the immediate compliance issue of conflicting timelines (Guideline 2: Prioritization). It does not involve stakeholder collaboration or adjust the implementation schedule to mitigate risks associated with the upcoming regulatory audits. 

Pause the implementation of the new EHR system until all interoperability issues are resolved, thereby ensuring a smooth transition and compliance with regulatory requirements: 
Halting the implementation could severely hinder productivity and delay critical improvements (Guideline 9: Balanced Approach). It is a reactive measure that might not be necessary if risks can be managed through better planning and stakeholder engagement. Additionally, it does not leverage proactive strategies to address both interoperability and compliance concerns simultaneously. 

Prioritize the development of a fallback plan that ensures patient care continues seamlessly despite potential disruptions during the data migration process, focusing on maintaining service quality throughout transitions: Developing a fallback plan is reactive and addresses the symptoms rather than the root cause of the issues (Guideline 5: Preventative Measures). While maintaining patient care is essential, this option does not proactively resolve the interoperability gaps or the scheduling conflict with regulatory audits. It overlooks the importance of stakeholder collaboration to prevent disruptions in the first place."
Domain
Domain 2: Asset Security

10-8

Overall explanation
The Correct Answer: Delay the LMS launch until all identified security vulnerabilities are remediated, ensuring compliance and maintaining trust with students and staff: 
This decision prioritizes the security of sensitive student data and aligns with regulatory compliance obligations. Ignoring significant security gaps for the sake of deadlines could expose the institution to severe data breaches and reputational damage. 

The Incorrect Answers: 
Authorize the launch of the LMS, assuring stakeholders that any security gaps will be addressed in a subsequent update while meeting the deployment deadline: This option disregards the risks associated with known vulnerabilities and could lead to major data breaches, undermining stakeholder trust and exposing the institution to regulatory penalties. 

Conduct a stakeholder meeting to discuss the consultant's findings and collaboratively decide on the necessary measures to secure the LMS while adhering to the timeline: While stakeholder engagement is important, it could lead to unnecessary delays in resolving vulnerabilities if the meeting does not yield immediate actionable solutions, risking compliance and security. 

Engage an external vendor to conduct penetration testing and vulnerability assessments on the LMS while simultaneously preparing for deployment to identify any critical threats: Although this approach offers an additional layer of evaluation, it does not address the immediate remediation of identified security gaps prior to deployment, potentially resulting in operational and security risks during go-live.
Domain
Domain 1: Security and Risk Management

10-9

the Chief Information Security Officer (CISO) discovers that the CSP‚Äôs security framework lacks certain key security controls commonly associated with NIST SP 800-53. The agency has a strict policy requiring full compliance with federal guidelines but is also under pressure to transfer records to the cloud to enhance operational efficiency.

Overall explanation
The Correct Answer: Conduct a comparative analysis of alternative CSPs that meet all necessary security controls outlined in NIST SP 800-53 before making a decision: This option is the best course of action because it adheres to the agency's strict policy requiring full compliance with federal guidelines. By conducting a comparative analysis, the CISO ensures that the new CSP can meet the regulatory requirements, thereby avoiding potential legal and security issues. It also allows the agency to make an informed decision that balances operational efficiency against compliance and security needs, addressing stakeholder concerns through careful evaluation.  The Incorrect Answers: Develop a comprehensive migration plan that incorporates additional security controls to mitigate the identified gaps while proceeding with the current CSP: While this approach seeks to address the security gaps, it goes against the agency's policy of requiring full compliance from the outset. Relying on mitigation strategies rather than starting with a compliant provider might lead to future compliance challenges and doesn't fully align with risk management objectives when better options may be available.  Initiate a dialogue with the CSP to negotiate the implementation of additional security measures that will align with the agency‚Äôs compliance needs: This option attempts to bring the CSP up to compliance, but it presupposes the CSP is willing and able to meet these demands. It can be time-consuming and might not yield results, delaying the process and risking non-compliance in the interim, which is contrary to the agency's strict policy requirements.  Outline the risks associated with both migrating to the current CSP and selecting an alternative provider, providing this analysis to inform the stakeholders‚Äô decision: While analyzing risks is a vital part of decision-making, it does not proactively seek a compliant solution and could lead to indecision or delay. The priority should be finding a CSP that meets compliance needs, rather than accepting risk mitigation strategies as a compromise when compliant options might be readily available.

Domain
Domain 1: Security and Risk Management

10-10

Overall explanation
The Correct Answer: Restrict access to the API endpoints to only those services that require them to operate, applying an additional layer of authentication: This option effectively mitigates the risk of unauthorized access by ensuring that only legitimate services can interact with the APIs, thereby securing sensitive patient information. It also allows the platform to launch on schedule by addressing vulnerabilities proactively without extensive delays. The Incorrect Answers: Conduct an immediate penetration test to assess the API's vulnerabilities before implementing the platform, delaying the launch until all vulnerabilities are addressed: While this approach may uncover vulnerabilities, delaying the launch could result in missed market opportunities and possible revenue loss, ultimately failing to meet the urgency of the business need. Deploy a monitoring solution that logs access attempts to the API, enabling post-launch identification of potential breaches without altering the launch schedule: Although monitoring can help identify and respond to breaches, it does not proactively mitigate risks or secure the API endpoints, leaving the sensitive data exposed during the critical launch period. Implement a dedicated firewall in front of the APIs to filter traffic based on predefined rules, allowing for real-time traffic analysis while enabling the platform to proceed as planned: This approach may enhance security, but if not combined with strict access controls and authentication protocols, it may not sufficiently address the vulnerabilities, risking unauthorized access to sensitive patient data.
Domain
Domain 4: Communication and Network Security

10-11